# -*- coding: utf-8 -*-
"""05_Streamlit_webapp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18AIxmrr61OuuyN7vjBk1zMAfhf-Ghy21

This is a streamlit app of a simpler version! To run please follow these steps:
To run:
1. pip install streamlit pandas numpy plotly
2. streamlit run 05_streamlit_webapp.py
3. Open http://localhost:8501
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import time
from datetime import datetime, timedelta
import warnings
from collections import Counter
import json
import random

st.set_page_config(
    page_title="Oil Rig Analysis Pipeline",
    page_icon="üõ¢Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
        font-weight: bold;
    }
    .status-badge {
        padding: 0.25rem 0.75rem;
        border-radius: 1rem;
        font-weight: bold;
        display: inline-block;
        margin: 0.25rem;
    }
    .status-green { background-color: #d4edda; color: #155724; }
    .status-yellow { background-color: #fff3cd; color: #856404; }
    .status-red { background-color: #f8d7da; color: #721c24; }
    .metric-highlight {
        font-size: 1.5rem;
        font-weight: bold;
        color: #1f77b4;
    }
</style>
""", unsafe_allow_html=True)

# ============================================================================
# SIMPLIFIED DATA GENERATION
# ============================================================================

class SimpleDataGenerator:
    """Simplified data generator for standalone demo"""

    def __init__(self, months=3, contamination=0.1):
        self.months = months
        self.contamination = contamination
        self.equipment_types = [
            'centrifugal_pump_01', 'gas_compressor_01', 'drilling_motor_01',
            'separator_01', 'generator_01'
        ]

    def generate_sensor_data(self):
        """Generate realistic sensor time series data"""

        start_date = pd.Timestamp('2024-01-01')
        end_date = start_date + pd.DateOffset(months=self.months)
        timestamps = pd.date_range(start=start_date, end=end_date, freq='5T')

        data = []
        for equipment in self.equipment_types:
            for i, ts in enumerate(timestamps):
                # Base patterns with daily and weekly cycles
                daily_cycle = np.sin(2 * np.pi * i / 288)  # 288 = 24 hours in 5-min intervals
                weekly_cycle = 0.3 * np.sin(2 * np.pi * i / (288 * 7))
                noise = np.random.normal(0, 0.1)

                # Equipment-specific base values
                base_values = self._get_equipment_base_values(equipment, daily_cycle, weekly_cycle, noise)

                # Add anomalies randomly
                if np.random.random() < self.contamination:
                    anomaly_sensor = np.random.choice(list(base_values.keys()))
                    base_values[anomaly_sensor] *= np.random.uniform(1.5, 2.5)

                row = {'timestamp': ts, 'equipment': equipment}
                row.update(base_values)
                data.append(row)

        return pd.DataFrame(data)

    def _get_equipment_base_values(self, equipment, daily_cycle, weekly_cycle, noise):
        """Get equipment-specific sensor base values"""

        base_configs = {
            'centrifugal_pump_01': {
                'pressure': 50 + 5 * daily_cycle + weekly_cycle + noise,
                'temperature': 75 + 10 * daily_cycle + weekly_cycle + noise,
                'vibration': abs(2 + 0.5 * daily_cycle + noise),
                'flow_rate': 1000 + 200 * daily_cycle + weekly_cycle * 50 + noise * 20,
                'power': 200 + 30 * daily_cycle + weekly_cycle * 10 + noise * 5
            },
            'gas_compressor_01': {
                'inlet_pressure': 12 + 2 * daily_cycle + weekly_cycle + noise,
                'outlet_pressure': 42 + 5 * daily_cycle + weekly_cycle + noise,
                'temperature': 140 + 15 * daily_cycle + weekly_cycle * 5 + noise * 3,
                'vibration': abs(2.5 + 0.8 * daily_cycle + noise),
                'flow_rate': 6500 + 1000 * daily_cycle + weekly_cycle * 200 + noise * 50
            },
            'drilling_motor_01': {
                'rpm': 100 + 15 * daily_cycle + weekly_cycle * 3 + noise * 2,
                'torque': 20000 + 3000 * daily_cycle + weekly_cycle * 500 + noise * 100,
                'temperature': 100 + 12 * daily_cycle + weekly_cycle * 3 + noise * 2,
                'vibration': abs(5 + 1.5 * daily_cycle + noise),
                'pressure': 275 + 50 * daily_cycle + weekly_cycle * 15 + noise * 10
            },
            'separator_01': {
                'pressure': 30 + 3 * daily_cycle + weekly_cycle + noise,
                'temperature': 75 + 8 * daily_cycle + weekly_cycle * 2 + noise,
                'liquid_level': 50 + 15 * daily_cycle + weekly_cycle * 5 + noise * 2,
                'gas_flow': 4000 + 800 * daily_cycle + weekly_cycle * 150 + noise * 30,
                'liquid_flow': 300 + 50 * daily_cycle + weekly_cycle * 10 + noise * 5
            },
            'generator_01': {
                'voltage': 400 + 15 * daily_cycle + weekly_cycle * 2 + noise,
                'current': 300 + 25 * daily_cycle + weekly_cycle * 5 + noise * 2,
                'frequency': 50 + 0.1 * daily_cycle + weekly_cycle * 0.02 + noise * 0.05,
                'temperature': 85 + 12 * daily_cycle + weekly_cycle * 3 + noise * 2,
                'power_output': 215 + 25 * daily_cycle + weekly_cycle * 5 + noise * 3
            }
        }

        return base_configs.get(equipment, {
            'sensor1': 50 + 10 * daily_cycle + noise,
            'sensor2': 100 + 20 * daily_cycle + noise
        })

    def generate_operator_logs(self, sensor_data, n_logs=100):
        """Generate operator logs correlated with sensor data"""

        # Log templates
        templates = [
            "Routine inspection of {} completed. All parameters normal.",
            "Temperature increase observed on {}. Investigating cause.",
            "Vibration levels elevated on {}. Scheduled for maintenance check.",
            "Pressure readings stable on {}. No issues reported.",
            "Completed maintenance on {}. System performance improved.",
            "Unusual noise detected on {}. Maintenance team notified.",
            "Filter replacement completed on {}. Flow rate normalized.",
            "Calibration performed on {} sensors. Accuracy verified.",
            "Minor leak detected on {}. Repair scheduled.",
            "Performance test completed on {}. Results satisfactory."
        ]

        operators = ['Smith_J', 'Johnson_M', 'Williams_R', 'Brown_K', 'Davis_L']
        shifts = ['Day', 'Evening', 'Night']
        log_types = ['routine', 'maintenance', 'observation', 'repair']

        logs = []
        timestamps = pd.date_range(
            sensor_data['timestamp'].min(),
            sensor_data['timestamp'].max(),
            periods=n_logs
        )

        for ts in timestamps:
            equipment = np.random.choice(self.equipment_types)
            template = np.random.choice(templates)
            entry = template.format(equipment.replace('_', ' ').title())

            logs.append({
                'timestamp': ts + pd.Timedelta(minutes=np.random.randint(-30, 30)),
                'equipment': equipment,
                'operator': np.random.choice(operators),
                'shift': np.random.choice(shifts),
                'entry': entry,
                'log_type': np.random.choice(log_types)
            })

        return pd.DataFrame(logs).sort_values('timestamp')

# ============================================================================
# SIMPLIFIED ANALYSIS MODULES
# ============================================================================

class SimpleAnomalyDetector:
    """Simplified anomaly detection using statistical methods"""

    def __init__(self, contamination=0.1):
        self.contamination = contamination

    def detect_anomalies(self, sensor_data):
        """Detect anomalies using Z-score and IQR methods"""

        anomaly_events = []

        for equipment in sensor_data['equipment'].unique():
            equip_data = sensor_data[sensor_data['equipment'] == equipment].copy()

            # Get numeric columns
            numeric_cols = [col for col in equip_data.columns
                          if col not in ['timestamp', 'equipment'] and
                          pd.api.types.is_numeric_dtype(equip_data[col])]

            for col in numeric_cols:
                # Z-score method
                z_scores = np.abs((equip_data[col] - equip_data[col].mean()) / equip_data[col].std())
                anomaly_indices = equip_data.index[z_scores > 3]

                for idx in anomaly_indices:
                    row = equip_data.loc[idx]
                    severity = 'high' if z_scores.loc[idx] > 4 else 'medium' if z_scores.loc[idx] > 3.5 else 'low'

                    anomaly_events.append({
                        'timestamp_start': row['timestamp'],
                        'timestamp_end': row['timestamp'] + pd.Timedelta(minutes=np.random.randint(15, 120)),
                        'equipment': equipment,
                        'sensor': col,
                        'severity': severity,
                        'score': z_scores.loc[idx],
                        'value': row[col],
                        'duration_minutes': np.random.randint(15, 120)
                    })

        return anomaly_events

class SimpleTextCorrelator:
    """Simplified text correlation using keyword matching"""

    def __init__(self, time_window_hours=2):
        self.time_window_hours = time_window_hours

    def correlate_anomalies_with_logs(self, anomaly_events, operator_logs):
        """Find correlations between anomalies and operator logs"""

        correlations = []

        for anomaly in anomaly_events:
            anomaly_time = pd.to_datetime(anomaly['timestamp_start'])
            equipment = anomaly['equipment']

            # Find logs within time window
            time_mask = (
                (operator_logs['timestamp'] >= anomaly_time - pd.Timedelta(hours=self.time_window_hours)) &
                (operator_logs['timestamp'] <= anomaly_time + pd.Timedelta(hours=self.time_window_hours))
            )

            relevant_logs = operator_logs[time_mask & (operator_logs['equipment'] == equipment)]

            if not relevant_logs.empty:
                # Simple keyword-based correlation
                for _, log in relevant_logs.iterrows():
                    similarity_score = self._calculate_similarity(anomaly, log['entry'])

                    if similarity_score > 0.5:  # Threshold for correlation
                        correlations.append({
                            'anomaly_timestamp': anomaly_time,
                            'log_timestamp': log['timestamp'],
                            'equipment': equipment,
                            'anomaly_severity': anomaly['severity'],
                            'anomaly_sensor': anomaly['sensor'],
                            'log_entry': log['entry'],
                            'log_operator': log['operator'],
                            'semantic_similarity': similarity_score,
                            'temporal_distance_hours': abs((log['timestamp'] - anomaly_time).total_seconds() / 3600),
                            'correlation_score': similarity_score * (1 / (1 + abs((log['timestamp'] - anomaly_time).total_seconds() / 3600)))
                        })

        return correlations

    def _calculate_similarity(self, anomaly, log_entry):
        """Simple keyword-based similarity calculation"""

        # Keywords related to different sensors and conditions
        sensor_keywords = {
            'pressure': ['pressure', 'psi', 'bar', 'vacuum'],
            'temperature': ['temperature', 'temp', 'heat', 'hot', 'cold'],
            'vibration': ['vibration', 'noise', 'shake', 'oscillation'],
            'flow_rate': ['flow', 'rate', 'volume', 'throughput'],
            'power': ['power', 'electricity', 'voltage', 'current']
        }

        severity_keywords = {
            'high': ['critical', 'urgent', 'emergency', 'severe', 'major'],
            'medium': ['moderate', 'significant', 'noticeable', 'elevated'],
            'low': ['minor', 'slight', 'small', 'little']
        }

        log_lower = log_entry.lower()
        score = 0.5  # Base score

        # Check for sensor-related keywords
        if anomaly['sensor'] in sensor_keywords:
            for keyword in sensor_keywords[anomaly['sensor']]:
                if keyword in log_lower:
                    score += 0.2

        # Check for severity-related keywords
        if anomaly['severity'] in severity_keywords:
            for keyword in severity_keywords[anomaly['severity']]:
                if keyword in log_lower:
                    score += 0.1

        # Check for equipment name
        if anomaly['equipment'].replace('_', ' ') in log_lower:
            score += 0.1

        return min(score, 1.0)

class SimplePredictor:
    """Simplified predictive analysis"""

    def __init__(self):
        pass

    def generate_predictions(self, anomaly_events, equipment_list):
        """Generate simple predictions based on historical anomaly patterns"""

        predictions = {}

        for equipment in equipment_list:
            equipment_anomalies = [a for a in anomaly_events if a['equipment'] == equipment]

            # Simple prediction based on recent anomaly frequency
            recent_anomaly_count = len(equipment_anomalies)
            predicted_anomalies = recent_anomaly_count * np.random.uniform(0.8, 1.2)

            confidence = min(0.9, max(0.6, 1.0 - (abs(predicted_anomalies - recent_anomaly_count) / max(1, recent_anomaly_count))))

            risk_level = 'high' if predicted_anomalies > 8 else 'medium' if predicted_anomalies > 3 else 'low'

            predictions[equipment] = {
                'predicted_anomalies': predicted_anomalies,
                'confidence': confidence,
                'risk_level': risk_level,
                'historical_count': recent_anomaly_count
            }

        return predictions

class SimpleRiskAssessor:
    """Simplified risk assessment"""

    def __init__(self):
        pass

    def assess_risks(self, anomaly_events, correlations, equipment_list):
        """Generate risk assessments for equipment"""

        risk_assessments = {}

        for equipment in equipment_list:
            equipment_anomalies = [a for a in anomaly_events if a['equipment'] == equipment]
            equipment_correlations = [c for c in correlations if c['equipment'] == equipment]

            # Risk factors
            anomaly_count = len(equipment_anomalies)
            high_severity_count = len([a for a in equipment_anomalies if a['severity'] == 'high'])
            correlation_rate = len(equipment_correlations) / max(1, anomaly_count)
            avg_response_time = np.mean([c['temporal_distance_hours'] for c in equipment_correlations]) if equipment_correlations else 4

            # Calculate risk score
            risk_score = (
                (anomaly_count * 0.1) +
                (high_severity_count * 0.2) +
                (max(0, 1 - correlation_rate) * 0.3) +
                (min(avg_response_time / 8, 1) * 0.2)
            )
            risk_score = min(risk_score, 1.0)

            risk_level = 'high' if risk_score > 0.7 else 'medium' if risk_score > 0.4 else 'low'

            # Generate recommendations
            recommendations = []
            if anomaly_count > 5:
                recommendations.append(f"Increase monitoring frequency for {equipment.replace('_', ' ')}")
            if high_severity_count > 2:
                recommendations.append(f"Schedule immediate inspection of {equipment.replace('_', ' ')}")
            if correlation_rate < 0.5:
                recommendations.append("Improve operator logging procedures")
            if avg_response_time > 2:
                recommendations.append("Reduce anomaly response time")

            risk_assessments[equipment] = {
                'overall_score': risk_score,
                'risk_level': risk_level,
                'anomaly_count': anomaly_count,
                'high_severity_count': high_severity_count,
                'correlation_rate': correlation_rate,
                'avg_response_time': avg_response_time,
                'recommendations': recommendations or ["Continue current monitoring procedures"]
            }

        return risk_assessments

# ============================================================================
# STREAMLIT APPLICATION
# ============================================================================

def main():
    """Main Streamlit application"""

    # Title
    st.markdown('<h1 class="main-header">üõ¢Ô∏è Oil Rig Analysis Pipeline</h1>', unsafe_allow_html=True)
    st.markdown("**Complete AI-Powered Anomaly Detection & Predictive Maintenance Demo**")
    st.markdown("---")

    # Sidebar configuration
    st.sidebar.header("‚öôÔ∏è Demo Configuration")

    months = st.sidebar.slider("Analysis Period (months)", 1, 6, 3)
    n_logs = st.sidebar.slider("Operator Log Entries", 50, 200, 100)
    contamination = st.sidebar.slider("Anomaly Rate (%)", 5, 20, 10) / 100
    time_window = st.sidebar.slider("Correlation Time Window (hours)", 1, 6, 2)

    # Initialize session state
    if 'demo_results' not in st.session_state:
        st.session_state.demo_results = None

    # Main demo button
    if st.button("üöÄ Run Complete Analysis", type="primary", use_container_width=True):
        run_demo_pipeline(months, n_logs, contamination, time_window)

    # Display results
    if st.session_state.demo_results:
        display_demo_results(st.session_state.demo_results)
    else:
        display_demo_overview(months, n_logs, contamination)

def run_demo_pipeline(months, n_logs, contamination, time_window):
    """Run the complete demo pipeline"""

    with st.spinner("üîÑ Running complete analysis pipeline..."):
        progress_bar = st.progress(0)
        status_text = st.empty()

        # Step 1: Data Generation
        status_text.text("üìä Generating synthetic oil rig data...")
        progress_bar.progress(20)

        generator = SimpleDataGenerator(months=months, contamination=contamination)
        sensor_data = generator.generate_sensor_data()
        operator_logs = generator.generate_operator_logs(sensor_data, n_logs)

        # Step 2: Anomaly Detection
        status_text.text("üîç Detecting equipment anomalies...")
        progress_bar.progress(40)

        detector = SimpleAnomalyDetector(contamination=contamination)
        anomaly_events = detector.detect_anomalies(sensor_data)

        # Step 3: Text Correlation
        status_text.text("üìù Correlating anomalies with operator logs...")
        progress_bar.progress(60)

        correlator = SimpleTextCorrelator(time_window_hours=time_window)
        correlations = correlator.correlate_anomalies_with_logs(anomaly_events, operator_logs)

        # Step 4: Predictions & Risk Assessment
        status_text.text("üîÆ Generating predictions and risk assessments...")
        progress_bar.progress(80)

        predictor = SimplePredictor()
        predictions = predictor.generate_predictions(anomaly_events, sensor_data['equipment'].unique())

        risk_assessor = SimpleRiskAssessor()
        risk_assessments = risk_assessor.assess_risks(anomaly_events, correlations, sensor_data['equipment'].unique())

        # Complete
        status_text.text("‚úÖ Analysis complete!")
        progress_bar.progress(100)

        # Store results
        st.session_state.demo_results = {
            'sensor_data': sensor_data,
            'operator_logs': operator_logs,
            'anomaly_events': anomaly_events,
            'correlations': correlations,
            'predictions': predictions,
            'risk_assessments': risk_assessments,
            'config': {
                'months': months,
                'n_logs': n_logs,
                'contamination': contamination,
                'time_window': time_window
            }
        }

        time.sleep(1)
        st.rerun()

def display_demo_overview(months, n_logs, contamination):
    """Display demo overview and expectations"""

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("üéØ What This Demo Will Do")

        st.markdown("""
        **üìä Data Generation**
        - Creates realistic sensor data for 5 oil rig equipment types
        - Generates correlated operator log entries
        - Introduces controlled anomalies and patterns

        **üîç Anomaly Detection**
        - Uses statistical methods (Z-score, IQR) for anomaly detection
        - Identifies severity levels and affected sensors
        - Provides confidence scoring for each detection

        **üìù Text Correlation**
        - Matches anomaly events with operator log entries
        - Calculates semantic similarity using keyword analysis
        - Identifies response patterns and communication gaps

        **üß† Predictive Analysis**
        - Forecasts potential equipment failures
        - Assesses risk levels for each equipment type
        - Generates actionable maintenance recommendations
        """)

    with col2:
        st.subheader("üìà Expected Results")

        expected_anomalies = int(months * 30 * 24 * 12 * 5 * contamination / 100)  # Rough estimate
        expected_correlations = int(expected_anomalies * 0.6)  # ~60% correlation rate

        st.info(f"""
        **Configuration:**
        - Period: {months} months
        - Equipment: 5 units
        - Log entries: {n_logs}
        - Anomaly rate: {contamination:.1%}

        **Expected Output:**
        - ~{expected_anomalies} anomaly events
        - ~{expected_correlations} text correlations
        - 5 equipment risk assessments
        - Predictive recommendations

        **Processing time:** ~10-15 seconds
        """)

        st.warning("‚ö° Click 'Run Complete Analysis' to start the demo!")

def display_demo_results(results):
    """Display comprehensive demo results"""

    st.success("‚úÖ Analysis Complete! Explore the results below.")

    # Executive Summary
    display_executive_summary(results)

    # Detailed tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "üîç Anomaly Detection",
        "üìù Text Correlation",
        "üîÆ Predictions & Risk",
        "üìä Data Overview"
    ])

    with tab1:
        display_anomaly_analysis(results)

    with tab2:
        display_correlation_analysis(results)

    with tab3:
        display_predictions_and_risk(results)

    with tab4:
        display_data_overview(results)

def display_executive_summary(results):
    """Display executive summary dashboard"""

    st.header("üìä Executive Summary")

    # Key metrics
    total_anomalies = len(results['anomaly_events'])
    total_correlations = len(results['correlations'])
    coverage_rate = total_correlations / max(1, total_anomalies)
    high_risk_equipment = len([k for k, v in results['risk_assessments'].items() if v['risk_level'] == 'high'])

    # Status determination
    if high_risk_equipment == 0 and coverage_rate > 0.7:
        status = "GREEN - Normal Operations"
        status_class = "status-green"
    elif high_risk_equipment <= 1 and coverage_rate > 0.5:
        status = "YELLOW - Monitoring Required"
        status_class = "status-yellow"
    else:
        status = "RED - Immediate Action Required"
        status_class = "status-red"

    st.markdown(f'<div class="status-badge {status_class}">{status}</div>', unsafe_allow_html=True)

    # Metrics row
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Total Anomalies", total_anomalies)
    with col2:
        st.metric("Text Correlations", total_correlations, f"{coverage_rate:.1%} coverage")
    with col3:
        st.metric("High-Risk Equipment", high_risk_equipment)
    with col4:
        st.metric("Analysis Period", f"{results['config']['months']} months")

    st.markdown("---")

def display_anomaly_analysis(results):
    """Display detailed anomaly analysis"""

    st.subheader("üîç Anomaly Detection Results")

    anomalies = results['anomaly_events']

    if not anomalies:
        st.warning("No anomalies detected in the analysis period.")
        return

    df_anomalies = pd.DataFrame(anomalies)

    # Severity and equipment distribution
    col1, col2 = st.columns(2)

    with col1:
        severity_counts = df_anomalies['severity'].value_counts()
        fig_sev = px.pie(
            values=severity_counts.values,
            names=severity_counts.index,
            title="Anomaly Severity Distribution",
            color_discrete_map={'high': '#dc3545', 'medium': '#ffc107', 'low': '#28a745'}
        )
        st.plotly_chart(fig_sev, use_container_width=True)

    with col2:
        equipment_counts = df_anomalies['equipment'].value_counts()
        equipment_names = [name.replace('_', ' ').title() for name in equipment_counts.index]
        fig_eq = px.bar(
            x=equipment_counts.values,
            y=equipment_names,
            orientation='h',
            title="Anomalies by Equipment",
            labels={'x': 'Count', 'y': 'Equipment'}
        )
        st.plotly_chart(fig_eq, use_container_width=True)

    # Timeline visualization
    st.subheader("üìà Anomaly Timeline")

    df_anomalies['timestamp_start'] = pd.to_datetime(df_anomalies['timestamp_start'])
    df_anomalies['equipment_display'] = df_anomalies['equipment'].str.replace('_', ' ').str.title()

    fig_timeline = px.scatter(
        df_anomalies,
        x='timestamp_start',
        y='equipment_display',
        color='severity',
        size='score',
        hover_data=['sensor', 'duration_minutes'],
        title="Anomaly Events Over Time",
        color_discrete_map={'high': '#dc3545', 'medium': '#ffc107', 'low': '#28a745'}
    )
    fig_timeline.update_layout(height=400)
    st.plotly_chart(fig_timeline, use_container_width=True)

    # Recent anomalies table
    st.subheader("üìã Recent High-Priority Anomalies")
    recent_anomalies = df_anomalies[df_anomalies['severity'].isin(['high', 'medium'])].nlargest(10, 'score')

    if not recent_anomalies.empty:
        display_df = recent_anomalies[['timestamp_start', 'equipment', 'sensor', 'severity', 'score']].copy()
        display_df['timestamp_start'] = display_df['timestamp_start'].dt.strftime('%Y-%m-%d %H:%M')
        display_df['equipment'] = display_df['equipment'].str.replace('_', ' ').str.title()
        display_df['score'] = display_df['score'].round(2)

        st.dataframe(display_df, use_container_width=True)

def display_correlation_analysis(results):
    """Display text correlation analysis"""

    st.subheader("üìù Anomaly-Text Correlation Analysis")

    correlations = results['correlations']

    if not correlations:
        st.warning("No significant correlations found between anomalies and operator logs.")
        st.info("Try adjusting the time window or reducing the correlation threshold.")
        return

    df_corr = pd.DataFrame(correlations)

    # Correlation metrics
    col1, col2, col3 = st.columns(3)

    with col1:
        avg_similarity = df_corr['semantic_similarity'].mean()
        st.metric("Avg Semantic Similarity", f"{avg_similarity:.3f}")

    with col2:
        avg_response_time = df_corr['temporal_distance_hours'].mean()
        st.metric("Avg Response Time", f"{avg_response_time:.1f} hours")

    with col3:
        coverage_rate = len(correlations) / len(results['anomaly_events'])
        st.metric("Correlation Coverage", f"{coverage_rate:.1%}")

    # Correlation quality scatter plot
    fig_scatter = px.scatter(
        df_corr,
        x='temporal_distance_hours',
        y='semantic_similarity',
        color='equipment',
        size='correlation_score',
        hover_data=['anomaly_severity', 'log_operator'],
        title="Correlation Quality Analysis",
        labels={'temporal_distance_hours': 'Response Time (hours)', 'semantic_similarity': 'Semantic Similarity'}
    )
    st.plotly_chart(fig_scatter, use_container_width=True)

    # Top correlations
    st.subheader("üèÜ Top Correlations")
    top_correlations = df_corr.nlargest(5, 'correlation_score')

    for idx, row in top_correlations.iterrows():
        with st.expander(f"üîó {row['equipment'].replace('_', ' ').title()} - Score: {row['correlation_score']:.3f}"):
            col1, col2 = st.columns(2)

            with col1:
                st.write("**Anomaly Information:**")
                st.write(f"- Equipment: {row['equipment'].replace('_', ' ').title()}")
                st.write(f"- Sensor: {row['anomaly_sensor']}")
                st.write(f"- Severity: {row['anomaly_severity'].title()}")
                st.write(f"- Time: {row['anomaly_timestamp']}")

            with col2:
                st.write("**Operator Response:**")
                st.write(f"- Operator: {row['log_operator']}")
                st.write(f"- Response Time: {row['temporal_distance_hours']:.1f} hours")
                st.write(f"- Similarity: {row['semantic_similarity']:.3f}")

            st.write(f"**Log Entry:** _{row['log_entry']}_")

def display_predictions_and_risk(results):
    """Display predictions and risk assessment"""

    st.subheader("üîÆ Predictive Analysis & Risk Assessment")

    predictions = results['predictions']
    risk_assessments = results['risk_assessments']

    # Predictions overview
    col1, col2 = st.columns(2)

    with col1:
        st.write("**30-Day Failure Predictions**")

        pred_df = pd.DataFrame.from_dict(predictions, orient='index')
        pred_df['equipment'] = pred_df.index
        pred_df['equipment_display'] = pred_df['equipment'].str.replace('_', ' ').str.title()

        fig_pred = px.bar(
            pred_df,
            x='equipment_display',
            y='predicted_anomalies',
            color='risk_level',
            title="Predicted Anomalies (Next 30 Days)",
            color_discrete_map={'high': '#dc3545', 'medium': '#ffc107', 'low': '#28a745'}
        )
        fig_pred.update_xaxes(tickangle=45)
        st.plotly_chart(fig_pred, use_container_width=True)

    with col2:
        st.write("**Risk Assessment Overview**")

        risk_df = pd.DataFrame.from_dict(risk_assessments, orient='index')
        risk_df['equipment'] = risk_df.index
        risk_df['equipment_display'] = risk_df['equipment'].str.replace('_', ' ').str.title()

        fig_risk = px.bar(
            risk_df.sort_values('overall_score'),
            x='overall_score',
            y='equipment_display',
            color='risk_level',
            orientation='h',
            title="Equipment Risk Scores",
            color_discrete_map={'high': '#dc3545', 'medium': '#ffc107', 'low': '#28a745'}
        )
        st.plotly_chart(fig_risk, use_container_width=True)

    # Detailed recommendations
    st.subheader("üéØ Equipment-Specific Recommendations")

    # Sort by risk score
    sorted_equipment = sorted(risk_assessments.items(), key=lambda x: x[1]['overall_score'], reverse=True)

    for equipment, risk_data in sorted_equipment:
        risk_emoji = "üî¥" if risk_data['risk_level'] == 'high' else "üü°" if risk_data['risk_level'] == 'medium' else "üü¢"

        with st.expander(f"{risk_emoji} {equipment.replace('_', ' ').title()} - {risk_data['risk_level'].title()} Risk"):
            col1, col2 = st.columns(2)

            with col1:
                st.write("**Risk Metrics:**")
                st.write(f"- Overall Score: {risk_data['overall_score']:.3f}")
                st.write(f"- Anomaly Count: {risk_data['anomaly_count']}")
                st.write(f"- High Severity Events: {risk_data['high_severity_count']}")
                st.write(f"- Correlation Rate: {risk_data['correlation_rate']:.1%}")
                st.write(f"- Avg Response Time: {risk_data['avg_response_time']:.1f} hours")

            with col2:
                st.write("**Predictions:**")
                if equipment in predictions:
                    pred = predictions[equipment]
                    st.write(f"- Predicted Anomalies: {pred['predicted_anomalies']:.1f}")
                    st.write(f"- Confidence: {pred['confidence']:.1%}")
                    st.write(f"- Risk Level: {pred['risk_level'].title()}")

            st.write("**Recommended Actions:**")
            for rec in risk_data['recommendations']:
                st.write(f"‚Ä¢ {rec}")

def display_data_overview(results):
    """Display data overview and export options"""

    st.subheader("üìä Data Overview & Export")

    # Data summary
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Sensor Data Points", len(results['sensor_data']))
    with col2:
        st.metric("Operator Logs", len(results['operator_logs']))
    with col3:
        st.metric("Anomaly Events", len(results['anomaly_events']))
    with col4:
        st.metric("Text Correlations", len(results['correlations']))

    # Sample data previews
    st.subheader("üëÄ Data Samples")

    preview_tab1, preview_tab2, preview_tab3 = st.tabs(["Sensor Data", "Operator Logs", "Anomalies"])

    with preview_tab1:
        st.dataframe(results['sensor_data'].head(20), use_container_width=True)

    with preview_tab2:
        st.dataframe(results['operator_logs'].head(20), use_container_width=True)

    with preview_tab3:
        if results['anomaly_events']:
            anomaly_df = pd.DataFrame(results['anomaly_events'])
            st.dataframe(anomaly_df.head(20), use_container_width=True)

    # Export options
    st.subheader("üíæ Export Results")

    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üìä Export Anomaly Data"):
            if results['anomaly_events']:
                anomaly_df = pd.DataFrame(results['anomaly_events'])
                csv = anomaly_df.to_csv(index=False)
                st.download_button(
                    "Download Anomaly CSV",
                    csv,
                    "oil_rig_anomalies.csv",
                    "text/csv"
                )

    with col2:
        if st.button("üìù Export Correlations"):
            if results['correlations']:
                correlation_df = pd.DataFrame(results['correlations'])
                csv = correlation_df.to_csv(index=False)
                st.download_button(
                    "Download Correlations CSV",
                    csv,
                    "oil_rig_correlations.csv",
                    "text/csv"
                )

    with col3:
        if st.button("üîÆ Export Risk Assessment"):
            risk_df = pd.DataFrame.from_dict(results['risk_assessments'], orient='index')
            csv = risk_df.to_csv()
            st.download_button(
                "Download Risk Assessment CSV",
                csv,
                "oil_rig_risk_assessment.csv",
                "text/csv"
            )

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; margin-top: 2rem;'>
    <p><b>üõ¢Ô∏è Oil Rig Analysis Pipeline - Standalone Demo</b></p>
    <p>Complete AI-powered solution for predictive maintenance and anomaly detection</p>
    <p><small>Built with Streamlit ‚Ä¢ Synthetic data for demonstration purposes</small></p>
</div>
""", unsafe_allow_html=True)

# Run the app
if __name__ == "__main__":
    main()

