# -*- coding: utf-8 -*-
"""04_Generate_Insights.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11C7jpxC3Sw5ycZ8jofceTPaavv13L20w
"""

###Install Libraries required
!pip install pandas numpy scipy scikit-learn
!pip install openai anthropic
!pip install langchain langchain-openai
!pip install plotly seaborn matplotlib
!pip install jinja2 reportlab
!pip install transformers torch
!pip install fpdf2 python-docx
!pip install wordcloud networkx

!pip install langchain-community

# IMPORTS
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

from datetime import datetime, timedelta, date
from typing import Dict, List, Tuple, Optional, Union, Any
import warnings
import json
import re
import os
from collections import Counter, defaultdict
from dataclasses import dataclass
import asyncio

# LLM and AI imports
import openai
try:
    import anthropic
except ImportError:
    anthropic = None

from langchain_community.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.schema import HumanMessage, SystemMessage

# Report generation
from jinja2 import Template
import base64
from io import BytesIO
import tempfile

# ML for predictions
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Suppress warnings
warnings.filterwarnings('ignore')

@dataclass
class InsightGenerationConfig:
    """Configuration class for insight generation parameters"""

    # LLM Configuration
    openai_api_key: Optional[str] = None
    anthropic_api_key: Optional[str] = None
    openai_model: str = "gpt-3.5-turbo"
    anthropic_model: str = "claude-3-sonnet-20240229"
    max_tokens: int = 2000
    temperature: float = 0.3

    # Analysis Configuration
    risk_assessment_enabled: bool = True
    predictive_modeling_enabled: bool = True
    automated_recommendations: bool = True
    maintenance_planning: bool = True

    # Report Configuration
    report_format: str = "html"  # "html", "pdf", "json"
    include_visualizations: bool = True
    include_raw_data: bool = False
    executive_summary: bool = True
    detailed_analysis: bool = True

    # Prediction Configuration
    prediction_horizon_days: int = 30
    min_historical_days: int = 7
    confidence_threshold: float = 0.7

    # Risk Scoring
    risk_factors: Dict[str, float] = None
    severity_weights: Dict[str, float] = None

    def __post_init__(self):
        if self.risk_factors is None:
            self.risk_factors = {
                'high_anomaly_frequency': 0.3,
                'poor_correlation_coverage': 0.2,
                'delayed_response_time': 0.2,
                'critical_equipment': 0.15,
                'maintenance_overdue': 0.15
            }

        if self.severity_weights is None:
            self.severity_weights = {
                'high': 1.0,
                'medium': 0.6,
                'low': 0.3
            }

class LLMInsightGenerator:
    """
    Generate insights using Large Language Models (OpenAI GPT or Anthropic Claude)
    """

    def __init__(self, config: InsightGenerationConfig):
        self.config = config
        self.openai_client = None
        self.anthropic_client = None
        self.chat_model = None

        # Initialize LLM clients
        self._initialize_llm_clients()

        # Prompt templates
        self.prompt_templates = self._create_prompt_templates()

    def _initialize_llm_clients(self):
        """Initialize LLM clients based on available API keys"""

        # Try to get API keys from environment or config
        openai_key = self.config.openai_api_key or os.getenv('OPENAI_API_KEY')
        anthropic_key = self.config.anthropic_api_key or os.getenv('ANTHROPIC_API_KEY')

        if openai_key:
            try:
                openai.api_key = openai_key
                self.chat_model = ChatOpenAI(
                    model=self.config.openai_model,
                    temperature=self.config.temperature,
                    max_tokens=self.config.max_tokens,
                    openai_api_key=openai_key
                )
                print("✅ OpenAI client initialized")
            except Exception as e:
                print(f"⚠️ OpenAI initialization failed: {e}")

        if anthropic_key and anthropic:
            try:
                self.anthropic_client = anthropic.Anthropic(api_key=anthropic_key)
                print("✅ Anthropic client initialized")
            except Exception as e:
                print(f"⚠️ Anthropic initialization failed: {e}")

        if not self.chat_model and not self.anthropic_client:
            print("⚠️ No LLM clients available. Using template-based insights only.")

    def _create_prompt_templates(self) -> Dict[str, str]:
        """Create prompt templates for different types of insights"""

        templates = {
            'anomaly_analysis': """
You are an expert oil rig maintenance engineer analyzing anomaly detection results.

ANOMALY DATA:
{anomaly_summary}

CORRELATION DATA:
{correlation_summary}

EQUIPMENT PATTERNS:
{equipment_patterns}

Please provide:
1. Key findings about the anomalies detected
2. Root cause analysis based on correlations with operator logs
3. Risk assessment for each equipment type
4. Immediate action items (next 24-48 hours)
5. Medium-term recommendations (next 2-4 weeks)

Focus on actionable insights that maintenance teams can implement immediately.
""",

            'predictive_maintenance': """
You are a predictive maintenance specialist analyzing oil rig equipment data.

HISTORICAL PATTERNS:
{historical_patterns}

CURRENT STATUS:
{current_status}

CORRELATION INSIGHTS:
{correlation_insights}

Based on this analysis, provide:
1. Probability of equipment failures in the next 30 days
2. Recommended maintenance schedule optimization
3. Priority ranking of equipment requiring attention
4. Cost-benefit analysis of preventive vs reactive maintenance
5. Resource allocation recommendations

Include specific timeframes and confidence levels where possible.
""",

            'risk_assessment': """
You are a risk management expert evaluating oil rig operational risks.

RISK INDICATORS:
{risk_indicators}

ANOMALY PATTERNS:
{anomaly_patterns}

OPERATIONAL CONTEXT:
{operational_context}

Provide a comprehensive risk assessment including:
1. Overall risk score (1-10) with justification
2. Top 3 critical risk factors
3. Potential impact assessment (safety, production, cost)
4. Risk mitigation strategies
5. Recommended monitoring frequency
6. Emergency response recommendations

Focus on both immediate and long-term risks.
""",

            'executive_summary': """
You are a senior technical advisor preparing an executive summary for oil rig management.

ANALYSIS RESULTS:
{analysis_results}

KEY METRICS:
{key_metrics}

BUSINESS IMPACT:
{business_impact}

Create an executive summary that includes:
1. Current operational status (Green/Yellow/Red)
2. Key performance indicators and trends
3. Critical issues requiring executive attention
4. Financial impact and cost implications
5. Strategic recommendations for the next quarter
6. Resource requirements and timeline

Keep the summary concise but comprehensive, suitable for C-level executives.
""",

            'operator_insights': """
You are analyzing operator communication patterns and anomaly correlations for oil rig operations.

OPERATOR LOG ANALYSIS:
{operator_analysis}

ANOMALY CORRELATIONS:
{correlations}

COMMUNICATION PATTERNS:
{communication_patterns}

Provide insights on:
1. Quality of operator anomaly reporting
2. Response time patterns and effectiveness
3. Training recommendations for operators
4. Communication protocol improvements
5. Equipment-specific reporting patterns
6. Shift-based performance variations

Focus on improving human factors and communication effectiveness.
"""
        }

        return templates

    async def generate_insight(self, insight_type: str, context_data: Dict) -> str:
        """Generate insights using LLM"""

        if insight_type not in self.prompt_templates:
            return f"Unknown insight type: {insight_type}"

        # Format the prompt with context data
        prompt = self.prompt_templates[insight_type].format(**context_data)

        # Generate insight using available LLM
        if self.chat_model:
            return await self._generate_with_openai(prompt)
        elif self.anthropic_client:
            return await self._generate_with_anthropic(prompt)
        else:
            return self._generate_template_based_insight(insight_type, context_data)

    async def _generate_with_openai(self, prompt: str) -> str:
        """Generate insight using OpenAI"""
        try:
            messages = [
                SystemMessage(content="You are an expert oil rig maintenance and operations analyst."),
                HumanMessage(content=prompt)
            ]

            response = await self.chat_model.agenerate([messages])
            return response.generations[0][0].text

        except Exception as e:
            print(f"OpenAI generation error: {e}")
            return f"Error generating insight with OpenAI: {e}"

    async def _generate_with_anthropic(self, prompt: str) -> str:
        """Generate insight using Anthropic Claude"""
        try:
            response = self.anthropic_client.messages.create(
                model=self.config.anthropic_model,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            return response.content[0].text

        except Exception as e:
            print(f"Anthropic generation error: {e}")
            return f"Error generating insight with Anthropic: {e}"

    def _generate_template_based_insight(self, insight_type: str, context_data: Dict) -> str:
        """Generate basic insights without LLM (fallback)"""

        insights = {
            'anomaly_analysis': self._template_anomaly_analysis(context_data),
            'predictive_maintenance': self._template_predictive_maintenance(context_data),
            'risk_assessment': self._template_risk_assessment(context_data),
            'executive_summary': self._template_executive_summary(context_data),
            'operator_insights': self._template_operator_insights(context_data)
        }

        return insights.get(insight_type, "Template-based insight not available for this type.")

    def _template_anomaly_analysis(self, context_data: Dict) -> str:
        """Template-based anomaly analysis"""
        return f"""
ANOMALY ANALYSIS SUMMARY (Template-based):

Key Findings:
• Total anomalies detected: {context_data.get('total_anomalies', 'N/A')}
• Equipment coverage: {context_data.get('equipment_coverage', 'N/A')}
• Correlation rate: {context_data.get('correlation_rate', 'N/A')}

Immediate Actions Required:
• Review high-severity anomalies
• Verify operator log completeness
• Schedule equipment inspections

Recommendations:
• Implement proactive monitoring for high-anomaly equipment
• Improve operator training on anomaly reporting
• Consider predictive maintenance scheduling
"""

    def _template_predictive_maintenance(self, context_data: Dict) -> str:
        """Template-based predictive maintenance insights"""
        return f"""
PREDICTIVE MAINTENANCE RECOMMENDATIONS (Template-based):

Priority Equipment:
• Equipment with highest anomaly rates require immediate attention
• Focus on equipment with poor correlation coverage

Maintenance Schedule:
• Weekly inspections for high-risk equipment
• Monthly comprehensive reviews for all equipment
• Quarterly deep maintenance cycles

Resource Allocation:
• Increase monitoring frequency during peak anomaly hours
• Allocate additional maintenance staff for high-priority equipment
"""

    def _template_risk_assessment(self, context_data: Dict) -> str:
        """Template-based risk assessment"""
        return f"""
RISK ASSESSMENT (Template-based):

Overall Risk Level: MODERATE

Critical Risk Factors:
1. Equipment with high anomaly frequency
2. Poor anomaly-log correlation coverage
3. Delayed response times to anomalies

Mitigation Strategies:
• Implement real-time monitoring systems
• Improve operator training and response procedures
• Schedule preventive maintenance based on anomaly patterns

Monitoring Recommendations:
• Continuous monitoring for critical equipment
• Daily review of anomaly reports
• Weekly trend analysis
"""

    def _template_executive_summary(self, context_data: Dict) -> str:
        """Template-based executive summary"""
        return f"""
EXECUTIVE SUMMARY (Template-based):

Operational Status: MONITORING REQUIRED

Key Metrics:
• Anomaly detection system operational
• Correlation analysis providing actionable insights
• Maintenance scheduling optimization opportunities identified

Strategic Recommendations:
• Invest in predictive maintenance capabilities
• Improve operator training and communication systems
• Consider additional sensor deployment for better coverage

Financial Impact:
• Potential cost savings through preventive maintenance
• Reduced unplanned downtime risk
• Improved operational efficiency
"""

    def _template_operator_insights(self, context_data: Dict) -> str:
        """Template-based operator insights"""
        return f"""
OPERATOR COMMUNICATION ANALYSIS (Template-based):

Communication Quality:
• Operator logs provide valuable context for anomalies
• Response times vary by shift and equipment type
• Documentation quality could be improved

Training Recommendations:
• Standardize anomaly reporting procedures
• Improve technical terminology usage
• Implement structured logging formats

Performance Variations:
• Some shifts show better anomaly documentation
• Equipment-specific reporting patterns identified
• Response time improvements needed
"""

class PredictiveAnalyzer:
    """
    Predictive modeling and trend analysis for maintenance planning
    """

    def __init__(self, config: InsightGenerationConfig):
        self.config = config
        self.models = {}
        self.scalers = {}
        self.predictions = {}

    def prepare_time_series_data(self, correlations: List[Dict],
                                anomaly_events: List[Dict]) -> pd.DataFrame:
        """Prepare time series data for predictive modeling"""

        # Create daily aggregated data
        all_timestamps = []

        # Extract timestamps from anomalies
        for event in anomaly_events:
            all_timestamps.append(pd.to_datetime(event['timestamp_start']))

        # Extract timestamps from correlations
        for corr in correlations:
            all_timestamps.append(pd.to_datetime(corr['anomaly_timestamp']))

        if not all_timestamps:
            return pd.DataFrame()

        # Create date range
        start_date = min(all_timestamps).date()
        end_date = max(all_timestamps).date()
        date_range = pd.date_range(start=start_date, end=end_date, freq='D')

        # Initialize DataFrame
        ts_data = pd.DataFrame({'date': date_range})

        # Aggregate anomalies by day and equipment
        equipment_list = list(set([event['equipment'] for event in anomaly_events]))

        for equipment in equipment_list:
            # Count daily anomalies for each equipment
            equipment_anomalies = [
                event for event in anomaly_events
                if event['equipment'] == equipment
            ]

            daily_counts = Counter()
            for event in equipment_anomalies:
                event_date = pd.to_datetime(event['timestamp_start']).date()
                daily_counts[event_date] += 1

            # Add to DataFrame
            ts_data[f'{equipment}_anomalies'] = [
                daily_counts.get(date.date(), 0) for date in date_range
            ]

            # Calculate rolling averages
            ts_data[f'{equipment}_7day_avg'] = ts_data[f'{equipment}_anomalies'].rolling(7, min_periods=1).mean()
            ts_data[f'{equipment}_trend'] = ts_data[f'{equipment}_anomalies'].rolling(7, min_periods=1).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0
            )

        # Add time-based features
        ts_data['day_of_week'] = ts_data['date'].dt.dayofweek
        ts_data['month'] = ts_data['date'].dt.month
        ts_data['day_of_month'] = ts_data['date'].dt.day

        # Add lag features
        for equipment in equipment_list:
            for lag in [1, 3, 7]:
                ts_data[f'{equipment}_lag_{lag}'] = ts_data[f'{equipment}_anomalies'].shift(lag)

        return ts_data

    def train_predictive_models(self, ts_data: pd.DataFrame) -> Dict[str, Any]:
        """Train predictive models for each equipment"""

        if ts_data.empty or len(ts_data) < self.config.min_historical_days:
            return {}

        results = {}
        equipment_columns = [col for col in ts_data.columns if col.endswith('_anomalies')]
        equipment_list = [col.replace('_anomalies', '') for col in equipment_columns]

        for equipment in equipment_list:
            try:
                # Prepare features and target
                feature_cols = [
                    col for col in ts_data.columns
                    if equipment in col and col != f'{equipment}_anomalies'
                ] + ['day_of_week', 'month', 'day_of_month']

                # Remove columns with all NaN values
                feature_cols = [col for col in feature_cols if col in ts_data.columns and not ts_data[col].isna().all()]

                if not feature_cols:
                    continue

                X = ts_data[feature_cols].fillna(0)
                y = ts_data[f'{equipment}_anomalies']

                # Skip if not enough data
                if len(X) < 10:
                    continue

                # Split data (use last 20% for testing)
                split_idx = int(len(X) * 0.8)
                X_train, X_test = X[:split_idx], X[split_idx:]
                y_train, y_test = y[:split_idx], y[split_idx:]

                # Scale features
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)

                # Train multiple models
                models = {
                    'random_forest': RandomForestRegressor(n_estimators=50, random_state=42),
                    'gradient_boosting': GradientBoostingRegressor(n_estimators=50, random_state=42),
                    'linear': LinearRegression()
                }

                model_results = {}
                for model_name, model in models.items():
                    try:
                        model.fit(X_train_scaled, y_train)
                        y_pred = model.predict(X_test_scaled)

                        # Calculate metrics
                        mae = mean_absolute_error(y_test, y_pred)
                        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                        r2 = r2_score(y_test, y_pred) if len(np.unique(y_test)) > 1 else 0

                        model_results[model_name] = {
                            'model': model,
                            'mae': mae,
                            'rmse': rmse,
                            'r2': r2,
                            'predictions': y_pred.tolist()
                        }
                    except Exception as e:
                        print(f"Error training {model_name} for {equipment}: {e}")
                        continue

                # Select best model based on MAE
                if model_results:
                    best_model_name = min(model_results.keys(), key=lambda k: model_results[k]['mae'])
                    best_model = model_results[best_model_name]

                    results[equipment] = {
                        'best_model_name': best_model_name,
                        'model': best_model['model'],
                        'scaler': scaler,
                        'feature_columns': feature_cols,
                        'performance': {
                            'mae': best_model['mae'],
                            'rmse': best_model['rmse'],
                            'r2': best_model['r2']
                        },
                        'all_models': model_results
                    }

            except Exception as e:
                print(f"Error training models for {equipment}: {e}")
                continue

        self.models = results
        return results

    def generate_predictions(self, ts_data: pd.DataFrame,
                           days_ahead: int = None) -> Dict[str, Any]:
        """Generate predictions for future anomalies"""

        days_ahead = days_ahead or self.config.prediction_horizon_days
        predictions = {}

        if not self.models or ts_data.empty:
            return predictions

        for equipment, model_info in self.models.items():
            try:
                # Get the most recent data point
                latest_data = ts_data.iloc[-1:][model_info['feature_columns']].fillna(0)

                # Scale features
                latest_scaled = model_info['scaler'].transform(latest_data)

                # Generate prediction
                prediction = model_info['model'].predict(latest_scaled)[0]

                # Calculate confidence based on model performance
                confidence = max(0, min(1, 1 - model_info['performance']['mae'] / (prediction + 1)))

                # Classify risk level
                if prediction > 2:
                    risk_level = 'high'
                elif prediction > 1:
                    risk_level = 'medium'
                else:
                    risk_level = 'low'

                predictions[equipment] = {
                    'predicted_anomalies': float(prediction),
                    'confidence': float(confidence),
                    'risk_level': risk_level,
                    'model_used': model_info['best_model_name'],
                    'performance_metrics': model_info['performance']
                }

            except Exception as e:
                print(f"Error generating predictions for {equipment}: {e}")
                continue

        self.predictions = predictions
        return predictions

class RiskAssessmentAnalyzer:
    """
    Comprehensive risk assessment and scoring
    """

    def __init__(self, config: InsightGenerationConfig):
        self.config = config
        self.risk_scores = {}
        self.risk_factors = {}

    def calculate_equipment_risk_scores(self,
                                      anomaly_events: List[Dict],
                                      correlations: List[Dict],
                                      predictions: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate comprehensive risk scores for each equipment"""

        equipment_list = list(set([event['equipment'] for event in anomaly_events]))
        risk_scores = {}

        for equipment in equipment_list:
            risk_factors = self._calculate_risk_factors(equipment, anomaly_events, correlations, predictions)
            overall_score = self._calculate_overall_risk_score(risk_factors)

            risk_scores[equipment] = {
                'overall_score': overall_score,
                'risk_level': self._classify_risk_level(overall_score),
                'risk_factors': risk_factors,
                'recommendations': self._generate_risk_recommendations(equipment, risk_factors)
            }

        self.risk_scores = risk_scores
        return risk_scores

    def _calculate_risk_factors(self, equipment: str, anomaly_events: List[Dict],
                               correlations: List[Dict], predictions: Dict[str, Any]) -> Dict[str, float]:
        """Calculate individual risk factors for equipment"""

        # Filter data for this equipment
        equipment_anomalies = [e for e in anomaly_events if e['equipment'] == equipment]
        equipment_correlations = [c for c in correlations if c['equipment'] == equipment]

        risk_factors = {}

        # 1. Anomaly frequency risk
        anomaly_count = len(equipment_anomalies)
        high_severity_count = len([e for e in equipment_anomalies if e['severity'] == 'high'])

        if anomaly_count > 10:
            frequency_risk = min(1.0, anomaly_count / 20)  # Normalize to 0-1
        else:
            frequency_risk = anomaly_count / 20

        frequency_risk += high_severity_count * 0.1  # Bonus for high severity
        risk_factors['anomaly_frequency'] = min(1.0, frequency_risk)

        # 2. Correlation coverage risk (poor coverage = higher risk)
        coverage_rate = len(equipment_correlations) / max(1, anomaly_count)
        risk_factors['poor_correlation_coverage'] = max(0, 1 - coverage_rate)

        # 3. Response time risk
        if equipment_correlations:
            avg_response_time = np.mean([c['temporal_distance_hours'] for c in equipment_correlations])
            response_risk = min(1.0, avg_response_time / 8)  # 8 hours = max risk
        else:
            response_risk = 1.0  # No correlations = maximum risk
        risk_factors['delayed_response_time'] = response_risk

        # 4. Equipment criticality (based on equipment type)
        criticality_map = {
            'generator': 0.9,
            'compressor': 0.8,
            'pump': 0.7,
            'motor': 0.6,
            'separator': 0.5
        }

        equipment_type = equipment.split('_')[0] if '_' in equipment else equipment
        risk_factors['critical_equipment'] = criticality_map.get(equipment_type, 0.5)

        # 5. Predictive risk (from ML models)
        if equipment in predictions:
            pred_risk = predictions[equipment]['predicted_anomalies'] / 5  # Normalize
            pred_confidence = predictions[equipment]['confidence']
            risk_factors['predicted_failures'] = min(1.0, pred_risk * pred_confidence)
        else:
            risk_factors['predicted_failures'] = 0.5  # Unknown = medium risk

        # 6. Trend risk (increasing anomaly trend)
        if len(equipment_anomalies) > 5:
            # Simple trend calculation
            timestamps = [pd.to_datetime(e['timestamp_start']) for e in equipment_anomalies]
            timestamps.sort()

            # Count anomalies in first and second half
            mid_point = timestamps[len(timestamps)//2]
            first_half = len([t for t in timestamps if t <= mid_point])
            second_half = len(timestamps) - first_half

            if first_half > 0:
                trend_risk = min(1.0, (second_half / first_half - 1) * 0.5)  # Increasing trend
            else:
                trend_risk = 0

            risk_factors['increasing_trend'] = max(0, trend_risk)
        else:
            risk_factors['increasing_trend'] = 0

        return risk_factors

    def _calculate_overall_risk_score(self, risk_factors: Dict[str, float]) -> float:
        """Calculate weighted overall risk score"""

        weights = self.config.risk_factors
        weighted_score = 0

        for factor, value in risk_factors.items():
            weight = weights.get(factor, 0.1)  # Default weight for unknown factors
            weighted_score += value * weight

        return min(1.0, weighted_score)

    def _classify_risk_level(self, score: float) -> str:
        """Classify risk level based on score"""

        if score >= 0.7:
            return 'high'
        elif score >= 0.4:
            return 'medium'
        else:
            return 'low'

    def _generate_risk_recommendations(self, equipment: str, risk_factors: Dict[str, float]) -> List[str]:
        """Generate specific recommendations based on risk factors"""

        recommendations = []

        # High anomaly frequency
        if risk_factors.get('anomaly_frequency', 0) > 0.6:
            recommendations.append(f"Increase monitoring frequency for {equipment} - high anomaly rate detected")

        # Poor correlation coverage
        if risk_factors.get('poor_correlation_coverage', 0) > 0.5:
            recommendations.append(f"Improve operator logging procedures for {equipment} - many anomalies not documented")

        # Slow response times
        if risk_factors.get('delayed_response_time', 0) > 0.6:
            recommendations.append(f"Reduce response time to anomalies on {equipment} - current average too high")

        # Critical equipment
        if risk_factors.get('critical_equipment', 0) > 0.8:
            recommendations.append(f"Prioritize {equipment} maintenance - critical to operations")

        # Increasing trend
        if risk_factors.get('increasing_trend', 0) > 0.3:
            recommendations.append(f"Investigate root cause of increasing anomaly trend on {equipment}")

        # Predictive risk
        if risk_factors.get('predicted_failures', 0) > 0.6:
            recommendations.append(f"Schedule preventive maintenance for {equipment} - high failure probability predicted")

        return recommendations

class ReportGenerator:
    """
    Generate comprehensive reports in multiple formats
    """

    def __init__(self, config: InsightGenerationConfig):
        self.config = config

    def generate_comprehensive_report(self,
                                    anomaly_events: List[Dict],
                                    correlations: List[Dict],
                                    patterns: Dict,
                                    predictions: Dict[str, Any],
                                    risk_scores: Dict[str, Any],
                                    llm_insights: Dict[str, str]) -> Dict[str, Any]:
        """Generate comprehensive analysis report"""

        report_data = {
            'metadata': self._generate_report_metadata(),
            'executive_summary': self._generate_executive_summary(
                anomaly_events, correlations, risk_scores, llm_insights
            ),
            'analysis_results': {
                'anomaly_analysis': self._analyze_anomaly_results(anomaly_events),
                'correlation_analysis': self._analyze_correlation_results(correlations, patterns),
                'predictive_analysis': self._analyze_prediction_results(predictions),
                'risk_analysis': self._analyze_risk_results(risk_scores)
            },
            'recommendations': self._generate_recommendations(risk_scores, predictions),
            'visualizations': self._prepare_visualization_data(anomaly_events, correlations, predictions),
            'llm_insights': llm_insights,
            'appendix': {
                'raw_data_summary': self._generate_data_summary(anomaly_events, correlations),
                'methodology': self._generate_methodology_summary(),
                'limitations': self._generate_limitations()
            }
        }

        return report_data

    def _generate_report_metadata(self) -> Dict[str, Any]:
        """Generate report metadata"""

        return {
            'generated_at': datetime.now().isoformat(),
            'report_type': 'Oil Rig Anomaly Analysis',
            'version': '1.0',
            'analysis_modules': ['Data Preprocessing', 'Anomaly Detection', 'Text Correlation', 'Insight Generation'],
            'config': {
                'prediction_horizon': self.config.prediction_horizon_days,
                'confidence_threshold': self.config.confidence_threshold,
                'report_format': self.config.report_format
            }
        }

    def _generate_executive_summary(self, anomaly_events: List[Dict],
                                   correlations: List[Dict],
                                   risk_scores: Dict[str, Any],
                                   llm_insights: Dict[str, str]) -> Dict[str, Any]:
        """Generate executive summary"""

        # Calculate key metrics
        total_anomalies = len(anomaly_events)
        total_correlations = len(correlations)
        coverage_rate = total_correlations / max(1, total_anomalies)

        # Risk distribution
        risk_distribution = Counter()
        for equipment, risk_data in risk_scores.items():
            risk_distribution[risk_data['risk_level']] += 1

        # High-risk equipment
        high_risk_equipment = [
            equipment for equipment, risk_data in risk_scores.items()
            if risk_data['risk_level'] == 'high'
        ]

        # Severity distribution
        severity_counts = Counter([event['severity'] for event in anomaly_events])

        return {
            'overall_status': self._determine_overall_status(risk_distribution),
            'key_metrics': {
                'total_anomalies': total_anomalies,
                'correlation_coverage': f"{coverage_rate:.1%}",
                'high_risk_equipment_count': len(high_risk_equipment),
                'critical_anomalies': severity_counts.get('high', 0)
            },
            'risk_distribution': dict(risk_distribution),
            'high_risk_equipment': high_risk_equipment,
            'top_recommendations': self._extract_top_recommendations(risk_scores),
            'llm_summary': llm_insights.get('executive_summary', 'LLM summary not available')
        }

    def _determine_overall_status(self, risk_distribution: Counter) -> str:
        """Determine overall operational status"""

        if risk_distribution.get('high', 0) > 2:
            return 'RED - Multiple High-Risk Equipment'
        elif risk_distribution.get('high', 0) > 0 or risk_distribution.get('medium', 0) > 3:
            return 'YELLOW - Monitoring Required'
        else:
            return 'GREEN - Normal Operations'

    def _extract_top_recommendations(self, risk_scores: Dict[str, Any]) -> List[str]:
        """Extract top recommendations across all equipment"""

        all_recommendations = []
        for equipment, risk_data in risk_scores.items():
            all_recommendations.extend(risk_data['recommendations'])

        # Count frequency and return most common
        rec_counts = Counter(all_recommendations)
        return [rec for rec, count in rec_counts.most_common(5)]

    def _analyze_anomaly_results(self, anomaly_events: List[Dict]) -> Dict[str, Any]:
        """Analyze anomaly detection results"""

        if not anomaly_events:
            return {'summary': 'No anomalies detected'}

        # Equipment distribution
        equipment_counts = Counter([event['equipment'] for event in anomaly_events])

        # Severity distribution
        severity_counts = Counter([event['severity'] for event in anomaly_events])

        # Temporal analysis
        timestamps = [pd.to_datetime(event['timestamp_start']) for event in anomaly_events]
        hourly_dist = Counter([ts.hour for ts in timestamps])
        daily_dist = Counter([ts.dayofweek for ts in timestamps])

        # Duration analysis
        durations = [event['duration_minutes'] for event in anomaly_events]

        return {
            'total_anomalies': len(anomaly_events),
            'equipment_distribution': dict(equipment_counts),
            'severity_distribution': dict(severity_counts),
            'temporal_patterns': {
                'peak_hour': max(hourly_dist.keys(), key=lambda k: hourly_dist[k]),
                'peak_day': max(daily_dist.keys(), key=lambda k: daily_dist[k]),
                'hourly_distribution': dict(hourly_dist)
            },
            'duration_stats': {
                'avg_duration_minutes': np.mean(durations),
                'max_duration_minutes': max(durations),
                'min_duration_minutes': min(durations)
            }
        }

    def _analyze_correlation_results(self, correlations: List[Dict], patterns: Dict) -> Dict[str, Any]:
        """Analyze correlation results"""

        if not correlations:
            return {'summary': 'No correlations found'}

        # Basic statistics
        semantic_similarities = [c['semantic_similarity'] for c in correlations]
        correlation_scores = [c['correlation_score'] for c in correlations]
        response_times = [c['temporal_distance_hours'] for c in correlations]

        return {
            'total_correlations': len(correlations),
            'quality_metrics': {
                'avg_semantic_similarity': np.mean(semantic_similarities),
                'avg_correlation_score': np.mean(correlation_scores),
                'avg_response_time_hours': np.mean(response_times)
            },
            'patterns': patterns,
            'top_correlations': sorted(correlations, key=lambda x: x['correlation_score'], reverse=True)[:5]
        }

    def _analyze_prediction_results(self, predictions: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze prediction results"""

        if not predictions:
            return {'summary': 'No predictions available'}

        # Risk level distribution
        risk_levels = Counter([pred['risk_level'] for pred in predictions.values()])

        # Confidence analysis
        confidences = [pred['confidence'] for pred in predictions.values()]

        # High-risk predictions
        high_risk_predictions = {
            equipment: pred for equipment, pred in predictions.items()
            if pred['risk_level'] == 'high'
        }

        return {
            'total_predictions': len(predictions),
            'risk_distribution': dict(risk_levels),
            'confidence_stats': {
                'avg_confidence': np.mean(confidences),
                'min_confidence': min(confidences),
                'max_confidence': max(confidences)
            },
            'high_risk_equipment': high_risk_predictions
        }

    def _analyze_risk_results(self, risk_scores: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze risk assessment results"""

        if not risk_scores:
            return {'summary': 'No risk scores calculated'}

        # Overall risk distribution
        risk_levels = Counter([risk['risk_level'] for risk in risk_scores.values()])
        overall_scores = [risk['overall_score'] for risk in risk_scores.values()]

        # Highest risk equipment
        highest_risk = max(risk_scores.items(), key=lambda x: x[1]['overall_score'])

        return {
            'risk_distribution': dict(risk_levels),
            'score_stats': {
                'avg_risk_score': np.mean(overall_scores),
                'max_risk_score': max(overall_scores),
                'min_risk_score': min(overall_scores)
            },
            'highest_risk_equipment': {
                'equipment': highest_risk[0],
                'score': highest_risk[1]['overall_score'],
                'level': highest_risk[1]['risk_level']
            }
        }

    def _generate_recommendations(self, risk_scores: Dict[str, Any],
                                 predictions: Dict[str, Any]) -> Dict[str, List[str]]:
        """Generate comprehensive recommendations"""

        recommendations = {
            'immediate_actions': [],
            'short_term': [],
            'long_term': [],
            'monitoring': []
        }

        # Extract from risk assessments
        for equipment, risk_data in risk_scores.items():
            if risk_data['risk_level'] == 'high':
                recommendations['immediate_actions'].extend([
                    f"Immediate inspection required for {equipment}",
                    f"Consider temporary shutdown of {equipment} if anomaly rate continues"
                ])

            recommendations['short_term'].extend(risk_data['recommendations'])

        # Extract from predictions
        for equipment, pred_data in predictions.items():
            if pred_data['risk_level'] == 'high':
                recommendations['monitoring'].append(
                    f"Increase monitoring frequency for {equipment} - high failure probability"
                )

        # Generic long-term recommendations
        recommendations['long_term'] = [
            "Implement predictive maintenance program",
            "Upgrade monitoring systems for better anomaly detection",
            "Improve operator training on anomaly identification and reporting",
            "Consider equipment modernization for frequently problematic units"
        ]

        # Remove duplicates and limit recommendations
        for category in recommendations:
            recommendations[category] = list(set(recommendations[category]))[:10]

        return recommendations

    def _prepare_visualization_data(self, anomaly_events: List[Dict],
                                   correlations: List[Dict],
                                   predictions: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare data for visualizations"""

        return {
            'anomaly_timeline': [
                {
                    'timestamp': event['timestamp_start'],
                    'equipment': event['equipment'],
                    'severity': event['severity'],
                    'duration': event['duration_minutes']
                }
                for event in anomaly_events
            ],
            'correlation_scatter': [
                {
                    'semantic_similarity': corr['semantic_similarity'],
                    'temporal_distance': corr['temporal_distance_hours'],
                    'correlation_score': corr['correlation_score'],
                    'equipment': corr['equipment']
                }
                for corr in correlations
            ],
            'prediction_bar': [
                {
                    'equipment': equipment,
                    'predicted_anomalies': pred['predicted_anomalies'],
                    'confidence': pred['confidence'],
                    'risk_level': pred['risk_level']
                }
                for equipment, pred in predictions.items()
            ]
        }

    def _generate_data_summary(self, anomaly_events: List[Dict], correlations: List[Dict]) -> Dict[str, Any]:
        """Generate summary of raw data"""

        return {
            'data_sources': [
                'Sensor data (preprocessed)',
                'Operator logs (processed)',
                'Anomaly detection results',
                'Text correlation analysis'
            ],
            'data_quality': {
                'anomaly_events': len(anomaly_events),
                'correlations': len(correlations),
                'coverage_rate': len(correlations) / max(1, len(anomaly_events))
            },
            'time_range': {
                'start': min([event['timestamp_start'] for event in anomaly_events]) if anomaly_events else 'N/A',
                'end': max([event['timestamp_start'] for event in anomaly_events]) if anomaly_events else 'N/A'
            }
        }

    def _generate_methodology_summary(self) -> List[str]:
        """Generate methodology summary"""

        return [
            "1. Data Preprocessing: Cleaned and normalized sensor data, processed operator logs",
            "2. Anomaly Detection: Applied Isolation Forest, LSTM Autoencoder, and statistical methods",
            "3. Text Correlation: Used BERT embeddings and semantic similarity analysis",
            "4. Predictive Modeling: Trained machine learning models for failure prediction",
            "5. Risk Assessment: Calculated weighted risk scores based on multiple factors",
            "6. LLM Analysis: Generated insights using large language models"
        ]

    def _generate_limitations(self) -> List[str]:
        """Generate analysis limitations"""

        return [
            "Analysis based on synthetic data - real-world performance may vary",
            "Predictive models require more historical data for improved accuracy",
            "Text correlation depends on operator logging quality and consistency",
            "Risk assessments are estimates based on available data patterns",
            "LLM insights are AI-generated and should be validated by domain experts"
        ]

    def export_report(self, report_data: Dict[str, Any], filepath: str = None) -> str:
        """Export report in specified format"""

        if self.config.report_format == 'html':
            return self._export_html_report(report_data, filepath)
        elif self.config.report_format == 'json':
            return self._export_json_report(report_data, filepath)
        else:
            return self._export_text_report(report_data, filepath)

    def _export_html_report(self, report_data: Dict[str, Any], filepath: str = None) -> str:
        """Export HTML report"""

        html_template = """
<!DOCTYPE html>
<html>
<head>
    <title>Oil Rig Anomaly Analysis Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .header { background-color: #2c3e50; color: white; padding: 20px; text-align: center; }
        .section { margin: 20px 0; padding: 15px; border-left: 4px solid #3498db; }
        .executive-summary { background-color: #f8f9fa; padding: 20px; border-radius: 5px; }
        .status-green { color: #27ae60; font-weight: bold; }
        .status-yellow { color: #f39c12; font-weight: bold; }
        .status-red { color: #e74c3c; font-weight: bold; }
        .recommendations { background-color: #fff3cd; padding: 15px; border-radius: 5px; }
        table { width: 100%; border-collapse: collapse; margin: 10px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
    </style>
</head>
<body>
    <div class="header">
        <h1>Oil Rig Anomaly Analysis Report</h1>
        <p>Generated: {{ metadata.generated_at }}</p>
    </div>

    <div class="executive-summary">
        <h2>Executive Summary</h2>
        <p><strong>Overall Status:</strong>
            <span class="status-{{ executive_summary.overall_status.split()[0].lower() }}">
                {{ executive_summary.overall_status }}
            </span>
        </p>

        <h3>Key Metrics</h3>
        <ul>
            <li>Total Anomalies: {{ executive_summary.key_metrics.total_anomalies }}</li>
            <li>Correlation Coverage: {{ executive_summary.key_metrics.correlation_coverage }}</li>
            <li>High-Risk Equipment: {{ executive_summary.key_metrics.high_risk_equipment_count }}</li>
            <li>Critical Anomalies: {{ executive_summary.key_metrics.critical_anomalies }}</li>
        </ul>

        {% if executive_summary.high_risk_equipment %}
        <h3>High-Risk Equipment</h3>
        <ul>
            {% for equipment in executive_summary.high_risk_equipment %}
            <li>{{ equipment }}</li>
            {% endfor %}
        </ul>
        {% endif %}
    </div>

    <div class="section">
        <h2>Analysis Results</h2>

        <h3>Anomaly Analysis</h3>
        <p>Total anomalies detected: {{ analysis_results.anomaly_analysis.total_anomalies }}</p>

        <h3>Risk Analysis</h3>
        <p>Risk distribution across equipment types shows varying levels of concern.</p>

        <h3>Predictive Analysis</h3>
        <p>Machine learning models provide {{ analysis_results.predictive_analysis.total_predictions }} equipment predictions.</p>
    </div>

    <div class="recommendations">
        <h2>Recommendations</h2>

        <h3>Immediate Actions</h3>
        <ul>
            {% for rec in recommendations.immediate_actions %}
            <li>{{ rec }}</li>
            {% endfor %}
        </ul>

        <h3>Short-term Actions</h3>
        <ul>
            {% for rec in recommendations.short_term[:5] %}
            <li>{{ rec }}</li>
            {% endfor %}
        </ul>
    </div>

    <div class="section">
        <h2>LLM Insights</h2>
        {% for insight_type, insight_text in llm_insights.items() %}
        <h3>{{ insight_type.replace('_', ' ').title() }}</h3>
        <pre>{{ insight_text }}</pre>
        {% endfor %}
    </div>
</body>
</html>
        """

        from jinja2 import Template
        template = Template(html_template)
        html_content = template.render(**report_data)

        if filepath:
            with open(filepath, 'w') as f:
                f.write(html_content)
            return filepath

        return html_content

    def _export_json_report(self, report_data: Dict[str, Any], filepath: str = None) -> str:
        """Export JSON report"""

        # Convert datetime objects to strings for JSON serialization
        json_data = json.dumps(report_data, default=str, indent=2)

        if filepath:
            with open(filepath, 'w') as f:
                f.write(json_data)
            return filepath

        return json_data

    def _export_text_report(self, report_data: Dict[str, Any], filepath: str = None) -> str:
        """Export simple text report"""

        lines = [
            "="*60,
            "OIL RIG ANOMALY ANALYSIS REPORT",
            "="*60,
            f"Generated: {report_data['metadata']['generated_at']}",
            "",
            "EXECUTIVE SUMMARY",
            "-"*20,
            f"Overall Status: {report_data['executive_summary']['overall_status']}",
            f"Total Anomalies: {report_data['executive_summary']['key_metrics']['total_anomalies']}",
            f"Correlation Coverage: {report_data['executive_summary']['key_metrics']['correlation_coverage']}",
            "",
            "TOP RECOMMENDATIONS",
            "-"*20
        ]

        for i, rec in enumerate(report_data['recommendations']['immediate_actions'][:5], 1):
            lines.append(f"{i}. {rec}")

        lines.extend([
            "",
            "DETAILED ANALYSIS",
            "-"*20,
            f"Anomaly Analysis: {report_data['analysis_results']['anomaly_analysis'].get('total_anomalies', 0)} anomalies detected",
            f"Risk Analysis: Risk assessment completed for all equipment",
            f"Predictive Analysis: {report_data['analysis_results']['predictive_analysis'].get('total_predictions', 0)} predictions generated",
            "",
            "="*60
        ])

        text_content = "\n".join(lines)

        if filepath:
            with open(filepath, 'w') as f:
                f.write(text_content)
            return filepath

        return text_content

class OilRigInsightGenerator:
    """
    Main orchestrator for comprehensive insight generation
    """

    def __init__(self, config: Optional[InsightGenerationConfig] = None):
        self.config = config or InsightGenerationConfig()

        # Initialize components
        self.llm_generator = LLMInsightGenerator(self.config)
        self.predictive_analyzer = PredictiveAnalyzer(self.config)
        self.risk_assessor = RiskAssessmentAnalyzer(self.config)
        self.report_generator = ReportGenerator(self.config)

        # Results storage
        self.analysis_results = {}
        self.insights = {}
        self.report_data = {}

        # Processing log
        self.processing_log = []

    def log_step(self, step_name: str, details: str):
        """Log processing steps"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.processing_log.append({
            'timestamp': timestamp,
            'step': step_name,
            'details': details
        })
        print(f"[{timestamp}] {step_name}: {details}")

    async def generate_comprehensive_insights(self,
                                            anomaly_events: List[Dict],
                                            correlations: List[Dict],
                                            patterns: Dict,
                                            processed_logs: pd.DataFrame) -> Dict[str, Any]:
        """Generate comprehensive insights using all analysis methods"""

        self.log_step("INSIGHT_GENERATION_START", f"Generating insights for {len(anomaly_events)} anomalies and {len(correlations)} correlations")

        # 1. Prepare data for predictive analysis
        self.log_step("PREDICTIVE_ANALYSIS", "Preparing time series data and training models")

        ts_data = self.predictive_analyzer.prepare_time_series_data(correlations, anomaly_events)
        model_results = self.predictive_analyzer.train_predictive_models(ts_data)
        predictions = self.predictive_analyzer.generate_predictions(ts_data)

        self.log_step("PREDICTIVE_ANALYSIS", f"Generated predictions for {len(predictions)} equipment units")

        # 2. Risk assessment
        self.log_step("RISK_ASSESSMENT", "Calculating comprehensive risk scores")

        risk_scores = self.risk_assessor.calculate_equipment_risk_scores(
            anomaly_events, correlations, predictions
        )

        self.log_step("RISK_ASSESSMENT", f"Calculated risk scores for {len(risk_scores)} equipment units")

        # 3. LLM-based insights
        self.log_step("LLM_INSIGHTS", "Generating AI-powered insights")

        llm_insights = await self._generate_llm_insights(
            anomaly_events, correlations, patterns, predictions, risk_scores
        )

        self.log_step("LLM_INSIGHTS", f"Generated {len(llm_insights)} types of AI insights")

        # 4. Generate comprehensive report
        self.log_step("REPORT_GENERATION", "Compiling comprehensive analysis report")

        report_data = self.report_generator.generate_comprehensive_report(
            anomaly_events, correlations, patterns, predictions, risk_scores, llm_insights
        )

        self.log_step("REPORT_GENERATION", "Comprehensive report compiled successfully")

        # 5. Store results
        self.analysis_results = {
            'predictive_analysis': {
                'time_series_data': ts_data,
                'model_results': model_results,
                'predictions': predictions
            },
            'risk_assessment': risk_scores,
            'llm_insights': llm_insights,
            'report_data': report_data
        }

        self.log_step("INSIGHT_GENERATION_COMPLETE", "All insights generated successfully")

        return self.analysis_results

    async def _generate_llm_insights(self,
                                   anomaly_events: List[Dict],
                                   correlations: List[Dict],
                                   patterns: Dict,
                                   predictions: Dict[str, Any],
                                   risk_scores: Dict[str, Any]) -> Dict[str, str]:
        """Generate insights using LLMs"""

        llm_insights = {}

        # Prepare context data for each insight type
        contexts = {
            'anomaly_analysis': {
                'anomaly_summary': f"Total: {len(anomaly_events)} anomalies detected",
                'correlation_summary': f"Correlations: {len(correlations)} anomaly-log correlations found",
                'equipment_patterns': str(patterns.get('equipment_patterns', {}))
            },
            'predictive_maintenance': {
                'historical_patterns': str(patterns),
                'current_status': f"{len(predictions)} equipment units analyzed",
                'correlation_insights': f"Average response time: {np.mean([c['temporal_distance_hours'] for c in correlations]):.1f} hours" if correlations else "No correlations"
            },
            'risk_assessment': {
                'risk_indicators': str({eq: data['risk_level'] for eq, data in risk_scores.items()}),
                'anomaly_patterns': f"{len(anomaly_events)} anomalies across {len(set([e['equipment'] for e in anomaly_events]))} equipment units",
                'operational_context': f"Correlation coverage: {len(correlations)/max(1, len(anomaly_events)):.1%}"
            },
            'executive_summary': {
                'analysis_results': f"Comprehensive analysis of {len(anomaly_events)} anomalies",
                'key_metrics': str({
                    'anomalies': len(anomaly_events),
                    'correlations': len(correlations),
                    'predictions': len(predictions),
                    'risk_assessments': len(risk_scores)
                }),
                'business_impact': f"Risk assessment completed for {len(risk_scores)} equipment units"
            },
            'operator_insights': {
                'operator_analysis': f"Analysis of operator communication patterns",
                'correlations': f"{len(correlations)} anomaly-log correlations found",
                'communication_patterns': str(patterns.get('text_patterns', {}))
            }
        }

        # Generate insights for each type
        for insight_type, context in contexts.items():
            try:
                insight = await self.llm_generator.generate_insight(insight_type, context)
                llm_insights[insight_type] = insight
            except Exception as e:
                print(f"Error generating {insight_type} insight: {e}")
                llm_insights[insight_type] = f"Error generating insight: {e}"

        return llm_insights

    def export_insights_report(self, filepath: str = None) -> str:
        """Export comprehensive insights report"""

        if not self.analysis_results:
            raise ValueError("No analysis results available. Run generate_comprehensive_insights() first.")

        report_data = self.analysis_results['report_data']

        # Generate filename if not provided
        if not filepath:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            extension = self.config.report_format
            filepath = f"oil_rig_analysis_report_{timestamp}.{extension}"

        # Export report
        exported_path = self.report_generator.export_report(report_data, filepath)

        self.log_step("EXPORT_COMPLETE", f"Report exported to: {exported_path}")

        return exported_path

    def get_key_insights_summary(self) -> Dict[str, Any]:
        """Get summary of key insights for quick review"""

        if not self.analysis_results:
            return {}

        report_data = self.analysis_results['report_data']

        return {
            'overall_status': report_data['executive_summary']['overall_status'],
            'total_anomalies': report_data['executive_summary']['key_metrics']['total_anomalies'],
            'high_risk_equipment': report_data['executive_summary']['high_risk_equipment'],
            'top_recommendations': report_data['executive_summary']['top_recommendations'],
            'prediction_summary': {
                equipment: pred['risk_level']
                for equipment, pred in self.analysis_results['predictive_analysis']['predictions'].items()
            },
            'processing_steps_completed': len(self.processing_log)
        }

# ============================================================================
# EXAMPLE USAGE AND TESTING
# ============================================================================

async def test_insight_generation_pipeline(anomaly_events: List[Dict],
                                          correlations: List[Dict],
                                          patterns: Dict,
                                          processed_logs: pd.DataFrame):
    """
    Test the complete insight generation pipeline
    """
    print("Testing Oil Rig Insight Generation Pipeline")
    print("=" * 60)

    # Initialize configuration (without API keys for demo)
    config = InsightGenerationConfig()
    config.openai_api_key = None  # Set to your API key if available
    config.anthropic_api_key = None  # Set to your API key if available

    # Initialize insight generator
    generator = OilRigInsightGenerator(config)

    # Generate comprehensive insights
    results = await generator.generate_comprehensive_insights(
        anomaly_events, correlations, patterns, processed_logs
    )

    # Get key insights summary
    summary = generator.get_key_insights_summary()

    # Export report
    report_path = generator.export_insights_report()

    # Display results
    print("\nInsight Generation Results:")
    print(f"Overall Status: {summary.get('overall_status', 'N/A')}")
    print(f"Total Anomalies: {summary.get('total_anomalies', 0)}")
    print(f"High-Risk Equipment: {len(summary.get('high_risk_equipment', []))}")
    print(f"Processing Steps: {summary.get('processing_steps_completed', 0)}")
    print(f"Report exported to: {report_path}")

    return generator, results, summary



# From Module 1
processed_operator_logs = pd.read_csv('processed_operator_logs.csv')
processed_operator_logs['timestamp'] = pd.to_datetime(processed_operator_logs['timestamp'])

# From Module 2
import json
with open('anomaly_detection_results.json', 'r') as f:
    anomaly_events = json.load(f)

# From Module 3
correlations =[]
# with open('anomaly_text_correlations.csv') as f:
#     correlations_df = pd.read_csv(f)
#     correlations = correlations_df.to_dict('records')
patterns=[]
# with open('text_processing_results.pkl', 'rb') as f:
#     import pickle
#     text_results = pickle.load(f)
#     patterns = text_results.get('patterns', {})

print(f"📊 Loaded data successfully:")
print(f"   Operator logs: {len(processed_operator_logs)}")
print(f"   Anomaly events: {len(anomaly_events)}")
print(f"   Correlations: {len(correlations)}")
print(f"   Text patterns: {len(patterns)} categories")

def load_your_anomaly_data(json_file="anomaly_events.json"):
    """
    Load your specific anomaly events JSON to DataFrame
    """

    print("Loading anomaly events JSON...")

    # Try different loading methods
    try:
        # Method 1: Direct load as list of dicts
        with open(json_file, 'r') as f:
            data = json.load(f)

        if isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict):
            # Handle different dictionary structures
            if 'anomaly_events' in data:
                df = pd.DataFrame(data['anomaly_events'])
            elif 'results' in data:
                df = pd.DataFrame(data['results'])
            else:
                # Try to find the list of dictionaries
                for key, value in data.items():
                    if isinstance(value, list) and len(value) > 0 and isinstance(value[0], dict):
                        df = pd.DataFrame(value)
                        break
                else:
                    # Convert single dict to single-row DataFrame
                    df = pd.DataFrame([data])

        print(f"✅ Successfully loaded DataFrame with {len(df)} rows and {len(df.columns)} columns")

        if 'equipment' in df.columns:
            machines = df['equipment'].unique()
            print(f"📊 Machines in data: {list(machines)}")
            print(f"📈 Records per machine:")
            print(df['equipment'].value_counts())

        print(f"🔍 DataFrame info:")
        print(df.info())

        return df

    except Exception as e:
        print(f"❌ Error loading JSON: {e}")
        return None

df = load_your_anomaly_data("anomaly_detection_results.json")
display(df)

# Create configuration for optimal performance
config = InsightGenerationConfig()

# LLM settings (will use template-based insights if no API keys)
config.openai_model = "gpt-3.5-turbo"  # Cost-effective model
config.anthropic_model = "claude-3-sonnet-20240229"
config.max_tokens = 1500
config.temperature = 0.3  # More focused, less creative

# Analysis settings
config.risk_assessment_enabled = True
config.predictive_modeling_enabled = True
config.automated_recommendations = True
config.maintenance_planning = True

# Report settings
config.report_format = "html"  # Easy to view in Colab
config.include_visualizations = True
config.executive_summary = True
config.detailed_analysis = True

# Prediction settings
config.prediction_horizon_days = 30
config.confidence_threshold = 0.7

# Initialize the insight generator
generator = OilRigInsightGenerator(config)

print("🧠 Starting comprehensive insight generation...")
print("   This may take 3-5 minutes depending on data size and LLM availability")

# Generate comprehensive insights (async function)
import asyncio

async def run_insight_generation():
    results = await generator.generate_comprehensive_insights(
        anomaly_events=anomaly_events,
        correlations=correlations,
        patterns=patterns,
        processed_logs=processed_operator_logs
    )
    return results

# Run the async function
results = await run_insight_generation()

print("✅ Insight generation complete!")

# Get quick summary
summary = generator.get_key_insights_summary()
print(f"\n📈 Key Results:")
print(f"   Overall Status: {summary.get('overall_status', 'N/A')}")
print(f"   Total Anomalies Analyzed: {summary.get('total_anomalies', 0)}")
print(f"   High-Risk Equipment: {len(summary.get('high_risk_equipment', []))}")

print("🔮 PREDICTIVE ANALYSIS RESULTS")
print("="*50)

predictions = results['predictive_analysis']['predictions']

if predictions:
    print(f"Generated predictions for {len(predictions)} equipment units:\n")

    # Display predictions by risk level
    for risk_level in ['high', 'medium', 'low']:
        equipment_list = [
            eq for eq, pred in predictions.items()
            if pred['risk_level'] == risk_level
        ]

        if equipment_list:
            print(f"🚨 {risk_level.upper()} RISK ({len(equipment_list)} units):")
            for eq in equipment_list:
                pred = predictions[eq]
                print(f"   • {eq.replace('_', ' ').title()}")
                print(f"     Predicted anomalies: {pred['predicted_anomalies']:.1f}")
                print(f"     Confidence: {pred['confidence']:.1%}")
                print(f"     Model used: {pred['model_used']}")
            print()
else:
    print("⚠️ No predictions generated - insufficient historical data")

risk_scores = results['risk_assessment']

if risk_scores:
    # Sort equipment by risk score
    sorted_equipment = sorted(
        risk_scores.items(),
        key=lambda x: x[1]['overall_score'],
        reverse=True
    )

    print(f"Risk assessment completed for {len(risk_scores)} equipment units:\n")

    for equipment, risk_data in sorted_equipment:
        score = risk_data['overall_score']
        level = risk_data['risk_level']

        # Color coding for display
        if level == 'high':
            emoji = "🔴"
        elif level == 'medium':
            emoji = "🟡"
        else:
            emoji = "🟢"

        print(f"{emoji} {equipment.replace('_', ' ').title()}")
        print(f"   Risk Score: {score:.3f} ({level.upper()})")

        # Show top risk factors
        factors = risk_data['risk_factors']
        top_factors = sorted(factors.items(), key=lambda x: x[1], reverse=True)[:3]
        print(f"   Top Risk Factors:")
        for factor, value in top_factors:
            print(f"     • {factor.replace('_', ' ').title()}: {value:.3f}")

        # Show recommendations
        recommendations = risk_data['recommendations'][:2]  # Top 2
        if recommendations:
            print(f"   Key Recommendations:")
            for rec in recommendations:
                print(f"     • {rec}")
        print()

print("🤖 AI-GENERATED INSIGHTS")
print("="*50)

llm_insights = results['llm_insights']

for insight_type, insight_text in llm_insights.items():
    print(f"\n📋 {insight_type.replace('_', ' ').upper()}")
    print("-" * 40)

    # Display first 500 characters of each insight
    if len(insight_text) > 500:
        print(f"{insight_text[:500]}...")
        print(f"[Truncated - full insight in generated report]")
    else:
        print(insight_text)
    print()

print("📊 GENERATING COMPREHENSIVE REPORT")
print("="*50)

# Export the comprehensive report
report_path = generator.export_insights_report()

print(f"✅ Comprehensive report generated!")
print(f"📁 Report saved to: {report_path}")

# Display report metadata
report_data = results['report_data']
metadata = report_data['metadata']