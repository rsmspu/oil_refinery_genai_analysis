# -*- coding: utf-8 -*-
"""03_Text_Processing_Correlation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TxoZI724pL3HWjbnbFdwbIc4fe2gr2fi
"""

##Install required packages
"""!pip install pandas numpy scipy scikit-learn
!pip install transformers sentence-transformers
!pip install torch torchvision torchaudio
!pip install nltk spacy textblob
!pip install bertopic umap-learn hdbscan
!pip install plotly seaborn matplotlib
!pip install wordcloud networkx
!python -m spacy download en_core_web_sm"""


# IMPORTS
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union, Any
import warnings
import json
import re
import string
from collections import Counter, defaultdict

# NLP and Text Processing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.chunk import ne_chunk
from nltk.tag import pos_tag

import spacy
from textblob import TextBlob

# BERT and Transformers
from transformers import AutoTokenizer, AutoModel, pipeline
from sentence_transformers import SentenceTransformer
import torch

# Topic Modeling
from bertopic import BERTopic
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import umap

# Similarity and Clustering
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.cluster import KMeans, DBSCAN
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import dendrogram, linkage

# Visualization
from wordcloud import WordCloud
import networkx as nx

# Suppress warnings
warnings.filterwarnings('ignore')
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('maxent_ne_chunker', quiet=True)
nltk.download('words', quiet=True)

class TextProcessingConfig:
    """Configuration class for text processing parameters"""

    def __init__(self):
        # Text preprocessing parameters
        self.remove_stopwords = True
        self.lemmatize = True
        self.min_word_length = 2
        self.max_word_length = 20
        self.remove_numbers = False  # Keep numbers for technical logs
        self.lowercase = True

        # BERT model parameters
        self.sentence_transformer_model = 'all-MiniLM-L6-v2'  # Lightweight but good
        self.bert_model = 'distilbert-base-uncased'  # Fast and efficient
        self.max_sequence_length = 512
        self.batch_size = 16

        # Similarity analysis parameters
        self.similarity_threshold = 0.7  # Minimum similarity for correlation
        self.time_window_hours = 4  # Time window for correlating anomalies with logs
        self.min_correlation_score = 0.1

        # Topic modeling parameters
        self.topic_model_params = {
            'n_topics': 10,  # For LDA
            'min_topic_size': 5,  # For BERTopic
            'nr_topics': 15,  # For BERTopic
            'top_k_words': 10,
            'diversity': 0.5
        }

        # Named Entity Recognition
        self.ner_model = 'en_core_web_sm'
        self.custom_entities = {
            'EQUIPMENT': ['pump', 'compressor', 'generator', 'motor', 'separator', 'valve', 'sensor'],
            'MEASUREMENT': ['pressure', 'temperature', 'vibration', 'flow', 'power', 'voltage', 'current'],
            'CONDITION': ['normal', 'high', 'low', 'alarm', 'trip', 'fault', 'failure', 'leak']
        }

        # Equipment mapping for text correlation
        self.equipment_mappings = {
            'pump': ['centrifugal_pump_01', 'pump_01', 'pump'],
            'compressor': ['gas_compressor_01', 'compressor_01', 'compressor'],
            'motor': ['drilling_motor_01', 'motor_01', 'motor'],
            'generator': ['generator_01', 'gen_01', 'generator'],
            'separator': ['separator_01', 'sep_01', 'separator']
        }

class TextPreprocessor:
    """
    Comprehensive text preprocessing for operator logs
    """

    def __init__(self, config: TextProcessingConfig):
        self.config = config

        # Initialize NLTK components
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()

        # Add technical stop words that might not be useful
        technical_stopwords = {
            'system', 'equipment', 'unit', 'reading', 'value', 'level', 'status',
            'operation', 'running', 'working', 'function', 'process'
        }
        if self.config.remove_stopwords:
            self.stop_words.update(technical_stopwords)

        # Initialize spaCy model for advanced NLP
        try:
            self.nlp = spacy.load(self.config.ner_model)
        except OSError:
            print(f"spaCy model {self.config.ner_model} not found. Using basic preprocessing only.")
            self.nlp = None

        # Preprocessing statistics
        self.preprocessing_stats = {}

    def clean_text(self, text: str) -> str:
        """Basic text cleaning"""
        if pd.isna(text) or not isinstance(text, str):
            return ""

        # Convert to lowercase
        if self.config.lowercase:
            text = text.lower()

        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        # Remove special characters but keep technical symbols
        # Keep: periods, commas, hyphens, underscores, forward slashes, parentheses
        text = re.sub(r'[^\w\s\.\,\-\_\/\(\)]', '', text)

        # Remove numbers if specified (usually keep for technical logs)
        if self.config.remove_numbers:
            text = re.sub(r'\d+', '', text)

        return text

    def tokenize_and_filter(self, text: str):
        """Tokenize text and apply filters"""
        if not text:
            return []

        # Tokenize
        tokens = word_tokenize(text)

        # Filter tokens
        filtered_tokens = []
        for token in tokens:
            # Skip if too short or too long
            if len(token) < self.config.min_word_length or len(token) > self.config.max_word_length:
                continue

            # Skip if punctuation only
            if token in string.punctuation:
                continue

            # Skip stopwords if enabled
            if self.config.remove_stopwords and token.lower() in self.stop_words:
                continue

            # Lemmatize if enabled
            if self.config.lemmatize:
                token = self.lemmatizer.lemmatize(token)

            filtered_tokens.append(token)

        return filtered_tokens

    def extract_technical_terms(self, text: str) -> Dict[str, List[str]]:
        """Extract technical terms using custom patterns and NER"""
        technical_terms = {
            'equipment': [],
            'measurements': [],
            'conditions': [],
            'values': [],
            'actions': []
        }

        if not text:
            return technical_terms

        # Extract using regex patterns
        # Equipment terms
        equipment_pattern = r'\b(?:pump|compressor|generator|motor|separator|valve|sensor|tank|vessel)\w*\b'
        technical_terms['equipment'].extend(re.findall(equipment_pattern, text, re.IGNORECASE))

        # Measurement terms
        measurement_pattern = r'\b(?:pressure|temperature|temp|vibration|vib|flow|power|voltage|current|rpm|torque)\b'
        technical_terms['measurements'].extend(re.findall(measurement_pattern, text, re.IGNORECASE))

        # Condition terms
        condition_pattern = r'\b(?:normal|high|low|alarm|alert|trip|fault|failure|fail|leak|noise|hot|cold|excessive)\b'
        technical_terms['conditions'].extend(re.findall(condition_pattern, text, re.IGNORECASE))

        # Numerical values with units
        value_pattern = r'\b\d+(?:\.\d+)?\s*(?:bar|psi|°c|°f|rpm|hz|kw|mw|v|a|mm/s|m³/h|l/min|%)\b'
        technical_terms['values'].extend(re.findall(value_pattern, text, re.IGNORECASE))

        # Action terms
        action_pattern = r'\b(?:check|inspect|replace|repair|maintain|service|clean|adjust|calibrate|start|stop|shutdown)\w*\b'
        technical_terms['actions'].extend(re.findall(action_pattern, text, re.IGNORECASE))

        # Use spaCy NER if available
        if self.nlp:
            doc = self.nlp(text)
            for ent in doc.ents:
                if ent.label_ in ['ORG', 'PRODUCT']:  # Organizations and products might be equipment
                    technical_terms['equipment'].append(ent.text)
                elif ent.label_ in ['QUANTITY', 'CARDINAL']:  # Quantities might be measurements
                    technical_terms['values'].append(ent.text)

        # Remove duplicates and convert to lowercase
        for key in technical_terms:
            technical_terms[key] = list(set([term.lower() for term in technical_terms[key]]))

        return technical_terms

    def preprocess_text_series(self, text_series: pd.Series) -> Dict[str, Any]:
        """Preprocess a pandas Series of text data"""
        results = {
            'original_texts': text_series.tolist(),
            'cleaned_texts': [],
            'tokens': [],
            'technical_terms': [],
            'text_lengths': [],
            'preprocessing_stats': {}
        }

        total_texts = len(text_series)

        for text in text_series:
            # Clean text
            cleaned = self.clean_text(str(text))
            results['cleaned_texts'].append(cleaned)
            results['text_lengths'].append(len(cleaned))

            # Tokenize
            tokens = self.tokenize_and_filter(cleaned)
            results['tokens'].append(tokens)

            # Extract technical terms
            tech_terms = self.extract_technical_terms(cleaned)
            results['technical_terms'].append(tech_terms)

        # Calculate preprocessing statistics
        results['preprocessing_stats'] = {
            'total_texts': total_texts,
            'empty_after_cleaning': sum(1 for text in results['cleaned_texts'] if not text),
            'avg_original_length': np.mean([len(str(text)) for text in text_series]),
            'avg_cleaned_length': np.mean(results['text_lengths']),
            'avg_tokens_per_text': np.mean([len(tokens) for tokens in results['tokens']]),
            'total_unique_tokens': len(set([token for tokens in results['tokens'] for token in tokens]))
        }

        return results

class BERTEmbeddingGenerator:
    """
    Generate BERT embeddings for operator logs
    """

    def __init__(self, config: TextProcessingConfig):
        self.config = config

        # Initialize sentence transformer (optimized for semantic similarity)
        print(f"Loading Sentence Transformer model: {config.sentence_transformer_model}")
        self.sentence_model = SentenceTransformer(config.sentence_transformer_model)

        # Initialize tokenizer and model for custom BERT usage
        self.tokenizer = None
        self.bert_model = None

        # Embedding cache for efficiency
        self.embedding_cache = {}

    def load_full_bert_model(self):
        """Load full BERT model for custom processing"""
        if self.tokenizer is None:
            print(f"Loading BERT model: {self.config.bert_model}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.bert_model)
            self.bert_model = AutoModel.from_pretrained(self.config.bert_model)

            if torch.cuda.is_available():
                self.bert_model = self.bert_model.cuda()
                print("BERT model moved to GPU")

    def generate_sentence_embeddings(self, texts: List[str], show_progress: bool = True) -> np.ndarray:
        """Generate embeddings using SentenceTransformer (recommended)"""
        if not texts:
            return np.array([])

        # Filter out empty texts
        non_empty_texts = [text if text and text.strip() else "empty text" for text in texts]

        print(f"Generating embeddings for {len(non_empty_texts)} texts...")

        # Generate embeddings in batches
        embeddings = self.sentence_model.encode(
            non_empty_texts,
            batch_size=self.config.batch_size,
            show_progress_bar=show_progress,
            convert_to_numpy=True
        )

        return embeddings

    def generate_bert_embeddings(self, texts: List[str]) -> np.ndarray:
        """Generate embeddings using custom BERT model processing"""
        if not self.bert_model:
            self.load_full_bert_model()

        embeddings = []

        for i, text in enumerate(texts):
            if i % 50 == 0:
                print(f"Processing text {i+1}/{len(texts)}")

            if not text or not text.strip():
                # Use zero embedding for empty texts
                embeddings.append(np.zeros(768))  # BERT base dimension
                continue

            # Tokenize and encode
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=self.config.max_sequence_length
            )

            # Move to GPU if available
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}

            # Generate embeddings
            with torch.no_grad():
                outputs = self.bert_model(**inputs)
                # Use [CLS] token embedding
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()
                embeddings.append(embedding)

        return np.array(embeddings)

    def compute_similarity_matrix(self, embeddings: np.ndarray) -> np.ndarray:
        """Compute cosine similarity matrix between embeddings"""
        if len(embeddings) == 0:
            return np.array([])

        similarity_matrix = cosine_similarity(embeddings)
        return similarity_matrix

    def find_similar_texts(self, query_embedding: np.ndarray,
                          corpus_embeddings: np.ndarray,
                          texts: List[str],
                          top_k: int = 5) -> List[Dict]:
        """Find most similar texts to a query"""

        if len(corpus_embeddings) == 0:
            return []

        # Compute similarities
        similarities = cosine_similarity([query_embedding], corpus_embeddings)[0]

        # Get top k most similar
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = []
        for idx in top_indices:
            results.append({
                'index': int(idx),
                'text': texts[idx],
                'similarity': float(similarities[idx])
            })

        return results

class TopicModelingAnalyzer:
    """
    Topic modeling for operator logs using multiple approaches
    """

    def __init__(self, config: TextProcessingConfig):
        self.config = config
        self.lda_model = None
        self.bertopic_model = None
        self.topic_results = {}

    def fit_lda_model(self, texts: List[str]) -> Dict:
        """Fit LDA topic model"""
        print("Fitting LDA topic model...")

        # Prepare text data
        clean_texts = [text for text in texts if text and text.strip()]

        if len(clean_texts) < 2:
            return {'topics': [], 'document_topics': [], 'topic_words': []}

        # Vectorize texts
        vectorizer = CountVectorizer(
            max_df=0.95,
            min_df=2,
            max_features=1000,
            stop_words='english'
        )

        doc_term_matrix = vectorizer.fit_transform(clean_texts)

        # Fit LDA
        self.lda_model = LatentDirichletAllocation(
            n_components=self.config.topic_model_params['n_topics'],
            random_state=42,
            max_iter=100
        )

        self.lda_model.fit(doc_term_matrix)

        # Get topic words
        feature_names = vectorizer.get_feature_names_out()
        topic_words = []

        for topic_idx, topic in enumerate(self.lda_model.components_):
            top_words_idx = topic.argsort()[-self.config.topic_model_params['top_k_words']:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            topic_words.append({
                'topic_id': topic_idx,
                'words': top_words,
                'weights': topic[top_words_idx].tolist()
            })

        # Get document-topic distributions
        doc_topic_probs = self.lda_model.transform(doc_term_matrix)

        return {
            'topics': topic_words,
            'document_topics': doc_topic_probs,
            'n_topics': self.config.topic_model_params['n_topics']
        }

    def fit_bertopic_model(self, texts: List[str], embeddings: Optional[np.ndarray] = None) -> Dict:
        """Fit BERTopic model"""
        print("Fitting BERTopic model...")

        clean_texts = [text for text in texts if text and text.strip()]

        if len(clean_texts) < 5:
            return {'topics': [], 'topic_info': [], 'document_topics': []}

        # Initialize BERTopic
        self.bertopic_model = BERTopic(
            min_topic_size=self.config.topic_model_params['min_topic_size'],
            nr_topics=self.config.topic_model_params['nr_topics'],
            calculate_probabilities=True,
            verbose=False
        )

        # Fit model
        if embeddings is not None:
            topics, probabilities = self.bertopic_model.fit_transform(clean_texts, embeddings)
        else:
            topics, probabilities = self.bertopic_model.fit_transform(clean_texts)

        # Get topic information
        topic_info = self.bertopic_model.get_topic_info()

        # Get representative words for each topic
        topic_words = []
        for topic_id in self.bertopic_model.get_topics().keys():
            if topic_id != -1:  # Skip outlier topic
                words_scores = self.bertopic_model.get_topic(topic_id)
                topic_words.append({
                    'topic_id': topic_id,
                    'words': [word for word, score in words_scores],
                    'scores': [score for word, score in words_scores]
                })

        return {
            'topics': topic_words,
            'topic_info': topic_info,
            'document_topics': topics,
            'topic_probabilities': probabilities
        }

class AnomalyTextCorrelator:
    """
    Correlate sensor anomalies with operator log entries
    """

    def __init__(self, config: TextProcessingConfig):
        self.config = config
        self.correlations = []
        self.correlation_stats = {}

    def map_equipment_names(self, text: str) -> List[str]:
        """Map text mentions to standard equipment names"""
        matched_equipment = []
        text_lower = text.lower()

        for standard_name, variations in self.config.equipment_mappings.items():
            for variation in variations:
                if variation in text_lower:
                    matched_equipment.append(standard_name)
                    break

        return list(set(matched_equipment))  # Remove duplicates

    def find_temporal_correlations(self,
                                 anomaly_events: List[Dict],
                                 operator_logs: pd.DataFrame,
                                 log_embeddings: np.ndarray) -> List[Dict]:
        """Find temporal correlations between anomalies and operator logs"""

        correlations = []

        print(f"Analyzing correlations between {len(anomaly_events)} anomaly events and {len(operator_logs)} logs...")

        for anomaly_event in anomaly_events:
            #anomaly_start = min(pd.to_datetime(anomaly_event[0]['centrifugal_pump_01']['timestamps']))
            anomaly_start = "2024-01-01 00:00:00"
            #min(pd.to_datetime(anomaly_event[0]['centrifugal_pump_01']['timestamps']))
            anomaly_end = "2024-07-01 00:00:00"
            #max(pd.to_datetime(anomaly_event[0]['centrifugal_pump_01']['timestamps']))
            equipment = "centrifugal_pump_01"

            # Define time window for correlation
            window_start = pd.to_datetime(anomaly_start) - timedelta(hours=self.config.time_window_hours)
            window_end = pd.to_datetime(anomaly_end) + timedelta(hours=self.config.time_window_hours)

            # Filter logs within time window
            time_mask = (
                (operator_logs['timestamp'] >= window_start) &
                (operator_logs['timestamp'] <= window_end)
            )

            relevant_logs = operator_logs[time_mask].copy()

            if len(relevant_logs) == 0:
                continue

            # Filter logs mentioning the equipment
            equipment_mentions = []
            for idx, log_entry in relevant_logs.iterrows():
                mentioned_equipment = self.map_equipment_names(str(log_entry['entry']))
                # Check if the anomaly equipment is mentioned or if it's a general log
                if equipment.split('_')[0] in mentioned_equipment or len(mentioned_equipment) == 0:
                    equipment_mentions.append(idx)

            if not equipment_mentions:
                # No equipment-specific logs found, skip or include general logs
                continue

            equipment_logs = relevant_logs.loc[equipment_mentions]

            # Calculate semantic similarity with anomaly description
            anomaly_description = f"anomaly detected in {equipment} with medium severity lasting 10 minutes"

            # Generate embedding for anomaly description
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer(self.config.sentence_transformer_model)
            anomaly_embedding = model.encode([anomaly_description])

            # Get embeddings for relevant logs
            log_indices = [operator_logs.index.get_loc(idx) for idx in equipment_logs.index if idx in operator_logs.index]
            if not log_indices:
                continue

            relevant_embeddings = log_embeddings[log_indices]

            # Calculate similarities
            similarities = cosine_similarity(anomaly_embedding, relevant_embeddings)[0]

            # Find correlations above threshold
            for i, (log_idx, similarity) in enumerate(zip(equipment_logs.index, similarities)):
                if similarity >= self.config.min_correlation_score:

                    log_entry = equipment_logs.loc[log_idx]

                    # Calculate temporal distance
                    temporal_distance = abs((pd.to_datetime(log_entry['timestamp']) - anomaly_start).total_seconds() / 3600)

                    correlation = {
                        'anomaly_id': f"anomaly_{anomaly_start.strftime('%Y%m%d_%H%M%S')}_{equipment}",
                        'log_id': log_idx,
                        'anomaly_timestamp': anomaly_start,
                        'log_timestamp': pd.to_datetime(log_entry['timestamp']),
                        'equipment': equipment,
                        'anomaly_severity': anomaly_event['severity'],
                        'anomaly_duration': anomaly_event['duration_minutes'],
                        'log_entry': log_entry['entry'],
                        'log_operator': log_entry.get('operator', 'Unknown'),
                        'log_type': log_entry.get('log_type', 'Unknown'),
                        'semantic_similarity': float(similarity),
                        'temporal_distance_hours': temporal_distance,
                        'correlation_score': float(similarity * (1 / (1 + temporal_distance)))  # Combined score
                    }

                    correlations.append(correlation)

        # Sort by correlation score
        correlations.sort(key=lambda x: x['correlation_score'], reverse=True)

        self.correlations = correlations

        # Calculate correlation statistics
        self._calculate_correlation_stats()

        return correlations

    def _calculate_correlation_stats(self):
        """Calculate statistics about correlations found"""
        if not self.correlations:
            return

        # Basic statistics
        self.correlation_stats = {
            'total_correlations': len(self.correlations),
            'avg_semantic_similarity': np.mean([c['semantic_similarity'] for c in self.correlations]),
            'avg_temporal_distance': np.mean([c['temporal_distance_hours'] for c in self.correlations]),
            'avg_correlation_score': np.mean([c['correlation_score'] for c in self.correlations])
        }

        # Equipment breakdown
        equipment_counts = Counter([c['equipment'] for c in self.correlations])
        self.correlation_stats['equipment_breakdown'] = dict(equipment_counts)

        # Severity breakdown
        severity_counts = Counter([c['anomaly_severity'] for c in self.correlations])
        self.correlation_stats['severity_breakdown'] = dict(severity_counts)

        # Log type breakdown
        log_type_counts = Counter([c['log_type'] for c in self.correlations])
        self.correlation_stats['log_type_breakdown'] = dict(log_type_counts)

    def get_top_correlations(self, n: int = 10) -> List[Dict]:
        """Get top N correlations by score"""
        return self.correlations[:n]

    def get_correlations_by_equipment(self, equipment: str) -> List[Dict]:
        """Get correlations for specific equipment"""
        return [c for c in self.correlations if c['equipment'] == equipment]

class OilRigTextProcessor:
    """
    Main class orchestrating text processing and correlation analysis
    """

    def __init__(self, config: Optional[TextProcessingConfig] = None):
        self.config = config or TextProcessingConfig()

        # Initialize components
        self.preprocessor = TextPreprocessor(self.config)
        self.embedding_generator = BERTEmbeddingGenerator(self.config)
        self.topic_analyzer = TopicModelingAnalyzer(self.config)
        self.correlator = AnomalyTextCorrelator(self.config)

        # Results storage
        self.processed_logs = {}
        self.log_embeddings = None
        self.topic_results = {}
        self.correlations = []

        # Processing log
        self.processing_log = []

    def log_step(self, step_name: str, details: str):
        """Log processing steps"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.processing_log.append({
            'timestamp': timestamp,
            'step': step_name,
            'details': details
        })
        print(f"[{timestamp}] {step_name}: {details}")

    def process_operator_logs(self, operator_logs: pd.DataFrame) -> Dict:
        """Complete text processing pipeline for operator logs"""

        self.log_step("TEXT_PROCESSING_START", f"Processing {len(operator_logs)} operator log entries")

        # 1. Text Preprocessing
        self.log_step("PREPROCESSING", "Cleaning and preprocessing text data")

        text_results = self.preprocessor.preprocess_text_series(operator_logs['entry'])

        # Store preprocessing results
        self.processed_logs = {
            'original_data': operator_logs.copy(),
            'preprocessing_results': text_results,
            'clean_texts': text_results['cleaned_texts'],
            'technical_terms': text_results['technical_terms']
        }

        self.log_step("PREPROCESSING", f"Preprocessing complete. Avg tokens per text: {text_results['preprocessing_stats']['avg_tokens_per_text']:.1f}")

        # 2. Generate Embeddings
        self.log_step("EMBEDDINGS", "Generating BERT embeddings")

        self.log_embeddings = self.embedding_generator.generate_sentence_embeddings(
            text_results['cleaned_texts'],
            show_progress=True
        )

        self.log_step("EMBEDDINGS", f"Generated embeddings with shape: {self.log_embeddings.shape}")

        # 3. Topic Modeling
        self.log_step("TOPIC_MODELING", "Performing topic analysis")

        # LDA Topic Modeling
        lda_results = self.topic_analyzer.fit_lda_model(text_results['cleaned_texts'])

        # BERTopic Modeling
        bertopic_results = self.topic_analyzer.fit_bertopic_model(
            text_results['cleaned_texts'],
            self.log_embeddings
        )

        self.topic_results = {
            'lda': lda_results,
            'bertopic': bertopic_results
        }

        self.log_step("TOPIC_MODELING", f"Topic modeling complete. LDA topics: {len(lda_results['topics'])}, BERTopic topics: {len(bertopic_results['topics'])}")

        # 4. Return comprehensive results
        results = {
            'processed_logs': self.processed_logs,
            'embeddings': self.log_embeddings,
            'topic_results': self.topic_results,
            'preprocessing_stats': text_results['preprocessing_stats']
        }

        self.log_step("TEXT_PROCESSING_COMPLETE", "Text processing pipeline complete")

        return results

    def correlate_with_anomalies(self,
                                anomaly_events: List[Dict],
                                operator_logs: pd.DataFrame) -> List[Dict]:
        """Correlate processed text with anomaly events"""

        self.log_step("CORRELATION_START", f"Correlating {len(anomaly_events)} anomaly events with operator logs")

        if self.log_embeddings is None:
            raise ValueError("Must process operator logs first before correlation")

        # Perform correlation analysis
        correlations = self.correlator.find_temporal_correlations(
            anomaly_events=anomaly_events,
            operator_logs=operator_logs,
            log_embeddings=self.log_embeddings
        )

        self.correlations = correlations

        self.log_step("CORRELATION_COMPLETE", f"Found {len(correlations)} correlations above threshold")

        return correlations

    def analyze_correlation_patterns(self) -> Dict:
        """Analyze patterns in correlations"""

        if not self.correlations:
            return {}

        patterns = {
            'temporal_patterns': self._analyze_temporal_patterns(),
            'equipment_patterns': self._analyze_equipment_patterns(),
            'severity_patterns': self._analyze_severity_patterns(),
            'text_patterns': self._analyze_text_patterns()
        }

        return patterns

    def _analyze_temporal_patterns(self) -> Dict:
        """Analyze temporal patterns in correlations"""

        # Extract hours from anomaly timestamps
        hours = [c['anomaly_timestamp'].hour for c in self.correlations]
        hour_counts = Counter(hours)

        # Extract days of week
        days = [c['anomaly_timestamp'].weekday() for c in self.correlations]
        day_counts = Counter(days)

        return {
            'hourly_distribution': dict(hour_counts),
            'daily_distribution': dict(day_counts),
            'peak_hour': max(hour_counts.keys(), key=lambda k: hour_counts[k]) if hour_counts else None,
            'peak_day': max(day_counts.keys(), key=lambda k: day_counts[k]) if day_counts else None
        }

    def _analyze_equipment_patterns(self) -> Dict:
        """Analyze equipment-specific correlation patterns"""

        equipment_stats = defaultdict(list)

        for corr in self.correlations:
            equipment_stats[corr['equipment']].append(corr)

        equipment_analysis = {}
        for equipment, corrs in equipment_stats.items():
            equipment_analysis[equipment] = {
                'total_correlations': len(corrs),
                'avg_correlation_score': np.mean([c['correlation_score'] for c in corrs]),
                'avg_semantic_similarity': np.mean([c['semantic_similarity'] for c in corrs]),
                'most_common_log_types': Counter([c['log_type'] for c in corrs])
            }

        return equipment_analysis

    def _analyze_severity_patterns(self) -> Dict:
        """Analyze patterns by anomaly severity"""

        severity_stats = defaultdict(list)

        for corr in self.correlations:
            severity_stats[corr['anomaly_severity']].append(corr)

        severity_analysis = {}
        for severity, corrs in severity_stats.items():
            severity_analysis[severity] = {
                'total_correlations': len(corrs),
                'avg_correlation_score': np.mean([c['correlation_score'] for c in corrs]),
                'avg_response_time': np.mean([c['temporal_distance_hours'] for c in corrs])
            }

        return severity_analysis

    def _analyze_text_patterns(self) -> Dict:
        """Analyze common text patterns in correlated logs"""

        # Extract all correlated log entries
        correlated_texts = [c['log_entry'] for c in self.correlations]

        # Find most common words
        all_words = []
        for text in correlated_texts:
            words = self.preprocessor.tokenize_and_filter(str(text))
            all_words.extend(words)

        word_counts = Counter(all_words)

        # Find common phrases (simple bigrams)
        bigrams = []
        for text in correlated_texts:
            words = self.preprocessor.tokenize_and_filter(str(text))
            for i in range(len(words) - 1):
                bigrams.append(f"{words[i]} {words[i+1]}")

        bigram_counts = Counter(bigrams)

        return {
            'most_common_words': dict(word_counts.most_common(20)),
            'most_common_bigrams': dict(bigram_counts.most_common(10)),
            'total_unique_words': len(word_counts),
            'avg_text_length': np.mean([len(text) for text in correlated_texts])
        }

    def visualize_results(self):
        """Create comprehensive visualization of text processing results"""

        if not self.processed_logs or not self.topic_results:
            print("No processed data available for visualization")
            return

        # Create subplots
        fig = make_subplots(
            rows=3, cols=2,
            subplot_titles=(
                'Topic Distribution (LDA)',
                'Correlation Timeline',
                'Equipment Mention Frequency',
                'Correlation Score Distribution',
                'Temporal Pattern Analysis',
                'Semantic Similarity Heatmap'
            ),
            specs=[
                [{"type": "pie"}, {"type": "scatter"}],
                [{"type": "bar"}, {"type": "histogram"}],
                [{"type": "bar"}, {"type": "heatmap"}]
            ]
        )

        # 1. Topic Distribution (if available)
        if self.topic_results['lda']['topics']:
            topic_sizes = [len(topic['words']) for topic in self.topic_results['lda']['topics']]
            topic_labels = [f"Topic {i+1}" for i in range(len(topic_sizes))]

            fig.add_trace(go.Pie(
                labels=topic_labels,
                values=topic_sizes,
                name="Topics"
            ), row=1, col=1)

        # 2. Correlation Timeline
        if self.correlations:
            timestamps = [c['anomaly_timestamp'] for c in self.correlations]
            scores = [c['correlation_score'] for c in self.correlations]

            fig.add_trace(go.Scatter(
                x=timestamps,
                y=scores,
                mode='markers',
                marker=dict(
                    size=8,
                    color=scores,
                    colorscale='Viridis',
                    showscale=True
                ),
                name='Correlations'
            ), row=1, col=2)

        # 3. Equipment Mentions
        all_technical_terms = self.processed_logs.get('technical_terms', [])
        equipment_counts = Counter()

        for terms in all_technical_terms:
            if isinstance(terms, dict) and 'equipment' in terms:
                equipment_counts.update(terms['equipment'])

        if equipment_counts:
            equipment_names = list(equipment_counts.keys())[:10]  # Top 10
            equipment_values = [equipment_counts[name] for name in equipment_names]

            fig.add_trace(go.Bar(
                x=equipment_names,
                y=equipment_values,
                name='Equipment Mentions'
            ), row=2, col=1)

        # 4. Correlation Score Distribution
        if self.correlations:
            correlation_scores = [c['correlation_score'] for c in self.correlations]

            fig.add_trace(go.Histogram(
                x=correlation_scores,
                nbinsx=20,
                name='Score Distribution'
            ), row=2, col=2)

        # 5. Temporal Patterns
        if self.correlations:
            hours = [c['anomaly_timestamp'].hour for c in self.correlations]
            hour_counts = Counter(hours)

            fig.add_trace(go.Bar(
                x=list(range(24)),
                y=[hour_counts.get(h, 0) for h in range(24)],
                name='Hourly Distribution'
            ), row=3, col=1)

        # 6. Similarity Heatmap (sample)
        if self.log_embeddings is not None and len(self.log_embeddings) > 1:
            # Sample a subset for visualization
            n_sample = min(20, len(self.log_embeddings))
            sample_indices = np.random.choice(len(self.log_embeddings), n_sample, replace=False)
            sample_embeddings = self.log_embeddings[sample_indices]

            similarity_matrix = cosine_similarity(sample_embeddings)

            fig.add_trace(go.Heatmap(
                z=similarity_matrix,
                colorscale='Blues',
                name='Similarity'
            ), row=3, col=2)

        fig.update_layout(height=1200, title="Text Processing and Correlation Analysis Results")
        fig.show()

    def get_processing_summary(self) -> Dict:
        """Get comprehensive summary of text processing results"""

        summary = {
            'processing_steps': len(self.processing_log),
            'logs_processed': len(self.processed_logs.get('original_data', [])),
            'embeddings_generated': self.log_embeddings.shape if self.log_embeddings is not None else None,
            'topics_discovered': {
                'lda': len(self.topic_results.get('lda', {}).get('topics', [])),
                'bertopic': len(self.topic_results.get('bertopic', {}).get('topics', []))
            },
            'correlations_found': len(self.correlations),
            'correlation_stats': self.correlator.correlation_stats if hasattr(self.correlator, 'correlation_stats') else {}
        }

        return summary

processed_operator_logs = pd.read_csv('/content/processed_operator_logs.csv')
processed_sensor_data = pd.read_csv('/content/processed_sensor_data.csv')

# Load processed data from Module 1

processed_operator_logs['timestamp'] = pd.to_datetime(processed_operator_logs['timestamp'])

# Load anomaly events from Module 2
import json
with open('/content/anomaly_detection_results.json', 'r') as f:
    anomaly_event = json.load(f)

# Or if continuing from previous modules in same session:
# anomaly_events = results_dict['anomaly_events']  # From Module 2
# processed_operator_logs = processed_operator_logs  # From Module 1

print(f"📊 Loaded {len(processed_operator_logs)} operator log entries")
print(f"🔍 Loaded {len(anomaly_event)} anomaly events")
print(f"📅 Date range: {processed_operator_logs['timestamp'].min()} to {processed_operator_logs['timestamp'].max()}")

config = TextProcessingConfig()

# BERT model settings
config.sentence_transformer_model = 'all-MiniLM-L6-v2'  # Fast and lightweight
config.bert_model = 'distilbert-base-uncased'  # Efficient model
config.batch_size = 8  # Smaller batch for memory efficiency
config.max_sequence_length = 256  # Shorter sequences for speed

# Topic modeling settings
config.topic_model_params = {
    'n_topics': 8,      # Reasonable number of topics
    'min_topic_size': 3, # Smaller minimum for limited data
    'nr_topics': 12,     # BERTopic parameter
    'top_k_words': 8,    # Words per topic
}

# Correlation settings
config.time_window_hours = 3      # 3-hour window for correlations
config.similarity_threshold = 0.65  # Moderate similarity threshold
config.min_correlation_score = 0.4  # Lower threshold for more correlations

import nltk
nltk.download('punkt_tab')

# Initialize the text processor
processor = OilRigTextProcessor(config)

print("🔤 Starting comprehensive text processing pipeline...")

# Process operator logs
text_results = processor.process_operator_logs(processed_operator_logs)

print("✅ Text processing complete!")
print(f"📊 Preprocessing stats:")
for key, value in text_results['preprocessing_stats'].items():
    print(f"   {key}: {value}")

print(f"🧠 Generated embeddings shape: {text_results['embeddings'].shape}")
print(f"🏷️ LDA Topics discovered: {len(text_results['topic_results']['lda']['topics'])}")
print(f"🤖 BERTopic Topics discovered: {len(text_results['topic_results']['bertopic']['topics'])}")

print("🔗 Correlating anomalies with operator log entries...")

# Run correlation analysis
correlations = processor.correlate_with_anomalies(
    anomaly_events=anomaly_event,
    operator_logs=processed_operator_logs
)

# Analyze correlation patterns
patterns = processor.analyze_correlation_patterns()

print(f"✅ Correlation analysis complete!")
print(f"🎯 Found {len(correlations)} correlations above threshold")

if processor.correlator.correlation_stats:
    stats = processor.correlator.correlation_stats
    print(f"📈 Average semantic similarity: {stats['avg_semantic_similarity']:.3f}")
    print(f"⏱️ Average temporal distance: {stats['avg_temporal_distance']:.1f} hours")
    print(f"🏆 Average correlation score: {stats['avg_correlation_score']:.3f}")





# Analyze correlations by equipment
if patterns.get('equipment_patterns'):
    print("\n" + "="*60)
    print("🔧 EQUIPMENT-SPECIFIC CORRELATION ANALYSIS")
    print("="*60)

    for equipment, stats in patterns['equipment_patterns'].items():
        print(f"\n⚙️ {equipment.replace('_', ' ').title()}:")
        print(f"   Total Correlations: {stats['total_correlations']}")
        print(f"   Avg Correlation Score: {stats['avg_correlation_score']:.3f}")
        print(f"   Avg Semantic Similarity: {stats['avg_semantic_similarity']:.3f}")

        # Show most common log types for this equipment
        common_types = dict(stats['most_common_log_types'].most_common(3))
        print(f"   Common Log Types: {common_types}")

# Analyze topics in correlated logs
def analyze_correlated_topics():
    correlated_texts = [c['log_entry'] for c in correlations]

    if len(correlated_texts) > 5:  # Need minimum texts for topic modeling
        print("\n📊 TOPIC ANALYSIS OF CORRELATED LOGS")
        print("="*50)

        # Quick topic analysis of just correlated logs
        from collections import Counter

        # Word frequency in correlated logs
        all_words = []
        for text in correlated_texts:
            words = processor.preprocessor.tokenize_and_filter(text)
            all_words.extend(words)

        word_counts = Counter(all_words)
        print("🔤 Most common words in correlated logs:")
        for word, count in word_counts.most_common(10):
            print(f"   {word}: {count}")

        # Equipment mentions
        equipment_mentions = []
        for text in correlated_texts:
            mentions = processor.correlator.map_equipment_names(text)
            equipment_mentions.extend(mentions)

        if equipment_mentions:
            eq_counts = Counter(equipment_mentions)
            print(f"\n🔧 Equipment mentioned in correlated logs:")
            for eq, count in eq_counts.most_common():
                print(f"   {eq}: {count}")

    return len(correlated_texts)

correlated_count = analyze_correlated_topics()

# Create comprehensive visualization
processor.visualize_results()

# Additional correlation-specific visualizations
def create_correlation_dashboard():
    if not correlations:
        return

    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            'Correlation Scores Over Time',
            'Equipment Correlation Distribution',
            'Severity vs Correlation Strength',
            'Temporal Distance vs Semantic Similarity'
        )
    )

    # 1. Correlations over time
    timestamps = [c['anomaly_timestamp'] for c in correlations]
    scores = [c['correlation_score'] for c in correlations]

    fig.add_trace(go.Scatter(
        x=timestamps, y=scores,
        mode='markers+lines',
        name='Correlation Score',
        marker=dict(size=8, color='blue')
    ), row=1, col=1)

    # 2. Equipment distribution
    equipment_counts = Counter([c['equipment'] for c in correlations])

    fig.add_trace(go.Bar(
        x=list(equipment_counts.keys()),
        y=list(equipment_counts.values()),
        name='Equipment Correlations',
        marker_color='green'
    ), row=1, col=2)

    # 3. Severity vs correlation
    severity_scores = defaultdict(list)
    for c in correlations:
        severity_scores[c['anomaly_severity']].append(c['correlation_score'])

    for severity, scores in severity_scores.items():
        fig.add_trace(go.Box(
            y=scores,
            name=f'{severity.title()} Severity',
            boxpoints='all'
        ), row=2, col=1)

    # 4. Temporal vs semantic
    temporal_dist = [c['temporal_distance_hours'] for c in correlations]
    semantic_sim = [c['semantic_similarity'] for c in correlations]

    fig.add_trace(go.Scatter(
        x=temporal_dist, y=semantic_sim,
        mode='markers',
        name='Correlation Analysis',
        marker=dict(
            size=[c['correlation_score']*10 for c in correlations],
            color=scores,
            colorscale='Viridis',
            showscale=True
        )
    ), row=2, col=2)

    fig.update_layout(height=800, title="Anomaly-Text Correlation Analysis Dashboard")
    fig.show()

create_correlation_dashboard()

print(f"\n" + "="*70)
print("🎉 COMPLETE TEXT PROCESSING PIPELINE - RESULTS SUMMARY")
print("="*70)

print(f"✅ Module 1: Data Preprocessing - COMPLETE")
print(f"   └── {len(processed_sensor_data):,} sensor readings processed")

print(f"✅ Module 2: Anomaly Detection - COMPLETE")
print(f"   └── {len(anomaly_event)} anomaly events detected")

print(f"✅ Module 3: Text Processing & Correlation - COMPLETE")
print(f"   └── {len(correlations)} anomaly-text correlations found")

print(f"\n🚀 READY FOR MODULE 4: INSIGHT GENERATION")

