# -*- coding: utf-8 -*-
"""01_02_oil_rig_data_generation_anomaly_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D2pWoi9VknmgQZGkNlLFcM3-4OQ7mIIs

#### Install Packages
"""

!pip install pandas numpy scipy scikit-learn
!pip install feature-engine tsfresh stumpy
!pip install plotly seaborn matplotlib
!pip install faker python-dateutil
!pip install pyod  # for anomaly detection (used in data quality)

"""## Module 0: Oil Sensor Data Generator

#### Load Packages
"""

import numpy as np
import pandas as pd
import seaborn as sns
from datetime import datetime, timedelta
import random
from faker import Faker
from scipy import signal
from sklearn.preprocessing import MinMaxScaler
import warnings
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter

warnings.filterwarnings('ignore')




import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union

import json
from pathlib import Path

# Statistical and ML imports
from scipy import stats
from scipy.signal import savgol_filter
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.impute import KNNImputer
from sklearn.feature_selection import SelectKBest, f_classif
from pyod.models.iforest import IForest

"""#### Oil Data Generator Class"""

# fake = Faker()
# class OilRigDataGenerator:
#     def __init__(self, start_date='2024-01-01', months=6):
#         self.start_date = pd.to_datetime(start_date)
#         self.months = months
#         self.end_date = self.start_date + pd.DateOffset(months=months)
#         self.freq = '5T'  # 5-minute intervals
#         self.time_index = pd.date_range(start=self.start_date, end=self.end_date, freq=self.freq)

#         # Store anomaly events for operator log generation
#         self.anomaly_events = []
#         self.maintenance_events = []

#         # Equipment configurations
#         self.equipment_config = {
#             'centrifugal_pump_01': {
#                 'sensors': ['suction_pressure', 'discharge_pressure', 'flow_rate', 'temperature', 'vibration', 'power'],
#                 'normal_ranges': {
#                     'suction_pressure': (15, 25),    # bar
#                     'discharge_pressure': (45, 65),  # bar
#                     'flow_rate': (800, 1200),        # m³/h
#                     'temperature': (65, 85),         # °C
#                     'vibration': (0.5, 2.0),         # mm/s
#                     'power': (180, 220)              # kW
#                 }
#             },
#             'gas_compressor_01': {
#                 'sensors': ['inlet_pressure', 'outlet_pressure', 'temperature', 'vibration', 'flow_rate', 'power'],
#                 'normal_ranges': {
#                     'inlet_pressure': (8, 15),       # bar
#                     'outlet_pressure': (35, 50),     # bar
#                     'temperature': (120, 160),       # °C
#                     'vibration': (1.0, 4.0),         # mm/s
#                     'flow_rate': (5000, 8000),       # Nm³/h
#                     'power': (450, 550)              # kW
#                 }
#             },
#             'drilling_motor_01': {
#                 'sensors': ['rpm', 'torque', 'temperature', 'vibration', 'pressure', 'power'],
#                 'normal_ranges': {
#                     'rpm': (80, 120),                # RPM
#                     'torque': (15000, 25000),        # N⋅m
#                     'temperature': (80, 120),        # °C
#                     'vibration': (2.0, 8.0),         # mm/s
#                     'pressure': (200, 350),          # bar
#                     'power': (800, 1200)             # kW
#                 }
#             },
#             'separator_01': {
#                 'sensors': ['pressure', 'temperature', 'liquid_level', 'gas_flow', 'liquid_flow'],
#                 'normal_ranges': {
#                     'pressure': (25, 35),            # bar
#                     'temperature': (60, 90),         # °C
#                     'liquid_level': (30, 70),        # %
#                     'gas_flow': (3000, 5000),        # Nm³/h
#                     'liquid_flow': (200, 400)        # m³/h
#                 }
#             },
#             'generator_01': {
#                 'sensors': ['voltage', 'current', 'frequency', 'temperature', 'vibration', 'power_output'],
#                 'normal_ranges': {
#                     'voltage': (380, 420),           # V
#                     'current': (280, 320),           # A
#                     'frequency': (49.8, 50.2),       # Hz
#                     'temperature': (70, 100),        # °C
#                     'vibration': (0.8, 3.0),         # mm/s
#                     'power_output': (180, 250)       # kW
#                 }
#             }
#         }

#     def generate_base_signal(self, normal_range, length):
#         """Generate base signal with daily and seasonal patterns"""
#         min_val, max_val = normal_range
#         mean_val = (min_val + max_val) / 2
#         amplitude = (max_val - min_val) / 4

#         # Time arrays
#         hours = np.arange(length) * 5 / 60  # Convert 5-min intervals to hours
#         days = hours / 24

#         # Daily cycle (peak during day shift)
#         daily_cycle = 0.15 * amplitude * np.sin(2 * np.pi * hours / 24 + np.pi/3)

#         # Weekly cycle (lower activity on weekends)
#         weekly_cycle = 0.1 * amplitude * np.sin(2 * np.pi * days / 7)

#         # Seasonal trend (slight increase over months)
#         seasonal_trend = 0.05 * amplitude * np.sin(2 * np.pi * days / (30 * self.months))

#         # Base signal
#         base = mean_val + daily_cycle + weekly_cycle + seasonal_trend

#         # Add some correlation between related sensors
#         base += np.random.normal(0, amplitude * 0.05, length)

#         return base

#     def add_anomalies(self, signal, equipment_name, sensor_name):
#         """Add various types of anomalies and record them for operator logs"""
#         signal_copy = signal.copy()
#         length = len(signal)

#         # Gradual degradation (bearing wear, fouling, etc.)
#         if random.random() < 0.7:  # 70% chance of gradual anomaly
#             start_point = random.randint(length//4, length//2)
#             degradation_rate = random.uniform(0.00001, 0.0001)
#             gradual_increase = np.zeros(length)
#             gradual_increase[start_point:] = np.cumsum(np.ones(length - start_point) * degradation_rate)

#             if sensor_name in ['vibration', 'temperature', 'pressure']:
#                 signal_copy += gradual_increase * np.mean(signal_copy)

#                 # Record event for operator logs
#                 self.anomaly_events.append({
#                     'timestamp': self.time_index[start_point],
#                     'equipment': equipment_name,
#                     'sensor': sensor_name,
#                     'type': 'gradual_degradation',
#                     'severity': 'medium',
#                     'value_before': signal_copy[start_point-1] if start_point > 0 else signal_copy[0],
#                     'value_at_detection': signal_copy[min(start_point + 1000, length-1)],
#                     'description': self._get_degradation_description(sensor_name, equipment_name)
#                 })

#         # Sudden spikes/drops (trips, starts, stops)
#         n_spikes = random.randint(2, 8)
#         for _ in range(n_spikes):
#             if random.random() < 0.6:  # 60% chance of spike
#                 spike_start = random.randint(100, length - 200)
#                 spike_duration = random.randint(5, 50)  # 25 minutes to 4 hours
#                 spike_magnitude = random.uniform(1.2, 2.5) if random.random() < 0.7 else random.uniform(0.3, 0.8)

#                 original_value = signal_copy[spike_start]
#                 signal_copy[spike_start:spike_start + spike_duration] *= spike_magnitude
#                 new_value = signal_copy[spike_start]

#                 # Record event for operator logs
#                 event_type = 'sudden_spike' if spike_magnitude > 1 else 'sudden_drop'
#                 severity = 'high' if abs(spike_magnitude - 1) > 1.5 else 'medium'

#                 self.anomaly_events.append({
#                     'timestamp': self.time_index[spike_start],
#                     'equipment': equipment_name,
#                     'sensor': sensor_name,
#                     'type': event_type,
#                     'severity': severity,
#                     'value_before': original_value,
#                     'value_at_detection': new_value,
#                     'duration_minutes': spike_duration * 5,
#                     'description': self._get_spike_description(sensor_name, equipment_name, event_type, severity)
#                 })

#         # Oscillations (mechanical looseness, control issues)
#         if random.random() < 0.3:  # 30% chance
#             osc_start = random.randint(length//3, 2*length//3)
#             osc_duration = random.randint(200, 1000)  # 16 hours to 3.5 days
#             osc_end = min(osc_start + osc_duration, length)

#             osc_freq = random.uniform(0.01, 0.1)  # Cycles per 5-minute interval
#             osc_amplitude = np.mean(signal_copy) * random.uniform(0.05, 0.15)
#             oscillation = osc_amplitude * np.sin(2 * np.pi * osc_freq * np.arange(osc_end - osc_start))

#             signal_copy[osc_start:osc_end] += oscillation

#             # Record event for operator logs
#             self.anomaly_events.append({
#                 'timestamp': self.time_index[osc_start],
#                 'equipment': equipment_name,
#                 'sensor': sensor_name,
#                 'type': 'oscillation',
#                 'severity': 'medium',
#                 'value_before': signal_copy[osc_start] - oscillation[0],
#                 'value_at_detection': signal_copy[osc_start],
#                 'duration_hours': osc_duration * 5 / 60,
#                 'description': self._get_oscillation_description(sensor_name, equipment_name)
#             })

#         return signal_copy

#     def _get_degradation_description(self, sensor, equipment):
#         descriptions = {
#             'vibration': [
#                 f"Gradual increase in vibration levels on {equipment}. Possible bearing wear.",
#                 f"Upward trend in {equipment} vibration. Monitoring for further increase.",
#                 f"Vibration signature changing on {equipment}. Scheduled for inspection."
#             ],
#             'temperature': [
#                 f"Rising temperature trend observed on {equipment}. Check cooling system.",
#                 f"Temperature creeping up on {equipment}. May indicate fouling or blockage.",
#                 f"Thermal behavior changing on {equipment}. Investigating root cause."
#             ],
#             'pressure': [
#                 f"Pressure differential increasing on {equipment}. Filter may need replacement.",
#                 f"Gradual pressure rise on {equipment}. Possible restriction developing.",
#                 f"Pressure trend abnormal on {equipment}. Checking for blockages."
#             ]
#         }
#         return random.choice(descriptions.get(sensor, [f"Gradual change in {sensor} on {equipment}"]))

#     def _get_spike_description(self, sensor, equipment, event_type, severity):
#         if event_type == 'sudden_spike':
#             if severity == 'high':
#                 descriptions = [
#                     f"Major {sensor} spike on {equipment}! Immediate investigation required.",
#                     f"Alert: {sensor} exceeded normal range on {equipment}. Equipment tripped.",
#                     f"High {sensor} alarm on {equipment}. Unit automatically shut down."
#                 ]
#             else:
#                 descriptions = [
#                     f"Noticed {sensor} spike on {equipment}. Appears to have self-corrected.",
#                     f"{sensor} briefly elevated on {equipment}. Monitoring situation.",
#                     f"Temporary {sensor} increase on {equipment}. Back to normal now."
#                 ]
#         else:  # sudden_drop
#             descriptions = [
#                 f"Sudden drop in {sensor} on {equipment}. Possible sensor issue or process upset.",
#                 f"{sensor} reading dropped unexpectedly on {equipment}. Checking connections.",
#                 f"Loss of {sensor} signal on {equipment}. Investigating cause."
#             ]
#         return random.choice(descriptions)

#     def _get_oscillation_description(self, sensor, equipment):
#         descriptions = [
#             f"Oscillatory behavior in {sensor} on {equipment}. Possible control loop issue.",
#             f"{sensor} showing hunting behavior on {equipment}. Control system needs tuning.",
#             f"Unstable {sensor} readings on {equipment}. Checking for mechanical looseness."
#         ]
#         return random.choice(descriptions)

#     def add_noise_and_missing(self, signal, missing_rate=0.02):
#         """Add realistic noise and missing values"""
#         # Add measurement noise
#         noise_std = np.std(signal) * 0.02  # 2% of signal std
#         signal += np.random.normal(0, noise_std, len(signal))

#         # Add missing values (sensor failures, communication issues)
#         missing_mask = np.random.random(len(signal)) < missing_rate

#         # Create clusters of missing values (more realistic)
#         for i in range(len(missing_mask) - 10):
#             if missing_mask[i] and random.random() < 0.7:
#                 cluster_size = random.randint(2, 20)
#                 missing_mask[i:i+cluster_size] = True

#         signal[missing_mask] = np.nan

#         return signal

#     def generate_sensor_data(self):
#         """Generate all sensor data and collect anomaly events"""
#         all_data = []
#         self.anomaly_events = []  # Reset anomaly events

#         for equipment_name, config in self.equipment_config.items():
#             equipment_data = {'timestamp': self.time_index}

#             for sensor_name in config['sensors']:
#                 normal_range = config['normal_ranges'][sensor_name]

#                 # Generate base signal
#                 base_signal = self.generate_base_signal(normal_range, len(self.time_index))

#                 # Add anomalies
#                 signal_with_anomalies = self.add_anomalies(base_signal, equipment_name, sensor_name)

#                 # Add noise and missing values
#                 final_signal = self.add_noise_and_missing(signal_with_anomalies)

#                 # Store in equipment data
#                 equipment_data[sensor_name] = final_signal

#             # Convert to DataFrame
#             df = pd.DataFrame(equipment_data)
#             df['equipment'] = equipment_name
#             all_data.append(df)

#         # Combine all equipment data
#         sensor_data = pd.concat(all_data, ignore_index=True)

#         return sensor_data

#     def generate_maintenance_events(self):
#         """Generate planned maintenance events"""
#         maintenance_types = [
#             'routine_inspection', 'oil_change', 'filter_replacement',
#             'bearing_replacement', 'valve_maintenance', 'calibration',
#             'overhaul', 'gasket_replacement'
#         ]

#         equipment_names = list(self.equipment_config.keys())

#         # Generate maintenance events every 2-8 weeks per equipment
#         for equipment in equipment_names:
#             n_events = random.randint(3, 8)  # 3-8 maintenance events over the period

#             maintenance_dates = pd.date_range(
#                 start=self.start_date + timedelta(days=random.randint(7, 21)),
#                 end=self.end_date - timedelta(days=7),
#                 periods=n_events
#             )

#             for date in maintenance_dates:
#                 # Add some randomness to the exact time
#                 actual_time = date + timedelta(hours=random.randint(6, 18),
#                                              minutes=random.randint(0, 59))

#                 maintenance_type = random.choice(maintenance_types)

#                 self.maintenance_events.append({
#                     'timestamp': actual_time,
#                     'equipment': equipment,
#                     'type': maintenance_type,
#                     'duration_hours': random.uniform(1, 8),
#                     'description': self._get_maintenance_description(maintenance_type, equipment)
#                 })

#     def _get_maintenance_description(self, maintenance_type, equipment):
#         descriptions = {
#             'routine_inspection': [
#                 f"Completed routine inspection on {equipment}. All systems normal.",
#                 f"Monthly check on {equipment} finished. Minor adjustments made.",
#                 f"Inspection of {equipment} complete. No issues found."
#             ],
#             'oil_change': [
#                 f"Oil change completed on {equipment}. Used synthetic grade oil.",
#                 f"Lubrication service on {equipment}. Oil analysis shows good condition.",
#                 f"Fresh oil installed in {equipment}. Next change due in 90 days."
#             ],
#             'filter_replacement': [
#                 f"Replaced filters on {equipment}. Old filters showed normal wear.",
#                 f"Filter change on {equipment} completed. System pressure improved.",
#                 f"New filters installed on {equipment}. Previous ones were due for replacement."
#             ],
#             'bearing_replacement': [
#                 f"Bearing replacement on {equipment} successful. Vibration levels reduced.",
#                 f"New bearings installed on {equipment}. Run-in period started.",
#                 f"Bearing change completed on {equipment}. System back to normal operation."
#             ],
#             'calibration': [
#                 f"Calibration of sensors on {equipment} completed. All readings verified.",
#                 f"Instrumentation calibration on {equipment}. Adjustments made to pressure sensors.",
#                 f"Sensor calibration finished on {equipment}. Accuracy confirmed."
#             ]
#         }

#         return random.choice(descriptions.get(maintenance_type,
#                            [f"{maintenance_type.replace('_', ' ').title()} completed on {equipment}."]))

#     def generate_operator_logs(self, additional_routine_entries=50):
#         """Generate operator logs based on anomaly events, maintenance, and routine operations"""

#         # Generate maintenance events first
#         self.generate_maintenance_events()

#         logs = []

#         # 1. Anomaly-based logs (80-90% of anomalies get logged)
#         for event in self.anomaly_events:
#             if random.random() < 0.85:  # 85% of anomalies are observed and logged
#                 # Detection might be delayed by 15 minutes to 4 hours
#                 detection_delay = timedelta(minutes=random.randint(15, 240))
#                 log_time = event['timestamp'] + detection_delay

#                 # Get actual sensor value at log time for more realism
#                 sensor_value = event['value_at_detection']

#                 # Create log entry
#                 entry_text = event['description']
#                 if 'value_at_detection' in event:
#                     entry_text += f" Reading: {sensor_value:.1f}"
#                 if 'duration_minutes' in event:
#                     entry_text += f" Duration: {event['duration_minutes']} minutes"

#                 logs.append({
#                     'timestamp': log_time,
#                     'operator': fake.name(),
#                     'shift': self._get_shift(log_time),
#                     'equipment': event['equipment'],
#                     'log_type': 'anomaly_observation',
#                     'sensor_affected': event['sensor'],
#                     'anomaly_type': event['type'],
#                     'severity': event['severity'],
#                     'entry': entry_text,
#                     'priority': 'High' if event['severity'] == 'high' else 'Medium'
#                 })

#                 # Sometimes add follow-up logs
#                 if event['severity'] == 'high' and random.random() < 0.6:
#                     followup_time = log_time + timedelta(hours=random.uniform(1, 6))
#                     followup_actions = [
#                         "Equipment secured and maintenance team notified.",
#                         "Temporary shutdown initiated as precautionary measure.",
#                         "Investigating root cause. Production impact minimal.",
#                         "Backup system activated. Repairs scheduled for next maintenance window."
#                     ]

#                     logs.append({
#                         'timestamp': followup_time,
#                         'operator': fake.name(),
#                         'shift': self._get_shift(followup_time),
#                         'equipment': event['equipment'],
#                         'log_type': 'followup_action',
#                         'sensor_affected': event['sensor'],
#                         'anomaly_type': event['type'],
#                         'severity': event['severity'],
#                         'entry': random.choice(followup_actions),
#                         'priority': 'Medium'
#                     })

#         # 2. Maintenance-based logs
#         for maintenance in self.maintenance_events:
#             # Pre-maintenance log
#             pre_time = maintenance['timestamp'] - timedelta(minutes=random.randint(30, 120))
#             logs.append({
#                 'timestamp': pre_time,
#                 'operator': fake.name(),
#                 'shift': self._get_shift(pre_time),
#                 'equipment': maintenance['equipment'],
#                 'log_type': 'maintenance_prep',
#                 'entry': f"Preparing {maintenance['equipment']} for {maintenance['type'].replace('_', ' ')}. Isolating systems.",
#                 'priority': 'Medium'
#             })

#             # Maintenance completion log
#             completion_time = maintenance['timestamp'] + timedelta(hours=maintenance['duration_hours'])
#             logs.append({
#                 'timestamp': completion_time,
#                 'operator': fake.name(),
#                 'shift': self._get_shift(completion_time),
#                 'equipment': maintenance['equipment'],
#                 'log_type': 'maintenance_complete',
#                 'maintenance_type': maintenance['type'],
#                 'entry': maintenance['description'],
#                 'priority': 'Low'
#             })

#         # 3. Routine operational logs
#         routine_templates = [
#             "Daily rounds completed. All equipment operating within normal parameters.",
#             "Shift handover: {equipment} running at {percentage}% capacity. No issues reported.",
#             "Process parameters stable. Production targets being met.",
#             "Weather conditions: {weather}. All outdoor equipment checked and secure.",
#             "Safety check completed. Emergency systems tested and operational.",
#             "Control room monitors showing normal readings across all systems.",
#             "Performed hourly gauge readings. All within acceptable ranges.",
#             "Communication test with offshore platform successful."
#         ]

#         weather_conditions = ['clear', 'overcast', 'light rain', 'strong winds', 'fog', 'sunny']
#         equipment_names = list(self.equipment_config.keys())

#         # Generate routine logs throughout the time period
#         routine_timestamps = pd.date_range(
#             start=self.start_date,
#             end=self.end_date,
#             periods=additional_routine_entries
#         ) + pd.to_timedelta(np.random.randint(0, 24*60, additional_routine_entries), unit='m')

#         for timestamp in routine_timestamps:
#             template = random.choice(routine_templates)
#             equipment = random.choice(equipment_names)

#             entry = template.format(
#                 equipment=equipment,
#                 percentage=random.randint(75, 100),
#                 weather=random.choice(weather_conditions)
#             )

#             logs.append({
#                 'timestamp': timestamp,
#                 'operator': fake.name(),
#                 'shift': self._get_shift(timestamp),
#                 'equipment': equipment if 'equipment' in template else 'General',
#                 'log_type': 'routine',
#                 'entry': entry,
#                 'priority': 'Low'
#             })

#         # Convert to DataFrame and sort by timestamp
#         logs_df = pd.DataFrame(logs).sort_values('timestamp').reset_index(drop=True)

#         return logs_df

#     def _get_shift(self, timestamp):
#         """Determine shift based on time of day"""
#         hour = timestamp.hour
#         if 6 <= hour < 14:
#             return 'Day'
#         elif 14 <= hour < 22:
#             return 'Evening'
#         else:
#             return 'Night'

# # Example usage and demonstration
# def demonstrate_integrated_data_generation():
#     """Demonstrate the integrated data generation"""
#     print("Generating integrated oil rig sensor data and operator logs...")

#     # Initialize generator
#     generator = OilRigDataGenerator(start_date='2024-01-01', months=3)

#     # Generate sensor data (this creates anomaly events internally)
#     sensor_data = generator.generate_sensor_data()
#     print(f"Generated sensor data shape: {sensor_data.shape}")
#     print(f"Detected {len(generator.anomaly_events)} anomaly events in sensor data")

#     # Generate operator logs (based on the anomaly events)
#     operator_logs = generator.generate_operator_logs(additional_routine_entries=100)
#     print(f"Generated {len(operator_logs)} operator log entries")

#     # Show the relationship between anomalies and logs
#     anomaly_logs = operator_logs[operator_logs['log_type'].isin(['anomaly_observation', 'followup_action'])]
#     print(f"Anomaly-related log entries: {len(anomaly_logs)}")

#     # Display sample data
#     print("\nSample Sensor Data:")
#     print(sensor_data.head())

#     print("\nSample Anomaly-Related Operator Logs:")
#     if len(anomaly_logs) > 0:
#         print(anomaly_logs[['timestamp', 'equipment', 'sensor_affected', 'anomaly_type', 'severity', 'entry']].head())

#     print("\nSample Routine Operator Logs:")
#     routine_logs = operator_logs[operator_logs['log_type'] == 'routine']
#     print(routine_logs[['timestamp', 'operator', 'equipment', 'entry']].head())

#     # Show time alignment between sensor anomalies and operator observations
#     print(f"\nData Relationships:")
#     print(f"Time range: {sensor_data['timestamp'].min()} to {sensor_data['timestamp'].max()}")
#     print(f"Log time range: {operator_logs['timestamp'].min()} to {operator_logs['timestamp'].max()}")
#     print(f"Equipment in sensor data: {sorted(sensor_data['equipment'].unique())}")
#     print(f"Equipment in logs: {sorted(operator_logs['equipment'].unique())}")

#     # Log type distribution
#     print(f"\nLog Type Distribution:")
#     print(operator_logs['log_type'].value_counts())

#     return sensor_data, operator_logs, generator.anomaly_events

fake = Faker()
class OilRigDataGenerator:
    def __init__(self, start_date='2024-01-01', months=6):
        self.start_date = pd.to_datetime(start_date)
        self.months = months
        self.end_date = self.start_date + pd.DateOffset(months=months)
        self.freq = '5T'  # 5-minute intervals
        self.time_index = pd.date_range(start=self.start_date, end=self.end_date, freq=self.freq)

        # Store anomaly events and maintenance for operator log generation
        self.anomaly_events = []
        self.maintenance_events = []

        # Equipment configurations
        self.equipment_config = {
            'centrifugal_pump_01': {
                'sensors': ['suction_pressure', 'discharge_pressure', 'flow_rate', 'temperature', 'vibration', 'power'],
                'normal_ranges': {
                    'suction_pressure': (15, 25),    # bar
                    'discharge_pressure': (45, 65),  # bar
                    'flow_rate': (800, 1200),        # m³/h
                    'temperature': (65, 85),         # °C
                    'vibration': (0.5, 2.0),         # mm/s
                    'power': (180, 220)              # kW
                }
            },
            'gas_compressor_01': {
                'sensors': ['inlet_pressure', 'outlet_pressure', 'temperature', 'vibration', 'flow_rate', 'power'],
                'normal_ranges': {
                    'inlet_pressure': (8, 15),       # bar
                    'outlet_pressure': (35, 50),     # bar
                    'temperature': (120, 160),       # °C
                    'vibration': (1.0, 4.0),         # mm/s
                    'flow_rate': (5000, 8000),       # Nm³/h
                    'power': (450, 550)              # kW
                }
            },
            'drilling_motor_01': {
                'sensors': ['rpm', 'torque', 'temperature', 'vibration', 'pressure', 'power'],
                'normal_ranges': {
                    'rpm': (80, 120),                # RPM
                    'torque': (15000, 25000),        # N⋅m
                    'temperature': (80, 120),        # °C
                    'vibration': (2.0, 8.0),         # mm/s
                    'pressure': (200, 350),          # bar
                    'power': (800, 1200)             # kW
                }
            },
            'separator_01': {
                'sensors': ['pressure', 'temperature', 'liquid_level', 'gas_flow', 'liquid_flow'],
                'normal_ranges': {
                    'pressure': (25, 35),            # bar
                    'temperature': (60, 90),         # °C
                    'liquid_level': (30, 70),        # %
                    'gas_flow': (3000, 5000),        # Nm³/h
                    'liquid_flow': (200, 400)        # m³/h
                }
            },
            'generator_01': {
                'sensors': ['voltage', 'current', 'frequency', 'temperature', 'vibration', 'power_output'],
                'normal_ranges': {
                    'voltage': (380, 420),           # V
                    'current': (280, 320),           # A
                    'frequency': (49.8, 50.2),       # Hz
                    'temperature': (70, 100),        # °C
                    'vibration': (0.8, 3.0),         # mm/s
                    'power_output': (180, 250)       # kW
                }
            }
        }

    def generate_base_signal(self, normal_range, length, equipment_name, sensor_name):
        """Generate base signal with daily, seasonal patterns and equipment-specific behaviors"""
        min_val, max_val = normal_range
        mean_val = (min_val + max_val) / 2
        amplitude = (max_val - min_val) / 4

        # Time arrays
        hours = np.arange(length) * 5 / 60  # Convert 5-min intervals to hours
        days = hours / 24

        # Equipment-specific operational patterns
        equipment_patterns = {
            'centrifugal_pump_01': {
                'daily_variation': 0.15,    # 15% daily variation
                'load_following': True,     # Follows production demand
                'weekend_reduction': 0.8    # 80% capacity on weekends
            },
            'gas_compressor_01': {
                'daily_variation': 0.20,    # 20% daily variation
                'load_following': True,     # Follows gas demand
                'weekend_reduction': 0.9    # 90% capacity on weekends
            },
            'drilling_motor_01': {
                'daily_variation': 0.25,    # 25% daily variation - drilling operations vary
                'load_following': False,    # More constant when drilling
                'weekend_reduction': 0.3    # Much lower weekend activity
            },
            'separator_01': {
                'daily_variation': 0.10,    # 10% daily variation - more stable
                'load_following': True,     # Follows production
                'weekend_reduction': 0.85   # 85% capacity on weekends
            },
            'generator_01': {
                'daily_variation': 0.12,    # 12% daily variation
                'load_following': True,     # Follows electrical demand
                'weekend_reduction': 0.75   # 75% load on weekends
            }
        }

        pattern = equipment_patterns.get(equipment_name, equipment_patterns['centrifugal_pump_01'])

        # Daily cycle - different patterns for different equipment
        if pattern['load_following']:
            # Peak during day shift, moderate evening, low night
            daily_cycle = pattern['daily_variation'] * amplitude * (
                0.7 * np.sin(2 * np.pi * hours / 24 + np.pi/3) +  # Main daily cycle
                0.3 * np.sin(4 * np.pi * hours / 24)  # Bi-daily cycle (lunch break, shift change)
            )
        else:
            # More constant operation for drilling equipment
            daily_cycle = pattern['daily_variation'] * amplitude * np.sin(2 * np.pi * hours / 24 + np.pi/3)

        # Weekly cycle (lower activity on weekends)
        weekly_cycle = 0.1 * amplitude * np.sin(2 * np.pi * days / 7)

        # Weekend reduction
        day_of_week = (days % 7).astype(int)
        weekend_mask = (day_of_week >= 5)  # Saturday and Sunday
        weekend_factor = np.ones(len(days))
        weekend_factor[weekend_mask] = pattern['weekend_reduction']

        # Seasonal trend (slight increase over months due to wear, fouling, etc.)
        seasonal_trend = 0.03 * amplitude * np.sin(2 * np.pi * days / (30 * self.months))

        # Sensor-specific variations
        sensor_variations = {
            'temperature': lambda: 0.02 * amplitude * np.sin(2 * np.pi * days / 30),  # Monthly ambient variation
            'pressure': lambda: 0.01 * amplitude * np.random.normal(0, 1, len(days)),  # Process variations
            'vibration': lambda: 0.05 * amplitude * np.abs(np.sin(2 * np.pi * hours / (24*7))),  # Weekly machinery variation
            'flow_rate': lambda: 0.03 * amplitude * np.sin(2 * np.pi * hours / 12),  # Semi-daily flow cycles
            'power': lambda: 0.02 * amplitude * np.sin(2 * np.pi * hours / 24),  # Power follows load
        }

        sensor_variation = np.zeros(length)
        for sensor_type, variation_func in sensor_variations.items():
            if sensor_type in sensor_name.lower():
                sensor_variation = variation_func()
                break

        # Base signal with all components
        base = (mean_val + daily_cycle + weekly_cycle + seasonal_trend + sensor_variation) * weekend_factor

        # Add operational noise (small random variations)
        operational_noise = np.random.normal(0, amplitude * 0.03, length)
        base += operational_noise

        # Ensure signal stays within reasonable bounds
        base = np.clip(base, min_val * 0.8, max_val * 1.2)

        return base

    def add_anomalies(self, signal, equipment_name, sensor_name):
        """Add various types of anomalies and record them for operator logs"""
        signal_copy = signal.copy()
        length = len(signal)

        # Gradual degradation (bearing wear, fouling, etc.)
        if random.random() < 0.7:  # 70% chance of gradual anomaly
            start_point = random.randint(length//4, length//2)
            degradation_rate = random.uniform(0.00001, 0.0001)
            gradual_increase = np.zeros(length)
            gradual_increase[start_point:] = np.cumsum(np.ones(length - start_point) * degradation_rate)

            if sensor_name in ['vibration', 'temperature', 'pressure']:
                signal_copy += gradual_increase * np.mean(signal_copy)

                # Record event for operator logs
                self.anomaly_events.append({
                    'timestamp': self.time_index[start_point],
                    'equipment': equipment_name,
                    'sensor': sensor_name,
                    'type': 'gradual_degradation',
                    'severity': 'medium',
                    'value_before': signal_copy[start_point-1] if start_point > 0 else signal_copy[0],
                    'value_at_detection': signal_copy[min(start_point + 1000, length-1)],
                    'description': self._get_degradation_description(sensor_name, equipment_name)
                })

        # Sudden spikes/drops (trips, starts, stops)
        n_spikes = random.randint(2, 8)
        for _ in range(n_spikes):
            if random.random() < 0.6:  # 60% chance of spike
                spike_start = random.randint(100, length - 200)
                spike_duration = random.randint(5, 50)  # 25 minutes to 4 hours
                spike_magnitude = random.uniform(1.2, 2.5) if random.random() < 0.7 else random.uniform(0.3, 0.8)

                original_value = signal_copy[spike_start]
                signal_copy[spike_start:spike_start + spike_duration] *= spike_magnitude
                new_value = signal_copy[spike_start]

                # Record event for operator logs
                event_type = 'sudden_spike' if spike_magnitude > 1 else 'sudden_drop'
                severity = 'high' if abs(spike_magnitude - 1) > 1.5 else 'medium'

                self.anomaly_events.append({
                    'timestamp': self.time_index[spike_start],
                    'equipment': equipment_name,
                    'sensor': sensor_name,
                    'type': event_type,
                    'severity': severity,
                    'value_before': original_value,
                    'value_at_detection': new_value,
                    'duration_minutes': spike_duration * 5,
                    'description': self._get_spike_description(sensor_name, equipment_name, event_type, severity)
                })

        # Oscillations (mechanical looseness, control issues)
        if random.random() < 0.3:  # 30% chance
            osc_start = random.randint(length//3, 2*length//3)
            osc_duration = random.randint(200, 1000)  # 16 hours to 3.5 days
            osc_end = min(osc_start + osc_duration, length)

            osc_freq = random.uniform(0.01, 0.1)  # Cycles per 5-minute interval
            osc_amplitude = np.mean(signal_copy) * random.uniform(0.05, 0.15)
            oscillation = osc_amplitude * np.sin(2 * np.pi * osc_freq * np.arange(osc_end - osc_start))

            signal_copy[osc_start:osc_end] += oscillation

            # Record event for operator logs
            self.anomaly_events.append({
                'timestamp': self.time_index[osc_start],
                'equipment': equipment_name,
                'sensor': sensor_name,
                'type': 'oscillation',
                'severity': 'medium',
                'value_before': signal_copy[osc_start] - oscillation[0],
                'value_at_detection': signal_copy[osc_start],
                'duration_hours': osc_duration * 5 / 60,
                'description': self._get_oscillation_description(sensor_name, equipment_name)
            })

        return signal_copy

    def _get_degradation_description(self, sensor, equipment):
        descriptions = {
            'vibration': [
                f"Gradual increase in vibration levels on {equipment}. Possible bearing wear.",
                f"Upward trend in {equipment} vibration. Monitoring for further increase.",
                f"Vibration signature changing on {equipment}. Scheduled for inspection."
            ],
            'temperature': [
                f"Rising temperature trend observed on {equipment}. Check cooling system.",
                f"Temperature creeping up on {equipment}. May indicate fouling or blockage.",
                f"Thermal behavior changing on {equipment}. Investigating root cause."
            ],
            'pressure': [
                f"Pressure differential increasing on {equipment}. Filter may need replacement.",
                f"Gradual pressure rise on {equipment}. Possible restriction developing.",
                f"Pressure trend abnormal on {equipment}. Checking for blockages."
            ]
        }
        return random.choice(descriptions.get(sensor, [f"Gradual change in {sensor} on {equipment}"]))

    def _get_spike_description(self, sensor, equipment, event_type, severity):
        if event_type == 'sudden_spike':
            if severity == 'high':
                descriptions = [
                    f"Major {sensor} spike on {equipment}! Immediate investigation required.",
                    f"Alert: {sensor} exceeded normal range on {equipment}. Equipment tripped.",
                    f"High {sensor} alarm on {equipment}. Unit automatically shut down."
                ]
            else:
                descriptions = [
                    f"Noticed {sensor} spike on {equipment}. Appears to have self-corrected.",
                    f"{sensor} briefly elevated on {equipment}. Monitoring situation.",
                    f"Temporary {sensor} increase on {equipment}. Back to normal now."
                ]
        else:  # sudden_drop
            descriptions = [
                f"Sudden drop in {sensor} on {equipment}. Possible sensor issue or process upset.",
                f"{sensor} reading dropped unexpectedly on {equipment}. Checking connections.",
                f"Loss of {sensor} signal on {equipment}. Investigating cause."
            ]
        return random.choice(descriptions)

    def _get_oscillation_description(self, sensor, equipment):
        descriptions = [
            f"Oscillatory behavior in {sensor} on {equipment}. Possible control loop issue.",
            f"{sensor} showing hunting behavior on {equipment}. Control system needs tuning.",
            f"Unstable {sensor} readings on {equipment}. Checking for mechanical looseness."
        ]
        return random.choice(descriptions)

    def add_noise_and_missing(self, signal, equipment_name, sensor_name):
        """Add realistic noise and missing values with equipment/sensor specific patterns"""
        signal_copy = signal.copy()

        # Add measurement noise based on sensor type
        noise_levels = {
            'pressure': 0.01,      # 1% - pressure sensors are quite accurate
            'temperature': 0.015,   # 1.5% - temperature sensors moderate accuracy
            'flow_rate': 0.025,    # 2.5% - flow sensors less accurate
            'vibration': 0.02,     # 2% - vibration sensors moderate accuracy
            'power': 0.01,         # 1% - power meters are accurate
            'voltage': 0.005,      # 0.5% - voltage sensors very accurate
            'current': 0.01,       # 1% - current sensors accurate
            'frequency': 0.001,    # 0.1% - frequency sensors very accurate
            'rpm': 0.015,          # 1.5% - tachometer moderate accuracy
            'torque': 0.03,        # 3% - torque sensors less accurate
            'level': 0.02          # 2% - level sensors moderate accuracy
        }

        # Determine noise level based on sensor name
        noise_factor = 0.02  # default
        for sensor_type, factor in noise_levels.items():
            if sensor_type in sensor_name.lower():
                noise_factor = factor
                break

        noise_std = np.std(signal_copy) * noise_factor
        signal_copy += np.random.normal(0, noise_std, len(signal_copy))

        # Equipment-specific missing data rates (much more realistic)
        equipment_reliability = {
            'centrifugal_pump_01': 0.003,      # 0.3% missing - pumps are reliable
            'gas_compressor_01': 0.005,        # 0.5% missing - compressors moderate
            'drilling_motor_01': 0.008,        # 0.8% missing - drilling equipment harsh environment
            'separator_01': 0.002,             # 0.2% missing - separators very reliable
            'generator_01': 0.001              # 0.1% missing - generators very reliable
        }

        base_missing_rate = equipment_reliability.get(equipment_name, 0.005)

        # Sensor-specific reliability adjustments
        sensor_reliability_multipliers = {
            'vibration': 1.5,      # Vibration sensors fail more often
            'flow_rate': 2.0,      # Flow sensors in harsh conditions
            'liquid_level': 1.3,   # Level sensors can get fouled
            'gas_flow': 1.2,       # Gas flow sensors moderate reliability
            'torque': 1.8          # Torque sensors in mechanical stress
        }

        reliability_multiplier = 1.0
        for sensor_type, multiplier in sensor_reliability_multipliers.items():
            if sensor_type in sensor_name.lower():
                reliability_multiplier = multiplier
                break

        adjusted_missing_rate = base_missing_rate * reliability_multiplier

        # Generate missing values with realistic patterns
        missing_mask = np.zeros(len(signal_copy), dtype=bool)

        # 1. Random individual missing points (sensor glitches) - 70% of missing data
        random_missing = np.random.random(len(signal_copy)) < (adjusted_missing_rate * 0.7)
        missing_mask |= random_missing

        # 2. Communication outages (small clusters, less frequent) - 20% of missing data
        n_outages = max(1, int(len(signal_copy) * adjusted_missing_rate * 0.05))  # Much fewer outages
        for _ in range(n_outages):
            if random.random() < 0.3:  # Only 30% chance of communication outage
                outage_start = random.randint(0, len(signal_copy) - 100)
                outage_duration = random.randint(2, 15)  # 10 minutes to 1.25 hours max
                outage_end = min(outage_start + outage_duration, len(signal_copy))
                missing_mask[outage_start:outage_end] = True

        # 3. Maintenance windows (planned missing data) - 10% of missing data, very rare
        if random.random() < 0.05:  # Only 5% chance of maintenance-related missing data
            maintenance_start = random.randint(len(signal_copy)//4, 3*len(signal_copy)//4)
            maintenance_duration = random.randint(12, 48)  # 1-4 hours
            maintenance_end = min(maintenance_start + maintenance_duration, len(signal_copy))
            missing_mask[maintenance_start:maintenance_end] = True

            # Log this as a maintenance event
            self.maintenance_events.append({
                'timestamp': self.time_index[maintenance_start],
                'equipment': equipment_name,
                'type': 'sensor_maintenance',
                'duration_hours': maintenance_duration * 5 / 60,  # Convert to hours
                'description': f"Sensor maintenance on {sensor_name} of {equipment_name}. Temporary data loss expected."
            })

        # Apply missing data
        signal_copy[missing_mask] = np.nan

        # Log significant missing data events for operator logs
        if np.sum(missing_mask) > len(signal_copy) * 0.05:  # If more than 5% missing in one go
            missing_start_idx = np.where(missing_mask)[0][0] if np.any(missing_mask) else 0
            self.anomaly_events.append({
                'timestamp': self.time_index[missing_start_idx],
                'equipment': equipment_name,
                'sensor': sensor_name,
                'type': 'data_loss',
                'severity': 'medium',
                'value_before': 'N/A',
                'value_at_detection': 'N/A',
                'description': f"Significant data loss on {sensor_name} of {equipment_name}. Communication or sensor issue suspected."
            })

        return signal_copy

    def generate_sensor_data(self):
        """Generate all sensor data with realistic cross-correlations"""
        all_data = []
        self.anomaly_events = []  # Reset anomaly events

        # Generate base signals first to create correlations
        equipment_signals = {}

        for equipment_name, config in self.equipment_config.items():
            equipment_data = {'timestamp': self.time_index}
            equipment_signals[equipment_name] = {}

            for sensor_name in config['sensors']:
                normal_range = config['normal_ranges'][sensor_name]

                # Generate base signal
                base_signal = self.generate_base_signal(normal_range, len(self.time_index), equipment_name, sensor_name)
                equipment_signals[equipment_name][sensor_name] = base_signal.copy()

                equipment_data[sensor_name] = base_signal

            # Add realistic cross-correlations within equipment
            equipment_data = self._add_sensor_correlations(equipment_data, equipment_name)

            # Store updated signals
            for sensor_name in config['sensors']:
                equipment_signals[equipment_name][sensor_name] = equipment_data[sensor_name].copy()

            equipment_data['equipment'] = equipment_name
            all_data.append(pd.DataFrame(equipment_data))

        # Add cross-equipment correlations (e.g., power grid effects)
        self._add_cross_equipment_correlations(equipment_signals)

        # Now add anomalies and noise to final signals
        final_data = []
        for i, equipment_name in enumerate(self.equipment_config.keys()):
            equipment_data = {'timestamp': self.time_index}

            for sensor_name in self.equipment_config[equipment_name]['sensors']:
                # Get the signal with correlations
                correlated_signal = equipment_signals[equipment_name][sensor_name]

                # Add anomalies
                signal_with_anomalies = self.add_anomalies(correlated_signal, equipment_name, sensor_name)

                # Add noise and missing values
                final_signal = self.add_noise_and_missing(signal_with_anomalies, equipment_name, sensor_name)

                equipment_data[sensor_name] = final_signal

            equipment_data['equipment'] = equipment_name
            final_data.append(pd.DataFrame(equipment_data))

        # Combine all equipment data
        sensor_data = pd.concat(final_data, ignore_index=True)

        return sensor_data

    def _add_sensor_correlations(self, equipment_data, equipment_name):
        """Add realistic correlations between sensors within same equipment"""

        correlations = {
            'centrifugal_pump_01': [
                ('discharge_pressure', 'suction_pressure', 0.3),  # Pressures somewhat correlated
                ('flow_rate', 'discharge_pressure', 0.7),         # Higher pressure = higher flow
                ('power', 'flow_rate', 0.8),                     # Power follows flow demand
                ('temperature', 'power', 0.6),                   # Heat from power consumption
                ('vibration', 'power', 0.4),                     # Mechanical stress from load
            ],
            'gas_compressor_01': [
                ('outlet_pressure', 'inlet_pressure', 0.5),      # Pressure ratio correlation
                ('power', 'outlet_pressure', 0.7),               # Power needed for compression
                ('temperature', 'power', 0.8),                   # Heat from compression work
                ('flow_rate', 'power', 0.6),                     # Flow affects power
                ('vibration', 'outlet_pressure', 0.3),           # Pressure affects mechanical stress
            ],
            'drilling_motor_01': [
                ('torque', 'pressure', 0.8),                     # Drilling load correlation
                ('power', 'torque', 0.9),                        # Power = torque × speed
                ('temperature', 'power', 0.7),                   # Heat from power
                ('vibration', 'rpm', 0.5),                       # Speed affects vibration
                ('vibration', 'torque', 0.6),                    # Load affects vibration
            ],
            'separator_01': [
                ('gas_flow', 'pressure', -0.4),                  # Higher flow = lower pressure
                ('liquid_flow', 'liquid_level', 0.6),            # Level affects outflow
                ('temperature', 'pressure', 0.3),                # Thermodynamic relationship
            ],
            'generator_01': [
                ('current', 'power_output', 0.95),               # Ohm's law relationship
                ('voltage', 'frequency', 0.4),                   # Grid stability correlation
                ('temperature', 'current', 0.7),                 # Heat from current
                ('vibration', 'power_output', 0.3),              # Load affects vibration
            ]
        }

        if equipment_name in correlations:
            for sensor1, sensor2, correlation_strength in correlations[equipment_name]:
                if sensor1 in equipment_data and sensor2 in equipment_data:
                    # Add correlated component to sensor1 based on sensor2
                    signal1 = equipment_data[sensor1]
                    signal2 = equipment_data[sensor2]

                    # Normalize signals for correlation
                    signal2_normalized = (signal2 - np.mean(signal2)) / (np.std(signal2) + 1e-8)
                    correlation_component = correlation_strength * np.std(signal1) * 0.1 * signal2_normalized

                    equipment_data[sensor1] = signal1 + correlation_component

        return equipment_data

    def _add_cross_equipment_correlations(self, equipment_signals):
        """Add correlations between different equipment (e.g., power grid effects)"""

        # Power system correlations
        power_sensors = []
        for equipment_name, signals in equipment_signals.items():
            for sensor_name in signals.keys():
                if 'power' in sensor_name.lower():
                    power_sensors.append((equipment_name, sensor_name))

        # Add small common power grid fluctuations
        if len(power_sensors) > 1:
            grid_fluctuation = np.random.normal(0, 1, len(self.time_index)) * 0.01
            for equipment_name, sensor_name in power_sensors:
                base_std = np.std(equipment_signals[equipment_name][sensor_name])
                equipment_signals[equipment_name][sensor_name] += grid_fluctuation * base_std

        # Temperature correlations (ambient temperature affects all)
        temp_sensors = []
        for equipment_name, signals in equipment_signals.items():
            for sensor_name in signals.keys():
                if 'temperature' in sensor_name.lower():
                    temp_sensors.append((equipment_name, sensor_name))

        if len(temp_sensors) > 1:
            # Daily ambient temperature variation
            hours = np.arange(len(self.time_index)) * 5 / 60
            ambient_temp_variation = 5 * np.sin(2 * np.pi * hours / 24)  # ±5°C daily variation

            for equipment_name, sensor_name in temp_sensors:
                equipment_signals[equipment_name][sensor_name] += ambient_temp_variation * 0.3

    def generate_maintenance_events(self):
        """Generate planned maintenance events"""
        maintenance_types = [
            'routine_inspection', 'oil_change', 'filter_replacement',
            'bearing_replacement', 'valve_maintenance', 'calibration',
            'overhaul', 'gasket_replacement'
        ]

        equipment_names = list(self.equipment_config.keys())

        # Generate maintenance events every 2-8 weeks per equipment
        for equipment in equipment_names:
            n_events = random.randint(3, 8)  # 3-8 maintenance events over the period

            maintenance_dates = pd.date_range(
                start=self.start_date + timedelta(days=random.randint(7, 21)),
                end=self.end_date - timedelta(days=7),
                periods=n_events
            )

            for date in maintenance_dates:
                # Add some randomness to the exact time
                actual_time = date + timedelta(hours=random.randint(6, 18),
                                             minutes=random.randint(0, 59))

                maintenance_type = random.choice(maintenance_types)

                self.maintenance_events.append({
                    'timestamp': actual_time,
                    'equipment': equipment,
                    'type': maintenance_type,
                    'duration_hours': random.uniform(1, 8),
                    'description': self._get_maintenance_description(maintenance_type, equipment)
                })

    def _get_maintenance_description(self, maintenance_type, equipment):
        descriptions = {
            'routine_inspection': [
                f"Completed routine inspection on {equipment}. All systems normal.",
                f"Monthly check on {equipment} finished. Minor adjustments made.",
                f"Inspection of {equipment} complete. No issues found."
            ],
            'oil_change': [
                f"Oil change completed on {equipment}. Used synthetic grade oil.",
                f"Lubrication service on {equipment}. Oil analysis shows good condition.",
                f"Fresh oil installed in {equipment}. Next change due in 90 days."
            ],
            'filter_replacement': [
                f"Replaced filters on {equipment}. Old filters showed normal wear.",
                f"Filter change on {equipment} completed. System pressure improved.",
                f"New filters installed on {equipment}. Previous ones were due for replacement."
            ],
            'bearing_replacement': [
                f"Bearing replacement on {equipment} successful. Vibration levels reduced.",
                f"New bearings installed on {equipment}. Run-in period started.",
                f"Bearing change completed on {equipment}. System back to normal operation."
            ],
            'calibration': [
                f"Calibration of sensors on {equipment} completed. All readings verified.",
                f"Instrumentation calibration on {equipment}. Adjustments made to pressure sensors.",
                f"Sensor calibration finished on {equipment}. Accuracy confirmed."
            ]
        }

        return random.choice(descriptions.get(maintenance_type,
                           [f"{maintenance_type.replace('_', ' ').title()} completed on {equipment}."]))

    def generate_operator_logs(self, additional_routine_entries=50):
        """Generate operator logs based on anomaly events, maintenance, and routine operations"""

        # Generate maintenance events first
        self.generate_maintenance_events()

        logs = []

        # 1. Anomaly-based logs (85% of anomalies get logged)
        for event in self.anomaly_events:
            if random.random() < 0.85:  # 85% of anomalies are observed and logged
                # Detection might be delayed by 15 minutes to 4 hours
                detection_delay = timedelta(minutes=random.randint(15, 240))
                log_time = event['timestamp'] + detection_delay

                # Get actual sensor value at log time for more realism
                sensor_value = event['value_at_detection']

                # Create log entry
                entry_text = event['description']
                if 'value_at_detection' in event and event['value_at_detection'] != 'N/A':
                    entry_text += f" Reading: {sensor_value:.1f}"
                if 'duration_minutes' in event:
                    entry_text += f" Duration: {event['duration_minutes']} minutes"

                logs.append({
                    'timestamp': log_time,
                    'operator': fake.name(),
                    'shift': self._get_shift(log_time),
                    'equipment': event['equipment'],
                    'log_type': 'anomaly_observation',
                    'sensor_affected': event['sensor'],
                    'anomaly_type': event['type'],
                    'severity': event['severity'],
                    'entry': entry_text,
                    'priority': 'High' if event['severity'] == 'high' else 'Medium'
                })

                # Sometimes add follow-up logs
                if event['severity'] == 'high' and random.random() < 0.6:
                    followup_time = log_time + timedelta(hours=random.uniform(1, 6))
                    followup_actions = [
                        "Equipment secured and maintenance team notified.",
                        "Temporary shutdown initiated as precautionary measure.",
                        "Investigating root cause. Production impact minimal.",
                        "Backup system activated. Repairs scheduled for next maintenance window."
                    ]

                    logs.append({
                        'timestamp': followup_time,
                        'operator': fake.name(),
                        'shift': self._get_shift(followup_time),
                        'equipment': event['equipment'],
                        'log_type': 'followup_action',
                        'sensor_affected': event['sensor'],
                        'anomaly_type': event['type'],
                        'severity': event['severity'],
                        'entry': random.choice(followup_actions),
                        'priority': 'Medium'
                    })

        # 2. Maintenance-based logs
        for maintenance in self.maintenance_events:
            # Pre-maintenance log
            pre_time = maintenance['timestamp'] - timedelta(minutes=random.randint(30, 120))
            logs.append({
                'timestamp': pre_time,
                'operator': fake.name(),
                'shift': self._get_shift(pre_time),
                'equipment': maintenance['equipment'],
                'log_type': 'maintenance_prep',
                'entry': f"Preparing {maintenance['equipment']} for {maintenance['type'].replace('_', ' ')}. Isolating systems.",
                'priority': 'Medium'
            })

            # Maintenance completion log
            completion_time = maintenance['timestamp'] + timedelta(hours=maintenance['duration_hours'])
            logs.append({
                'timestamp': completion_time,
                'operator': fake.name(),
                'shift': self._get_shift(completion_time),
                'equipment': maintenance['equipment'],
                'log_type': 'maintenance_complete',
                'maintenance_type': maintenance['type'],
                'entry': maintenance['description'],
                'priority': 'Low'
            })

        # 3. Routine operational logs
        routine_templates = [
            "Daily rounds completed. All equipment operating within normal parameters.",
            "Shift handover: {equipment} running at {percentage}% capacity. No issues reported.",
            "Process parameters stable. Production targets being met.",
            "Weather conditions: {weather}. All outdoor equipment checked and secure.",
            "Safety check completed. Emergency systems tested and operational.",
            "Control room monitors showing normal readings across all systems.",
            "Performed hourly gauge readings. All within acceptable ranges.",
            "Communication test with offshore platform successful."
        ]

        weather_conditions = ['clear', 'overcast', 'light rain', 'strong winds', 'fog', 'sunny']
        equipment_names = list(self.equipment_config.keys())

        # Generate routine logs throughout the time period
        routine_timestamps = pd.date_range(
            start=self.start_date,
            end=self.end_date,
            periods=additional_routine_entries
        ) + pd.to_timedelta(np.random.randint(0, 24*60, additional_routine_entries), unit='m')

        for timestamp in routine_timestamps:
            template = random.choice(routine_templates)
            equipment = random.choice(equipment_names)

            entry = template.format(
                equipment=equipment,
                percentage=random.randint(75, 100),
                weather=random.choice(weather_conditions)
            )

            logs.append({
                'timestamp': timestamp,
                'operator': fake.name(),
                'shift': self._get_shift(timestamp),
                'equipment': equipment if 'equipment' in template else 'General',
                'log_type': 'routine',
                'entry': entry,
                'priority': 'Low'
            })

        # Convert to DataFrame and sort by timestamp
        logs_df = pd.DataFrame(logs).sort_values('timestamp').reset_index(drop=True)

        return logs_df

    def _get_shift(self, timestamp):
        """Determine shift based on time of day"""
        hour = timestamp.hour
        if 6 <= hour < 14:
            return 'Day'
        elif 14 <= hour < 22:
            return 'Evening'
        else:
            return 'Night'

# Example usage and demonstration with enhanced data quality metrics
def demonstrate_integrated_data_generation():
    """Demonstrate the integrated data generation with data quality metrics"""
    print("Generating integrated oil rig sensor data and operator logs...")

    # Initialize generator
    generator = OilRigDataGenerator(start_date='2024-01-01', months=3)

    # Generate sensor data (this creates anomaly events internally)
    sensor_data = generator.generate_sensor_data()
    print(f"Generated sensor data shape: {sensor_data.shape}")
    print(f"Detected {len(generator.anomaly_events)} anomaly events in sensor data")

    # Generate operator logs (based on the anomaly events)
    operator_logs = generator.generate_operator_logs(additional_routine_entries=100)
    print(f"Generated {len(operator_logs)} operator log entries")

    # Show the relationship between anomalies and logs
    anomaly_logs = operator_logs[operator_logs['log_type'].isin(['anomaly_observation', 'followup_action'])]
    print(f"Anomaly-related log entries: {len(anomaly_logs)}")

    # Data Quality Assessment
    print(f"\n=== DATA QUALITY METRICS ===")

    # Missing data analysis by equipment and sensor
    sensor_columns = [col for col in sensor_data.columns if col not in ['timestamp', 'equipment']]

    print(f"Missing data percentages by sensor:")
    for equipment in sensor_data['equipment'].unique():
        equipment_data = sensor_data[sensor_data['equipment'] == equipment]
        print(f"\n{equipment}:")

        for sensor in sensor_columns:
            if sensor in equipment_data.columns:
                missing_pct = equipment_data[sensor].isna().sum() / len(equipment_data) * 100
                print(f"  {sensor}: {missing_pct:.1f}%")

    # Overall statistics
    overall_missing = sensor_data[sensor_columns].isna().sum() / len(sensor_data) * 100
    print(f"\nOverall missing data rates:")
    for sensor, pct in overall_missing.items():
        if pct > 0:
            print(f"  {sensor}: {pct:.1f}%")

    # Data ranges and realistic values
    print(f"\nData range validation (min/max values):")
    for equipment in sensor_data['equipment'].unique():
        equipment_data = sensor_data[sensor_data['equipment'] == equipment]
        print(f"\n{equipment}:")

        for sensor in sensor_columns:
            if sensor in equipment_data.columns and not equipment_data[sensor].isna().all():
                min_val = equipment_data[sensor].min()
                max_val = equipment_data[sensor].max()
                mean_val = equipment_data[sensor].mean()
                print(f"  {sensor}: {min_val:.1f} - {max_val:.1f} (avg: {mean_val:.1f})")

    # Display sample data
    print("\nSample Sensor Data:")
    print(sensor_data.head())

    print("\nSample Anomaly-Related Operator Logs:")
    if len(anomaly_logs) > 0:
        print(anomaly_logs[['timestamp', 'equipment', 'sensor_affected', 'anomaly_type', 'severity', 'entry']].head())

    print("\nSample Routine Operator Logs:")
    routine_logs = operator_logs[operator_logs['log_type'] == 'routine']
    print(routine_logs[['timestamp', 'operator', 'equipment', 'entry']].head())

    # Show time alignment between sensor anomalies and operator observations
    print(f"\nData Relationships:")
    print(f"Time range: {sensor_data['timestamp'].min()} to {sensor_data['timestamp'].max()}")
    print(f"Log time range: {operator_logs['timestamp'].min()} to {operator_logs['timestamp'].max()}")
    print(f"Equipment in sensor data: {sorted(sensor_data['equipment'].unique())}")
    print(f"Equipment in logs: {sorted(operator_logs['equipment'].unique())}")

    # Log type distribution
    print(f"\nLog Type Distribution:")
    print(operator_logs['log_type'].value_counts())

    # Data completeness summary for preprocessing pipeline
    print(f"\n=== PREPROCESSING PIPELINE READINESS ===")
    columns_to_drop = []
    columns_to_keep = []

    for sensor in sensor_columns:
        missing_pct = sensor_data[sensor].isna().sum() / len(sensor_data) * 100
        if missing_pct > 95:
            columns_to_drop.append(sensor)
        else:
            columns_to_keep.append(sensor)

    print(f"Columns that will be KEPT (≤95% missing): {len(columns_to_keep)}")
    for col in columns_to_keep:
        missing_pct = sensor_data[col].isna().sum() / len(sensor_data) * 100
        print(f"  {col}: {missing_pct:.1f}% missing")

    if columns_to_drop:
        print(f"\nColumns that will be DROPPED (>95% missing): {len(columns_to_drop)}")
        for col in columns_to_drop:
            missing_pct = sensor_data[col].isna().sum() / len(sensor_data) * 100
            print(f"  {col}: {missing_pct:.1f}% missing")
    else:
        print(f"\nNo columns will be dropped! All sensors have <95% missing data.")

    expected_quality_score = 100 - (sum(sensor_data[sensor_columns].isna().sum()) / (len(sensor_data) * len(sensor_columns)) * 100)
    print(f"\nExpected data quality score: {expected_quality_score:.1f}/100")

    return sensor_data, operator_logs, generator.anomaly_events

if __name__ == "__main__":
    # Generate the integrated data (assumes the OilRigDataGenerator class is available)
    generator = OilRigDataGenerator(start_date='2024-01-01', months=6)
    sensor_data = generator.generate_sensor_data()
    operator_logs = generator.generate_operator_logs(additional_routine_entries=100)

"""####Visualize Generated Data"""

def visualize_integrated_oil_rig_data(sensor_data, operator_logs):

    # Set up the plotting style
    plt.style.use('seaborn-v0_8')
    fig = plt.figure(figsize=(20, 16))

    # Filter anomaly-related logs
    anomaly_logs = operator_logs[operator_logs['log_type'].isin(['anomaly_observation', 'followup_action'])]

    # 1. Multi-sensor time series with anomaly markers
    ax1 = plt.subplot(3, 3, (1, 3))
    pump_data = sensor_data[sensor_data['equipment'] == 'centrifugal_pump_01'].copy()

    # Plot multiple sensors with different scales
    ax1_twin = ax1.twinx()
    ax1_twin2 = ax1.twinx()
    ax1_twin2.spines['right'].set_position(('outward', 60))

    # Primary axis - Pressure
    line1 = ax1.plot(pump_data['timestamp'], pump_data['suction_pressure'],
                     label='Suction Pressure (bar)', color='blue', alpha=0.7, linewidth=1)
    line2 = ax1.plot(pump_data['timestamp'], pump_data['discharge_pressure'],
                     label='Discharge Pressure (bar)', color='darkblue', alpha=0.7, linewidth=1)

    # Secondary axis - Flow rate
    line3 = ax1_twin.plot(pump_data['timestamp'], pump_data['flow_rate'],
                         label='Flow Rate (m³/h)', color='green', alpha=0.7, linewidth=1)

    # Third axis - Temperature and Vibration
    line4 = ax1_twin2.plot(pump_data['timestamp'], pump_data['temperature'],
                          label='Temperature (°C)', color='red', alpha=0.7, linewidth=1)
    line5 = ax1_twin2.plot(pump_data['timestamp'], pump_data['vibration']*20,  # Scale for visibility
                          label='Vibration (mm/s × 20)', color='orange', alpha=0.7, linewidth=1)

    # Add operator observation markers
    pump_anomalies = anomaly_logs[anomaly_logs['equipment'] == 'centrifugal_pump_01']
    colors = {'high': 'red', 'medium': 'orange', 'low': 'yellow'}

    for _, anomaly in pump_anomalies.iterrows():
        color = colors.get(anomaly.get('severity', 'medium'), 'orange')
        ax1.axvline(x=anomaly['timestamp'], color=color, linestyle='--', alpha=0.8, linewidth=2)

        # Add text annotation for significant anomalies
        if anomaly.get('severity') == 'high':
            ax1.annotate(f'{anomaly.get("anomaly_type", "anomaly")}',
                        xy=(anomaly['timestamp'], ax1.get_ylim()[1] * 0.9),
                        rotation=90, fontsize=8, alpha=0.7)

    # Formatting
    ax1.set_xlabel('Time')
    ax1.set_ylabel('Pressure (bar)', color='blue')
    ax1_twin.set_ylabel('Flow Rate (m³/h)', color='green')
    ax1_twin2.set_ylabel('Temperature (°C) / Vibration', color='red')
    ax1.set_title('Centrifugal Pump 01 - Sensor Data with Operator Observations', fontsize=14, fontweight='bold')

    # Combine legends
    lines = line1 + line2 + line3 + line4 + line5
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='upper left', bbox_to_anchor=(0, 1))

    # Format dates on x-axis
    ax1.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)

    # 2. Sensor correlation heatmap
    ax2 = plt.subplot(3, 3, 4)
    pump_sensors = ['suction_pressure', 'discharge_pressure', 'flow_rate', 'temperature', 'vibration', 'power']
    corr_matrix = pump_data[pump_sensors].corr()

    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                square=True, cbar_kws={'shrink': 0.8}, ax=ax2)
    ax2.set_title('Sensor Correlation Matrix\n(Centrifugal Pump)', fontsize=12, fontweight='bold')

    # 3. Anomaly detection timeline with log entries
    ax3 = plt.subplot(3, 3, 5)
    if len(anomaly_logs) > 0:
        # Create daily anomaly counts
        anomaly_logs_copy = anomaly_logs.copy()
        anomaly_logs_copy['date'] = anomaly_logs_copy['timestamp'].dt.date
        daily_anomalies = anomaly_logs_copy.groupby(['date', 'severity']).size().reset_index()
        daily_anomalies.columns = ['date', 'severity', 'count']

        # Pivot for stacked bar chart
        if len(daily_anomalies) > 0:
            anomaly_pivot = daily_anomalies.pivot(index='date', columns='severity', values='count').fillna(0)
            colors_dict = {'low': '#2ca02c', 'medium': '#ff7f0e', 'high': '#d62728'}
            anomaly_pivot.plot(kind='bar', stacked=True, ax=ax3,
                              color=[colors_dict.get(col, '#1f77b4') for col in anomaly_pivot.columns])
            ax3.set_title('Daily Anomaly Observations by Severity', fontsize=12, fontweight='bold')
            ax3.set_xlabel('Date')
            ax3.set_ylabel('Number of Observations')
            plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
        else:
            ax3.text(0.5, 0.5, 'No anomaly logs found', ha='center', va='center', transform=ax3.transAxes)
    else:
        ax3.text(0.5, 0.5, 'No anomaly logs generated', ha='center', va='center', transform=ax3.transAxes)
        ax3.set_title('Anomaly Observations Timeline', fontsize=12, fontweight='bold')

    # 4. Equipment maintenance vs anomaly correlation
    ax4 = plt.subplot(3, 3, 6)
    maintenance_logs = operator_logs[operator_logs['log_type'].isin(['maintenance_prep', 'maintenance_complete'])]

    # Count logs by equipment
    equipment_stats = []
    for equipment in operator_logs['equipment'].unique():
        if equipment != 'General':
            maintenance_count = len(maintenance_logs[maintenance_logs['equipment'] == equipment])
            anomaly_count = len(anomaly_logs[anomaly_logs['equipment'] == equipment])
            equipment_stats.append({
                'equipment': equipment,
                'maintenance': maintenance_count,
                'anomalies': anomaly_count
            })

    if equipment_stats:
        stats_df = pd.DataFrame(equipment_stats)
        x = np.arange(len(stats_df))
        width = 0.35

        bars1 = ax4.bar(x - width/2, stats_df['maintenance'], width, label='Maintenance Events', alpha=0.8)
        bars2 = ax4.bar(x + width/2, stats_df['anomalies'], width, label='Anomaly Observations', alpha=0.8)

        ax4.set_title('Maintenance Events vs Anomaly Observations', fontsize=12, fontweight='bold')
        ax4.set_xlabel('Equipment')
        ax4.set_ylabel('Count')
        ax4.set_xticks(x)
        ax4.set_xticklabels([eq.replace('_', '\n') for eq in stats_df['equipment']], rotation=45, ha='right')
        ax4.legend()

    # 5. Operator log types distribution
    ax5 = plt.subplot(3, 3, 7)
    log_type_counts = operator_logs['log_type'].value_counts()
    colors = plt.cm.Set3(np.linspace(0, 1, len(log_type_counts)))

    wedges, texts, autotexts = ax5.pie(log_type_counts.values, labels=log_type_counts.index,
                                      autopct='%1.1f%%', colors=colors)
    ax5.set_title('Distribution of Operator Log Types', fontsize=12, fontweight='bold')

    # 6. Time-based analysis: Shift vs anomaly observations
    ax6 = plt.subplot(3, 3, 8)
    if len(anomaly_logs) > 0:
        shift_anomalies = anomaly_logs['shift'].value_counts()
        bars = ax6.bar(shift_anomalies.index, shift_anomalies.values,
                      color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax6.set_title('Anomaly Observations by Shift', fontsize=12, fontweight='bold')
        ax6.set_xlabel('Shift')
        ax6.set_ylabel('Number of Observations')

        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{int(height)}', ha='center', va='bottom')
    else:
        ax6.text(0.5, 0.5, 'No shift data available', ha='center', va='center', transform=ax6.transAxes)
        ax6.set_title('Anomaly Observations by Shift', fontsize=12, fontweight='bold')

    # 7. Response time analysis (time between anomaly occurrence and log entry)
    ax7 = plt.subplot(3, 3, 9)
    if len(anomaly_logs) > 0 and 'severity' in anomaly_logs.columns:
        severity_counts = anomaly_logs['severity'].value_counts()
        severity_colors = {'low': '#2ca02c', 'medium': '#ff7f0e', 'high': '#d62728'}

        bars = ax7.bar(severity_counts.index, severity_counts.values,
                       color=[severity_colors.get(s, '#1f77b4') for s in severity_counts.index])
        ax7.set_title('Anomaly Observations by Severity', fontsize=12, fontweight='bold')
        ax7.set_xlabel('Severity Level')
        ax7.set_ylabel('Count')

        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax7.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{int(height)}', ha='center', va='bottom')
    else:
        ax7.text(0.5, 0.5, 'No severity data available', ha='center', va='center', transform=ax7.transAxes)
        ax7.set_title('Anomaly Severity Distribution', fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.show()

    # Additional detailed analysis plot
    if len(anomaly_logs) > 0:
        fig2, ((ax8, ax9), (ax10, ax11)) = plt.subplots(2, 2, figsize=(16, 12))

        # 8. Sensor-wise anomaly analysis
        if 'sensor_affected' in anomaly_logs.columns:
            sensor_anomalies = anomaly_logs['sensor_affected'].value_counts().head(10)
            ax8.barh(sensor_anomalies.index, sensor_anomalies.values, alpha=0.8)
            ax8.set_title('Top 10 Sensors with Anomaly Observations', fontsize=14, fontweight='bold')
            ax8.set_xlabel('Number of Observations')

        # 9. Equipment vs anomaly type heatmap
        if 'anomaly_type' in anomaly_logs.columns:
            anomaly_equipment_type = anomaly_logs.groupby(['equipment', 'anomaly_type']).size().reset_index()
            anomaly_equipment_type.columns = ['equipment', 'type', 'count']
            anomaly_heatmap = anomaly_equipment_type.pivot(index='equipment', columns='type', values='count').fillna(0)

            sns.heatmap(anomaly_heatmap, annot=True, fmt='.0f', cmap='YlOrRd', ax=ax9)
            ax9.set_title('Anomaly Types by Equipment', fontsize=14, fontweight='bold')
            ax9.set_xlabel('Anomaly Type')
            ax9.set_ylabel('Equipment')

        # 10. Time distribution of anomalies (hourly)
        anomaly_logs['hour'] = anomaly_logs['timestamp'].dt.hour
        hourly_anomalies = anomaly_logs.groupby('hour').size()

        ax10.bar(hourly_anomalies.index, hourly_anomalies.values, alpha=0.7, color='coral')
        ax10.set_title('Anomaly Observations by Hour of Day', fontsize=14, fontweight='bold')
        ax10.set_xlabel('Hour')
        ax10.set_ylabel('Number of Observations')
        ax10.set_xticks(range(0, 24, 2))

        # Add shift boundaries
        ax10.axvline(x=6, color='gray', linestyle='--', alpha=0.5, label='Day Shift Start')
        ax10.axvline(x=14, color='gray', linestyle='--', alpha=0.5, label='Evening Shift Start')
        ax10.axvline(x=22, color='gray', linestyle='--', alpha=0.5, label='Night Shift Start')
        ax10.legend()

        # 11. Log entry text analysis (word frequency)
        from collections import Counter
        import re

        # Combine all anomaly-related log entries
        all_entries = ' '.join(anomaly_logs['entry'].fillna('').values)
        # Clean and extract words
        words = re.findall(r'\b[a-zA-Z]{4,}\b', all_entries.lower())
        # Filter out common words
        stop_words = {'equipment', 'reading', 'system', 'normal', 'completed', 'minutes', 'hours'}
        filtered_words = [w for w in words if w not in stop_words]

        if filtered_words:
            word_freq = Counter(filtered_words)
            common_words = dict(word_freq.most_common(15))

            ax11.barh(list(common_words.keys()), list(common_words.values()), alpha=0.8)
            ax11.set_title('Most Common Words in Anomaly Log Entries', fontsize=14, fontweight='bold')
            ax11.set_xlabel('Frequency')
        else:
            ax11.text(0.5, 0.5, 'No text data available for analysis', ha='center', va='center', transform=ax11.transAxes)
            ax11.set_title('Log Entry Text Analysis', fontsize=14, fontweight='bold')

        plt.tight_layout()
        plt.show()

def demonstrate_data_relationships(sensor_data, operator_logs):

    print("=== SENSOR DATA & OPERATOR LOG RELATIONSHIPS ===\n")

    # Filter anomaly-related logs
    anomaly_logs = operator_logs[operator_logs['log_type'].isin(['anomaly_observation', 'followup_action'])]

    if len(anomaly_logs) == 0:
        print("No anomaly observations found in operator logs.")
        return

    # Show a few examples of anomaly events and corresponding logs
    print("Sample Anomaly Events and Corresponding Operator Observations:\n")

    for i, (_, log_entry) in enumerate(anomaly_logs.head(3).iterrows()):
        print(f"EXAMPLE {i+1}:")
        print(f"Timestamp: {log_entry['timestamp']}")
        print(f"Equipment: {log_entry['equipment']}")
        print(f"Operator: {log_entry['operator']} ({log_entry['shift']} shift)")

        if 'sensor_affected' in log_entry and pd.notna(log_entry['sensor_affected']):
            print(f"Sensor Affected: {log_entry['sensor_affected']}")
        if 'anomaly_type' in log_entry and pd.notna(log_entry['anomaly_type']):
            print(f"Anomaly Type: {log_entry['anomaly_type']}")
        if 'severity' in log_entry and pd.notna(log_entry['severity']):
            print(f"Severity: {log_entry['severity']}")

        print(f"Log Entry: {log_entry['entry']}")
        print("-" * 80)

    # Show statistics about the relationship
    print(f"\nRELATIONSHIP STATISTICS:")
    print(f"Total sensor data points: {len(sensor_data):,}")
    print(f"Total operator log entries: {len(operator_logs)}")
    print(f"Anomaly-related log entries: {len(anomaly_logs)} ({len(anomaly_logs)/len(operator_logs)*100:.1f}% of all logs)")

    if 'severity' in anomaly_logs.columns:
        print(f"\nAnomaly Severity Distribution:")
        severity_dist = anomaly_logs['severity'].value_counts()
        for severity, count in severity_dist.items():
            print(f"  {severity.capitalize()}: {count} ({count/len(anomaly_logs)*100:.1f}%)")

    # Show maintenance correlation
    maintenance_logs = operator_logs[operator_logs['log_type'].isin(['maintenance_prep', 'maintenance_complete'])]
    print(f"\nMaintenance Events: {len(maintenance_logs)}")

    print(f"\nTime Coverage:")
    print(f"Sensor data: {sensor_data['timestamp'].min()} to {sensor_data['timestamp'].max()}")
    print(f"Operator logs: {operator_logs['timestamp'].min()} to {operator_logs['timestamp'].max()}")

if __name__ == "__main__":
    # Generate the integrated data (assumes the OilRigDataGenerator class is available)
    #generator = OilRigDataGenerator(start_date='2024-01-01', months=6)
    #sensor_data = generator.generate_sensor_data()
    #operator_logs = generator.generate_operator_logs(additional_routine_entries=100)

    # Show relationships
    demonstrate_data_relationships(sensor_data, operator_logs)

    # Create visualizations
    visualize_integrated_oil_rig_data(sensor_data, operator_logs)

    print("Integrated data visualization complete!")
    print(f"Generated {len(sensor_data)} sensor readings with corresponding operator observations")
    print(f"Generated {len(operator_logs)} total operator log entries")

"""## Module 1: Oil Rig Data Pre-processor

####Load additional packages for pre-processor
"""

import feature_engine.imputation as mdi

"""####Oil Rig Data Preprocessor"""

sensor_data.head()

class DataPreprocessingConfig:
    """Configuration class for data preprocessing parameters"""

    def __init__(self):
        # Time series parameters
        self.target_frequency = '5T'  # 5-minute intervals
        self.max_gap_minutes = 30     # Maximum gap to interpolate
        self.outlier_threshold = 3.0  # Z-score threshold for outliers

        # Missing data parameters
        self.missing_threshold = 0.95  # Drop columns with >95% missing
        self.imputation_method = 'knn'  # 'knn', 'forward_fill', 'interpolate'
        self.knn_neighbors = 5

        # Feature engineering parameters
        self.window_sizes = [12, 24, 48]  # Rolling windows (hours)
        self.lag_periods = [1, 3, 6, 12]  # Lag features (5-min periods)
        self.seasonal_periods = [288]     # Daily seasonality (288 * 5min = 24h)

        # Data quality parameters
        self.quality_checks = {
            'completeness': True,
            'consistency': True,
            'validity': True,
            'anomaly_detection': True
        }

        # Sensor-specific parameters
        self.sensor_configs = {
            'pressure': {'unit': 'bar', 'min_val': 0, 'max_val': 500},
            'temperature': {'unit': '°C', 'min_val': -50, 'max_val': 300},
            'vibration': {'unit': 'mm/s', 'min_val': 0, 'max_val': 100},
            'flow_rate': {'unit': 'm³/h', 'min_val': 0, 'max_val': 10000},
            'power': {'unit': 'kW', 'min_val': 0, 'max_val': 2000},
            'voltage': {'unit': 'V', 'min_val': 100, 'max_val': 500},
            'current': {'unit': 'A', 'min_val': 0, 'max_val': 1000},
            'frequency': {'unit': 'Hz', 'min_val': 45, 'max_val': 55},
            'rpm': {'unit': 'RPM', 'min_val': 0, 'max_val': 5000},
            'torque': {'unit': 'N⋅m', 'min_val': 0, 'max_val': 50000}
        }

class OilRigDataPreprocessor:
    """
    Comprehensive data preprocessing pipeline for oil rig sensor data and operator logs
    """

    def __init__(self, config: Optional[DataPreprocessingConfig] = None):
        self.config = config or DataPreprocessingConfig()
        self.scalers = {}
        self.imputers = {}
        self.data_quality_report = {}
        self.feature_names = []
        self.preprocessing_log = []

        # Initialize plotting style
        plt.style.use('seaborn-v0_8')

    def log_step(self, step_name: str, details: str):
        """Log preprocessing steps for audit trail"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.preprocessing_log.append({
            'timestamp': timestamp,
            'step': step_name,
            'details': details
        })
        print(f"[{timestamp}] {step_name}: {details}")

    def ingest_data(self, sensor_data: pd.DataFrame, operator_logs: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Ingest and perform initial validation of sensor data and operator logs

        Args:
            sensor_data: DataFrame with sensor readings
            operator_logs: DataFrame with operator log entries

        Returns:
            Tuple of validated DataFrames
        """
        self.log_step("DATA_INGESTION", f"Ingesting {len(sensor_data)} sensor records and {len(operator_logs)} log entries")

        # Basic validation
        sensor_data = self._validate_sensor_data(sensor_data)
        operator_logs = self._validate_operator_logs(operator_logs)

        # Store original data info
        self.original_sensor_shape = sensor_data.shape
        self.original_logs_shape = operator_logs.shape

        return sensor_data, operator_logs

    def _validate_sensor_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Validate sensor data structure and types"""
        required_columns = ['timestamp', 'equipment']

        # Check required columns
        missing_cols = [col for col in required_columns if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")

        # Convert timestamp
        if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
            df['timestamp'] = pd.to_datetime(df['timestamp'])

        # Sort by timestamp
        df = df.sort_values('timestamp').reset_index(drop=True)

        # Identify sensor columns (numeric columns excluding metadata)
        metadata_cols = ['timestamp', 'equipment']
        sensor_cols = [col for col in df.columns if col not in metadata_cols and pd.api.types.is_numeric_dtype(df[col])]

        self.log_step("VALIDATION", f"Found {len(sensor_cols)} sensor columns: {sensor_cols}")

        return df

    def _validate_operator_logs(self, df: pd.DataFrame) -> pd.DataFrame:
        """Validate operator logs structure"""
        required_columns = ['timestamp', 'entry']

        # Check required columns
        missing_cols = [col for col in required_columns if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns in operator logs: {missing_cols}")

        # Convert timestamp
        if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
            df['timestamp'] = pd.to_datetime(df['timestamp'])

        # Sort by timestamp
        df = df.sort_values('timestamp').reset_index(drop=True)

        # Clean text entries
        df['entry'] = df['entry'].astype(str).str.strip()

        self.log_step("VALIDATION", f"Validated {len(df)} operator log entries")

        return df

    def assess_data_quality(self, sensor_data: pd.DataFrame) -> Dict:
        """
        Comprehensive data quality assessment

        Args:
            sensor_data: Sensor DataFrame

        Returns:
            Dictionary containing quality metrics
        """
        self.log_step("DATA_QUALITY", "Starting comprehensive data quality assessment")

        quality_report = {}

        # Get sensor columns
        metadata_cols = ['timestamp', 'equipment']
        sensor_cols = [col for col in sensor_data.columns if col not in metadata_cols]

        # 1. Completeness Analysis
        completeness = {}
        for col in sensor_cols:
            missing_pct = sensor_data[col].isnull().sum() / len(sensor_data) * 100
            completeness[col] = {
                'missing_count': sensor_data[col].isnull().sum(),
                'missing_percentage': missing_pct,
                'status': 'GOOD' if missing_pct < 5 else 'WARNING' if missing_pct < 20 else 'CRITICAL'
            }

        quality_report['completeness'] = completeness

        # 2. Consistency Analysis (time gaps, duplicate timestamps)
        time_diffs = sensor_data['timestamp'].diff().dt.total_seconds() / 60  # in minutes
        expected_interval = 5  # 5 minutes

        consistency = {
            'time_gaps': {
                'large_gaps_count': (time_diffs > expected_interval * 3).sum(),
                'max_gap_minutes': time_diffs.max(),
                'avg_interval_minutes': time_diffs.mean()
            },
            'duplicates': {
                'duplicate_timestamps': sensor_data['timestamp'].duplicated().sum(),
                'total_duplicates': sensor_data.duplicated().sum()
            }
        }

        quality_report['consistency'] = consistency

        # 3. Validity Analysis (range checks)
        validity = {}
        for col in sensor_cols:
            if col in self.config.sensor_configs:
                config = self.config.sensor_configs[col]
                col_data = sensor_data[col].dropna()

                validity[col] = {
                    'min_value': col_data.min(),
                    'max_value': col_data.max(),
                    'expected_min': config['min_val'],
                    'expected_max': config['max_val'],
                    'out_of_range_count': ((col_data < config['min_val']) | (col_data > config['max_val'])).sum(),
                    'out_of_range_percentage': ((col_data < config['min_val']) | (col_data > config['max_val'])).sum() / len(col_data) * 100
                }
            else:
                # Generic validity check using statistical outliers
                col_data = sensor_data[col].dropna()
                q1, q3 = col_data.quantile([0.25, 0.75])
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr

                validity[col] = {
                    'min_value': col_data.min(),
                    'max_value': col_data.max(),
                    'statistical_outliers': ((col_data < lower_bound) | (col_data > upper_bound)).sum(),
                    'outlier_percentage': ((col_data < lower_bound) | (col_data > upper_bound)).sum() / len(col_data) * 100
                }

        quality_report['validity'] = validity

        # 4. Anomaly Detection using Isolation Forest
        if self.config.quality_checks['anomaly_detection']:
            anomalies = {}
            for equipment in sensor_data['equipment'].unique():
                equip_data = sensor_data[sensor_data['equipment'] == equipment]
                equip_sensors = [col for col in sensor_cols if col in equip_data.columns]

                if len(equip_sensors) > 1:  # Need multiple features for IF
                    sensor_matrix = equip_data[equip_sensors].dropna()
                    if len(sensor_matrix) > 10:  # Minimum samples for IF
                        clf = IForest(contamination=0.1, random_state=42)
                        anomaly_labels = clf.fit_predict(sensor_matrix)
                        anomaly_count = (anomaly_labels == -1).sum()

                        anomalies[equipment] = {
                            'anomaly_count': int(anomaly_count),
                            'anomaly_percentage': float(anomaly_count / len(sensor_matrix) * 100),
                            'total_samples': len(sensor_matrix)
                        }

            quality_report['anomalies'] = anomalies

        # 5. Overall Quality Score
        overall_score = self._calculate_quality_score(quality_report)
        quality_report['overall_score'] = overall_score

        self.data_quality_report = quality_report

        self.log_step("DATA_QUALITY", f"Quality assessment complete. Overall score: {overall_score:.2f}/100")

        return quality_report

    def _calculate_quality_score(self, quality_report: Dict) -> float:
        """Calculate overall data quality score (0-100)"""
        scores = []

        # Completeness score (40% weight)
        if 'completeness' in quality_report:
            completeness_scores = []
            for col, metrics in quality_report['completeness'].items():
                if metrics['missing_percentage'] < 5:
                    completeness_scores.append(100)
                elif metrics['missing_percentage'] < 20:
                    completeness_scores.append(70)
                else:
                    completeness_scores.append(30)

            if completeness_scores:
                scores.append(('completeness', np.mean(completeness_scores), 0.4))

        # Consistency score (30% weight)
        if 'consistency' in quality_report:
            consistency_score = 100
            if quality_report['consistency']['duplicates']['duplicate_timestamps'] > 0:
                consistency_score -= 20
            if quality_report['consistency']['time_gaps']['large_gaps_count'] > 10:
                consistency_score -= 30

            scores.append(('consistency', consistency_score, 0.3))

        # Validity score (30% weight)
        if 'validity' in quality_report:
            validity_scores = []
            for col, metrics in quality_report['validity'].items():
                if 'out_of_range_percentage' in metrics:
                    pct = metrics['out_of_range_percentage']
                elif 'outlier_percentage' in metrics:
                    pct = metrics['outlier_percentage']
                else:
                    continue

                if pct < 1:
                    validity_scores.append(100)
                elif pct < 5:
                    validity_scores.append(80)
                else:
                    validity_scores.append(50)

            if validity_scores:
                scores.append(('validity', np.mean(validity_scores), 0.3))

        # Calculate weighted average
        if scores:
            weighted_score = sum(score * weight for _, score, weight in scores) / sum(weight for _, _, weight in scores)
        else:
            weighted_score = 0

        return weighted_score

    def handle_missing_data(self, sensor_data: pd.DataFrame) -> pd.DataFrame:
        """
        Handle missing data using multiple strategies
        """
        self.log_step("MISSING_DATA", f"Handling missing data using {self.config.imputation_method} method")

        # Get sensor columns
        metadata_cols = ['timestamp', 'equipment']
        sensor_cols = [col for col in sensor_data.columns if col not in metadata_cols]

        # Drop columns with too much missing data
        cols_to_drop = []
        for col in sensor_cols:
            missing_pct = sensor_data[col].isnull().sum() / len(sensor_data)
            if missing_pct > (1 - self.config.missing_threshold):
                cols_to_drop.append(col)

        if cols_to_drop:
            sensor_data = sensor_data.drop(columns=cols_to_drop)
            self.log_step("MISSING_DATA", f"Dropped columns with >95% missing data: {cols_to_drop}")

        # Update sensor columns list
        sensor_cols = [col for col in sensor_cols if col not in cols_to_drop]

        # Handle missing data by equipment (to maintain correlations)
        processed_dfs = []

        for equipment in sensor_data['equipment'].unique():
            equip_data = sensor_data[sensor_data['equipment'] == equipment].copy()
            equip_sensors = [col for col in sensor_cols if col in equip_data.columns and equip_data[col].notna().sum() > 0]

            if not equip_sensors:
                continue

            if self.config.imputation_method == 'knn':
                # KNN Imputation
                imputer = KNNImputer(n_neighbors=self.config.knn_neighbors)
                equip_data[equip_sensors] = imputer.fit_transform(equip_data[equip_sensors])
                self.imputers[equipment] = imputer

            elif self.config.imputation_method == 'forward_fill':
                # Forward fill with limit
                equip_data[equip_sensors] = equip_data[equip_sensors].fillna(method='ffill', limit=6)  # 30 minutes
                equip_data[equip_sensors] = equip_data[equip_sensors].fillna(method='bfill', limit=6)

            elif self.config.imputation_method == 'interpolate':
                # Linear interpolation
                equip_data[equip_sensors] = equip_data[equip_sensors].interpolate(method='linear', limit=6)
                equip_data[equip_sensors] = equip_data[equip_sensors].fillna(method='bfill')

            processed_dfs.append(equip_data)

        if processed_dfs:
            result = pd.concat(processed_dfs, ignore_index=True)
            result = result.sort_values('timestamp').reset_index(drop=True)

            missing_after = result[sensor_cols].isnull().sum().sum()
            self.log_step("MISSING_DATA", f"Imputation complete. Remaining missing values: {missing_after}")

            return result
        else:
            self.log_step("MISSING_DATA", "No valid data found for imputation")
            return sensor_data

    def engineer_features(self, sensor_data: pd.DataFrame) -> pd.DataFrame:
        """
        Engineer time series features for anomaly detection
        """
        self.log_step("FEATURE_ENGINEERING", "Creating time series features")

        # Get sensor columns
        metadata_cols = ['timestamp', 'equipment']
        sensor_cols = [col for col in sensor_data.columns if col not in metadata_cols and pd.api.types.is_numeric_dtype(sensor_data[col])]

        feature_dfs = []

        for equipment in sensor_data['equipment'].unique():
            equip_data = sensor_data[sensor_data['equipment'] == equipment].copy()
            equip_data = equip_data.set_index('timestamp').sort_index()

            # 1. Rolling statistics
            for window in self.config.window_sizes:
                window_periods = window * 12  # Convert hours to 5-minute periods

                for sensor in sensor_cols:
                    if sensor in equip_data.columns:
                        # Rolling mean, std, min, max
                        equip_data[f'{sensor}_rolling_mean_{window}h'] = equip_data[sensor].rolling(window_periods, min_periods=1).mean()
                        equip_data[f'{sensor}_rolling_std_{window}h'] = equip_data[sensor].rolling(window_periods, min_periods=1).std()
                        equip_data[f'{sensor}_rolling_min_{window}h'] = equip_data[sensor].rolling(window_periods, min_periods=1).min()
                        equip_data[f'{sensor}_rolling_max_{window}h'] = equip_data[sensor].rolling(window_periods, min_periods=1).max()

                        # Ratio to rolling mean (normalized features)
                        mean_col = f'{sensor}_rolling_mean_{window}h'
                        equip_data[f'{sensor}_ratio_to_mean_{window}h'] = equip_data[sensor] / (equip_data[mean_col] + 1e-8)

            # 2. Lag features
            for lag in self.config.lag_periods:
                for sensor in sensor_cols:
                    if sensor in equip_data.columns:
                        equip_data[f'{sensor}_lag_{lag}'] = equip_data[sensor].shift(lag)
                        # Change from previous periods
                        equip_data[f'{sensor}_change_{lag}'] = equip_data[sensor] - equip_data[f'{sensor}_lag_{lag}']

            # 3. Time-based features
            equip_data['hour'] = equip_data.index.hour
            equip_data['day_of_week'] = equip_data.index.dayofweek
            equip_data['is_weekend'] = (equip_data.index.dayofweek >= 5).astype(int)
            equip_data['is_business_hours'] = ((equip_data.index.hour >= 6) & (equip_data.index.hour <= 18)).astype(int)

            # 4. Statistical features
            for sensor in sensor_cols:
                if sensor in equip_data.columns:
                    # Z-score (standardized values)
                    equip_data[f'{sensor}_zscore'] = (equip_data[sensor] - equip_data[sensor].mean()) / (equip_data[sensor].std() + 1e-8)

                    # Rate of change
                    equip_data[f'{sensor}_rate_of_change'] = equip_data[sensor].pct_change()

                    # Smoothed values (Savitzky-Golay filter)
                    if len(equip_data) > 21:  # Need minimum points for filter
                        equip_data[f'{sensor}_smoothed'] = savgol_filter(equip_data[sensor].fillna(method='ffill'),
                                                                       window_length=21, polyorder=3)

            # Reset index and add equipment column back
            equip_data = equip_data.reset_index()
            equip_data['equipment'] = equipment
            feature_dfs.append(equip_data)

        # Combine all equipment data
        if feature_dfs:
            result = pd.concat(feature_dfs, ignore_index=True)
            result = result.sort_values('timestamp').reset_index(drop=True)

            # Store feature names for later use
            self.feature_names = [col for col in result.columns if col not in ['timestamp', 'equipment']]

            self.log_step("FEATURE_ENGINEERING", f"Created {len(self.feature_names)} features")

            return result
        else:
            return sensor_data

    def normalize_data(self, sensor_data: pd.DataFrame, method: str = 'robust') -> pd.DataFrame:
        """
        Normalize sensor data using various scaling methods

        Args:
            sensor_data: DataFrame with sensor data
            method: 'standard', 'minmax', or 'robust'
        """
        self.log_step("NORMALIZATION", f"Normalizing data using {method} scaling")

        # Get numeric columns (excluding metadata)
        metadata_cols = ['timestamp', 'equipment', 'hour', 'day_of_week', 'is_weekend', 'is_business_hours']
        numeric_cols = [col for col in sensor_data.columns
                       if col not in metadata_cols and pd.api.types.is_numeric_dtype(sensor_data[col])]

        # Choose scaler
        if method == 'standard':
            scaler_class = StandardScaler
        elif method == 'minmax':
            scaler_class = MinMaxScaler
        elif method == 'robust':
            scaler_class = RobustScaler
        else:
            raise ValueError(f"Unknown scaling method: {method}")

        # Scale by equipment to maintain correlations
        processed_dfs = []

        for equipment in sensor_data['equipment'].unique():
            equip_data = sensor_data[sensor_data['equipment'] == equipment].copy()

            # Select columns that have data for this equipment
            equip_numeric_cols = [col for col in numeric_cols if col in equip_data.columns and equip_data[col].notna().sum() > 0]

            if equip_numeric_cols:
                scaler = scaler_class()
                equip_data[equip_numeric_cols] = scaler.fit_transform(equip_data[equip_numeric_cols])
                self.scalers[equipment] = scaler

            processed_dfs.append(equip_data)

        # Combine results
        result = pd.concat(processed_dfs, ignore_index=True)
        result = result.sort_values('timestamp').reset_index(drop=True)

        self.log_step("NORMALIZATION", f"Normalized {len(numeric_cols)} numeric columns")

        return result

    def preprocess_pipeline(self, sensor_data: pd.DataFrame, operator_logs: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:
        """
        Complete preprocessing pipeline

        Args:
            sensor_data: Raw sensor data
            operator_logs: Raw operator logs

        Returns:
            Tuple of (processed_sensor_data, processed_operator_logs, quality_report)
        """
        self.log_step("PIPELINE_START", "Starting complete preprocessing pipeline")

        # 1. Data Ingestion
        sensor_data, operator_logs = self.ingest_data(sensor_data, operator_logs)

        # 2. Data Quality Assessment
        quality_report = self.assess_data_quality(sensor_data)

        # 3. Handle Missing Data
        sensor_data = self.handle_missing_data(sensor_data)

        # 4. Feature Engineering
        sensor_data = self.engineer_features(sensor_data)

        # 5. Data Normalization
        sensor_data = self.normalize_data(sensor_data, method='robust')

        self.log_step("PIPELINE_COMPLETE", f"Preprocessing complete. Final sensor data shape: {sensor_data.shape}")

        return sensor_data, operator_logs, quality_report

    def visualize_data_quality(self, quality_report: Optional[Dict] = None):
        """
        Create comprehensive data quality visualization
        """
        if quality_report is None:
            quality_report = self.data_quality_report

        if not quality_report:
            print("No quality report available. Run assess_data_quality() first.")
            return

        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('Data Completeness', 'Validity Check', 'Anomaly Detection', 'Overall Quality'),
            specs=[[{"type": "bar"}, {"type": "bar"}],
                   [{"type": "bar"}, {"type": "indicator"}]]
        )

        # 1. Completeness
        if 'completeness' in quality_report:
            sensors = list(quality_report['completeness'].keys())
            missing_pct = [quality_report['completeness'][s]['missing_percentage'] for s in sensors]
            colors = ['red' if pct > 20 else 'orange' if pct > 5 else 'green' for pct in missing_pct]

            fig.add_trace(go.Bar(x=sensors, y=missing_pct, name="Missing %", marker_color=colors), row=1, col=1)
            fig.update_xaxes(tickangle=45, row=1, col=1)
            fig.update_yaxes(title="Missing Percentage", row=1, col=1)

        # 2. Validity
        if 'validity' in quality_report:
            sensors = list(quality_report['validity'].keys())
            invalid_pct = []
            for s in sensors:
                if 'out_of_range_percentage' in quality_report['validity'][s]:
                    invalid_pct.append(quality_report['validity'][s]['out_of_range_percentage'])
                elif 'outlier_percentage' in quality_report['validity'][s]:
                    invalid_pct.append(quality_report['validity'][s]['outlier_percentage'])
                else:
                    invalid_pct.append(0)

            colors = ['red' if pct > 10 else 'orange' if pct > 2 else 'green' for pct in invalid_pct]
            fig.add_trace(go.Bar(x=sensors, y=invalid_pct, name="Invalid %", marker_color=colors), row=1, col=2)
            fig.update_xaxes(tickangle=45, row=1, col=2)
            fig.update_yaxes(title="Invalid Percentage", row=1, col=2)

        # 3. Anomalies
        if 'anomalies' in quality_report:
            equipment = list(quality_report['anomalies'].keys())
            anomaly_pct = [quality_report['anomalies'][e]['anomaly_percentage'] for e in equipment]

            fig.add_trace(go.Bar(x=equipment, y=anomaly_pct, name="Anomaly %", marker_color='orange'), row=2, col=1)
            fig.update_xaxes(tickangle=45, row=2, col=1)
            fig.update_yaxes(title="Anomaly Percentage", row=2, col=1)

        # 4. Overall Quality Score
        overall_score = quality_report.get('overall_score', 0)
        fig.add_trace(go.Indicator(
            mode="gauge+number+delta",
            value=overall_score,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': "Quality Score"},
            gauge={'axis': {'range': [None, 100]},
                   'bar': {'color': "darkblue"},
                   'steps': [
                       {'range': [0, 50], 'color': "lightgray"},
                       {'range': [50, 80], 'color': "yellow"},
                       {'range': [80, 100], 'color': "green"}],
                   'threshold': {'line': {'color': "red", 'width': 4},
                                'thickness': 0.75, 'value': 90}}), row=2, col=2)

        fig.update_layout(height=800, showlegend=False, title_text="Data Quality Assessment Dashboard")
        fig.show()

    def get_preprocessing_summary(self) -> Dict:
        """
        Get summary of preprocessing operations
        """
        summary = {
            'original_shapes': {
                'sensor_data': self.original_sensor_shape if hasattr(self, 'original_sensor_shape') else 'Unknown',
                'operator_logs': self.original_logs_shape if hasattr(self, 'original_logs_shape') else 'Unknown'
            },
            'scalers_fitted': list(self.scalers.keys()),
            'imputers_fitted': list(self.imputers.keys()),
            'features_created': len(self.feature_names),
            'processing_steps': len(self.preprocessing_log),
            'data_quality_score': self.data_quality_report.get('overall_score', 0) if self.data_quality_report else 0
        }

        return summary

# ============================================================================
# EXAMPLE USAGE AND TESTING
# ============================================================================

def test_preprocessing_pipeline():
    """
    Test the preprocessing pipeline with synthetic data
    """
    print("Oil Rig Data Preprocessing Pipeline")
    print("=" * 50)

    # For this test, we'll create a simple synthetic dataset
    # In real use, you would load your actual oil rig data

    # Create sample sensor data
    # timestamps = pd.date_range('2024-01-01', '2024-01-07', freq='5T')
    # equipment_list = ['pump_01', 'compressor_01', 'generator_01']

    # sensor_data_list = []

    # for equipment in equipment_list:
    #     for i, ts in enumerate(timestamps):
    #         # Add some realistic sensor patterns with missing values and outliers
    #         base_pressure = 50 + 10 * np.sin(2 * np.pi * i / (288))  # Daily cycle
    #         base_temp = 80 + 15 * np.sin(2 * np.pi * i / (288 * 7))  # Weekly cycle

    #         # Add noise and occasional missing values
    #         pressure = base_pressure + np.random.normal(0, 2)
    #         temperature = base_temp + np.random.normal(0, 3)

    #         # Simulate missing values (5% chance)
    #         if np.random.random() < 0.05:
    #             pressure = np.nan
    #         if np.random.random() < 0.03:
    #             temperature = np.nan

    #         # Simulate outliers (1% chance)
    #         if np.random.random() < 0.01:
    #             pressure *= 2
    #         if np.random.random() < 0.01:
    #             temperature *= 1.5

    #         sensor_data_list.append({
    #             'timestamp': ts,
    #             'equipment': equipment,
    #             'pressure': pressure,
    #             'temperature': temperature,
    #             'vibration': abs(np.random.normal(2, 0.5)),
    #             'power': np.random.normal(200, 20)
    #         })

    # sensor_data = pd.DataFrame(sensor_data_list)

    # # Create sample operator logs
    # log_timestamps = pd.date_range('2024-01-01', '2024-01-07', freq='4H')
    # operator_logs = pd.DataFrame({
    #     'timestamp': log_timestamps,
    #     'operator': ['John Doe', 'Jane Smith'] * (len(log_timestamps) // 2 + 1),
    #     'equipment': np.random.choice(equipment_list, len(log_timestamps)),
    #     'entry': ['Routine check completed', 'Minor vibration noted', 'System operating normally'] * (len(log_timestamps) // 3 + 1),
    #     'log_type': 'routine'
    # })[:len(log_timestamps)]

    # Initialize preprocessor
    config = DataPreprocessingConfig()
    preprocessor = OilRigDataPreprocessor(config)

    # Run preprocessing pipeline
    processed_sensor_data, processed_operator_logs, quality_report = preprocessor.preprocess_pipeline(
        sensor_data, operator_logs
    )

    # Display results
    print("\nPreprocessing Results:")
    print(f"Original sensor data shape: {sensor_data.shape}")
    print(f"Processed sensor data shape: {processed_sensor_data.shape}")
    print(f"Features created: {len(preprocessor.feature_names)}")
    print(f"Data quality score: {quality_report.get('overall_score', 0):.2f}/100")

    # Display sample of processed data
    print("\nSample processed data:")
    print(processed_sensor_data.head(10))

    # Show preprocessing log
    print("\nProcessing Log:")
    for log_entry in preprocessor.preprocessing_log[-5:]:  # Show last 5 steps
        print(f"  {log_entry['timestamp']}: {log_entry['step']} - {log_entry['details']}")

    # Visualize data quality
    preprocessor.visualize_data_quality()

    return preprocessor, processed_sensor_data, processed_operator_logs, quality_report

# Run Pre-processing
if __name__ == "__main__":
    preprocessor, processed_data, processed_logs, quality_report = test_preprocessing_pipeline()

    print("\n" + "="*50)
    print("Module 1: Data Preprocessing - COMPLETE")
    print("Ready for Module 2: Anomaly Detection")
    print("="*50)

# ============================================================================
# OPTIONAL: Detailed Analysis
# ============================================================================

def analyze_preprocessing_results(processed_sensor_data, quality_report, preprocessor):
    """
    Detailed analysis of preprocessing results
    """
    print("\n🔍 DETAILED PREPROCESSING ANALYSIS")
    print("=" * 50)

    # Feature breakdown
    features = preprocessor.feature_names
    feature_types = {
        'Original Sensors': len([f for f in features if not any(kw in f for kw in ['rolling', 'lag', 'zscore', 'change', 'ratio', 'hour', 'day'])]),
        'Rolling Features': len([f for f in features if 'rolling' in f]),
        'Lag Features': len([f for f in features if 'lag' in f]),
        'Time Features': len([f for f in features if f in ['hour', 'day_of_week', 'is_weekend', 'is_business_hours']]),
        'Statistical': len([f for f in features if any(kw in f for kw in ['zscore', 'change', 'ratio', 'smoothed'])])
    }

    print("Feature Engineering Breakdown:")
    for ftype, count in feature_types.items():
        print(f"   {ftype}: {count} features")

    # Equipment-wise analysis
    print(f"\nEquipment Analysis:")
    for equipment in processed_sensor_data['equipment'].unique():
        eq_data = processed_sensor_data[processed_sensor_data['equipment'] == equipment]
        missing_pct = (eq_data.isnull().sum().sum() / (len(eq_data) * len(eq_data.columns))) * 100
        print(f"   {equipment}: {len(eq_data)} records, {missing_pct:.1f}% missing after processing")

    # Memory usage
    memory_mb = processed_sensor_data.memory_usage(deep=True).sum() / 1024 / 1024
    print(f"\nMemory Usage: {memory_mb:.1f} MB")

    # Processing performance
    print(f"Processing Performance:")
    print(f"   Steps completed: {len(preprocessor.preprocessing_log)}")
    print(f"   Scalers fitted: {len(preprocessor.scalers)}")
    print(f"   Imputers fitted: {len(preprocessor.imputers)}")

    # Data readiness check
    print(f"\n✅ DATA READINESS FOR ML:")

    # Check for remaining missing values
    total_missing = processed_sensor_data.isnull().sum().sum()
    if total_missing == 0:
        print("   🟢 No missing values - Ready for ML")
    else:
        print(f"   🟡 {total_missing} missing values remaining - May need additional handling")

    # Check data types
    numeric_cols = processed_sensor_data.select_dtypes(include=[np.number]).columns
    print(f"   🟢 {len(numeric_cols)} numeric features ready for ML algorithms")

    # Check for infinite values
    infinite_count = np.isinf(processed_sensor_data.select_dtypes(include=[np.number])).sum().sum()
    if infinite_count == 0:
        print("   🟢 No infinite values detected")
    else:
        print(f"   🟡 {infinite_count} infinite values detected - may need handling")

# ============================================================================
# EXPORT FUNCTIONS
# ============================================================================

def save_processed_data(processed_sensor_data, processed_operator_logs, quality_report, preprocessor):
    """
    Save processed data and metadata for Module 2
    """
    print("\n💾 SAVING PROCESSED DATA")
    print("=" * 30)

    # Save main datasets
    processed_sensor_data.to_csv('processed_sensor_data.csv', index=False)
    processed_operator_logs.to_csv('processed_operator_logs.csv', index=False)
    print("✅ Saved processed_sensor_data.csv")
    print("✅ Saved processed_operator_logs.csv")

    # Save metadata
    metadata = {
        'feature_names': preprocessor.feature_names,
        'equipment_list': list(processed_sensor_data['equipment'].unique()),
        'scalers_fitted': list(preprocessor.scalers.keys()),
        'imputers_fitted': list(preprocessor.imputers.keys()),
        'data_quality_score': quality_report.get('overall_score', 0),
        'processing_timestamp': pd.Timestamp.now().isoformat(),
        'data_shape': processed_sensor_data.shape,
        'time_range': {
            'start': processed_sensor_data['timestamp'].min().isoformat(),
            'end': processed_sensor_data['timestamp'].max().isoformat()
        }
    }

    import json
    with open('preprocessing_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    print("✅ Saved preprocessing_metadata.json")

    # Save quality report
    with open('data_quality_report.json', 'w') as f:
        # Convert any non-serializable objects to strings
        serializable_report = json.loads(json.dumps(quality_report, default=str))
        json.dump(serializable_report, f, indent=2)
    print("✅ Saved data_quality_report.json")

    print(f"\n📁 Files saved for Module 2:")
    print("   • processed_sensor_data.csv - Main dataset for anomaly detection")
    print("   • processed_operator_logs.csv - Operator logs for text analysis")
    print("   • preprocessing_metadata.json - Feature names and processing info")
    print("   • data_quality_report.json - Data quality metrics")

# Example usage with saving
def complete_preprocessing_with_save():
    """
    Complete preprocessing workflow with data saving
    """
    # Run preprocessing
    #processed_data, processed_logs, quality, preprocessor = quick_preprocessing_demo()
    preprocessor, processed_data, processed_logs, quality = test_preprocessing_pipeline()

    # Detailed analysis
    analyze_preprocessing_results(processed_data, quality, preprocessor)

    # Save for Module 2
    save_processed_data(processed_data, processed_logs, quality, preprocessor)

    return processed_data, processed_logs, quality, preprocessor

analyze_preprocessing_results(processed_data, quality_report, preprocessor)

# preprocessor, processed_data, processed_logs, quality_report
save_processed_data(processed_data, processed_logs, quality_report, preprocessor)

"""##Module 2: Anomaly detection Analysis

####Load Packages required for this module
"""

!pip install ruptures tensorflow

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union, Any
import warnings
import json
from pathlib import Path

# Machine Learning imports
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA

# Deep Learning imports
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Anomaly Detection specific imports
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.models.ocsvm import OCSVM
from pyod.models.abod import ABOD
from pyod.utils.utility import standardizer

from scipy import stats
from scipy.signal import find_peaks
import ruptures as rpt

# import os
# os.environ['NUMBA_DISABLE_CUDA'] = '1'
# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Hide GPU from numba

# # Now import normally
# import stumpy
# import numpy as np

"""#### Anomaly Detection Class"""

# try:
#     import stumpy
#     STUMPY_AVAILABLE = True
#     print("✅ STUMPY loaded successfully (CPU mode)")
# except ImportError as e:
#     STUMPY_AVAILABLE = False
#     print(f"⚠️ STUMPY not available: {e}")
#     print("Will use alternative Matrix Profile implementation")
STUMPY_AVAILABLE = False

# Suppress warnings
warnings.filterwarnings('ignore')
tf.get_logger().setLevel('ERROR')

class AnomalyDetectionConfig:
    """Configuration class for anomaly detection parameters"""

    def __init__(self):
        # General parameters
        self.random_state = 42
        self.test_size = 0.2
        self.contamination = 0.1  # Expected anomaly ratio

        # Isolation Forest parameters
        self.iforest_params = {
            'contamination': 0.1,
            'n_estimators': 100,
            'max_samples': 'auto',
            'random_state': 42,
            'n_jobs': -1
        }

        # LSTM Autoencoder parameters
        # self.lstm_params = {
        #     'sequence_length': 24,    # 2 hours at 5-min intervals
        #     'encoding_dim': 32,       # Compressed representation size
        #     'lstm_units': [64, 32],   # LSTM layer sizes
        #     'dropout': 0.2,
        #     'batch_size': 32,
        #     'epochs': 50,
        #     'patience': 10,
        #     'learning_rate': 0.001
        # }
        # self.lstm_params = {
        #     'sequence_length': 8,        # Reduced from 24 (3x less memory)
        #     'encoding_dim': 4,          # Reduced from 32 (2x less memory)
        #     'lstm_units': [32, 16],      # Reduced from [64, 32] (2x less memory)
        #     'dropout': 0.3,              # Increased for regularization
        #     'batch_size': 512,           # Increased from 32 (much more efficient!)
        #     'epochs': 10,                # Reduced from 50
        #     'patience': 5,               # Reduced from 10
        #     'learning_rate': 0.001
        #     }

        # Statistical anomaly detection
        self.statistical_params = {
            'zscore_threshold': 3.0,
            'iqr_factor': 1.5,
            'mad_threshold': 3.5,  # Median Absolute Deviation
            'rolling_window': 48   # 4 hours
        }

        # Matrix Profile parameters
        self.matrix_profile_params = {
            'window_size': 24,     # 2 hours
            'normalize': True,
            'p': 2.0
        }

        # Change Point Detection
        self.changepoint_params = {
            'model': 'rbf',        # 'l1', 'l2', 'rbf', 'normal'
            'min_size': 12,        # Minimum segment size (1 hour)
            'jump': 1,
            'pen': 3
        }

        # Ensemble parameters
        self.ensemble_params = {
            'voting': 'soft',      # 'hard' or 'soft' voting
            'weights': {
                'isolation_forest': 0.3,
                'lstm_autoencoder': 0.3,
                'statistical': 0.2,
                'matrix_profile': 0.1,
                'changepoint': 0.1
            }
        }

        # Equipment-specific thresholds
        self.equipment_thresholds = {
            'centrifugal_pump_01': {'contamination': 0.08},
            'gas_compressor_01': {'contamination': 0.12},
            'drilling_motor_01': {'contamination': 0.15},
            'separator_01': {'contamination': 0.10},
            'generator_01': {'contamination': 0.07}
        }

# class LSTMAutoencoder:
#     """
#     LSTM Autoencoder for temporal anomaly detection in time series data
#     """

#     def __init__(self, input_dim: int, sequence_length: int, config: dict):
#         self.input_dim = input_dim
#         self.sequence_length = sequence_length
#         self.config = config
#         self.model = None
#         self.scaler = StandardScaler()
#         self.history = None
#         self.threshold = None

#     def build_model(self):
#         """Build LSTM Autoencoder architecture"""
#         # Input layer
#         input_layer = layers.Input(shape=(self.sequence_length, self.input_dim))

#         # Encoder
#         encoded = input_layer
#         for i, units in enumerate(self.config['lstm_units']):
#             return_sequences = (i < len(self.config['lstm_units']) - 1)
#             encoded = layers.LSTM(
#                 units,
#                 return_sequences=return_sequences,
#                 dropout=self.config['dropout'],
#                 name=f'encoder_lstm_{i+1}'
#             )(encoded)

#         # Repeat vector to prepare for decoder
#         encoded = layers.RepeatVector(self.sequence_length)(encoded)

#         # Decoder
#         decoded = encoded
#         for i, units in enumerate(reversed(self.config['lstm_units'])):
#             decoded = layers.LSTM(
#                 units,
#                 return_sequences=True,
#                 dropout=self.config['dropout'],
#                 name=f'decoder_lstm_{i+1}'
#             )(decoded)

#         # Output layer
#         output_layer = layers.TimeDistributed(
#             layers.Dense(self.input_dim, activation='linear'),
#             name='output'
#         )(decoded)

#         # Create model
#         self.model = Model(input_layer, output_layer, name='lstm_autoencoder')

#         # Compile
#         optimizer = keras.optimizers.Adam(learning_rate=self.config['learning_rate'])
#         self.model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

#         return self.model

#     def prepare_sequences(self, data: np.ndarray) -> np.ndarray:
#         """Convert time series data into sequences"""
#         sequences = []
#         for i in range(len(data) - self.sequence_length + 1):
#             sequences.append(data[i:i + self.sequence_length])
#         return np.array(sequences)

#     def fit(self, X: np.ndarray, validation_split: float = 0.2):
#         """Train the LSTM Autoencoder"""
#         print(f"Training LSTM Autoencoder on {X.shape[0]} samples...")

#         # Scale the data
#         X_scaled = self.scaler.fit_transform(X)

#         # Prepare sequences
#         X_sequences = self.prepare_sequences(X_scaled)

#         # Build model if not already built
#         if self.model is None:
#             self.build_model()

#         # Callbacks
#         callbacks = [
#             EarlyStopping(
#                 monitor='val_loss',
#                 patience=self.config['patience'],
#                 restore_best_weights=True
#             ),
#             ReduceLROnPlateau(
#                 monitor='val_loss',
#                 factor=0.5,
#                 patience=self.config['patience']//2
#             )
#         ]

#         # Train the model
#         self.history = self.model.fit(
#             X_sequences, X_sequences,
#             batch_size=self.config['batch_size'],
#             epochs=self.config['epochs'],
#             validation_split=validation_split,
#             callbacks=callbacks,
#             verbose=1
#         )

#         # Set threshold based on training data reconstruction error
#         train_predictions = self.model.predict(X_sequences)
#         train_errors = np.mean(np.square(X_sequences - train_predictions), axis=(1, 2))
#         self.threshold = np.percentile(train_errors, 95)  # 95th percentile as threshold

#         print(f"Training complete. Reconstruction threshold: {self.threshold:.4f}")

#         return self

#     def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
#         """Predict anomalies using reconstruction error"""
#         X_scaled = self.scaler.transform(X)
#         X_sequences = self.prepare_sequences(X_scaled)

#         # Get reconstructions
#         reconstructions = self.model.predict(X_sequences)

#         # Calculate reconstruction errors
#         reconstruction_errors = np.mean(np.square(X_sequences - reconstructions), axis=(1, 2))

#         # Determine anomalies
#         anomaly_scores = reconstruction_errors / (self.threshold + 1e-8)
#         anomalies = (reconstruction_errors > self.threshold).astype(int)

#         # Pad arrays to match original length
#         padded_scores = np.zeros(len(X))
#         padded_anomalies = np.zeros(len(X), dtype=int)

#         start_idx = self.sequence_length - 1
#         padded_scores[start_idx:start_idx + len(anomaly_scores)] = anomaly_scores
#         padded_anomalies[start_idx:start_idx + len(anomalies)] = anomalies

#         return padded_anomalies, padded_scores

class StatisticalAnomalyDetector:
    """
    Statistical methods for anomaly detection
    """

    def __init__(self, config: dict):
        self.config = config

    def zscore_detection(self, data: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """Z-score based anomaly detection"""
        z_scores = np.abs(stats.zscore(data.fillna(data.mean()), nan_policy='omit'))
        anomalies = (z_scores > self.config['zscore_threshold']).astype(int)
        return anomalies, z_scores

    def iqr_detection(self, data: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """IQR-based anomaly detection"""
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - self.config['iqr_factor'] * IQR
        upper_bound = Q3 + self.config['iqr_factor'] * IQR

        anomalies = ((data < lower_bound) | (data > upper_bound)).astype(int)
        scores = np.maximum(
            (lower_bound - data) / IQR,
            (data - upper_bound) / IQR
        ).fillna(0)
        scores = np.maximum(scores, 0)

        return anomalies.values, scores.values

    def mad_detection(self, data: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """Median Absolute Deviation based detection"""
        median = data.median()
        mad = np.median(np.abs(data - median))

        modified_z_scores = 0.6745 * (data - median) / (mad + 1e-8)
        anomalies = (np.abs(modified_z_scores) > self.config['mad_threshold']).astype(int)

        return anomalies.values, np.abs(modified_z_scores).values

    def rolling_anomaly_detection(self, data: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """Rolling window statistical anomaly detection"""
        window = self.config['rolling_window']

        rolling_mean = data.rolling(window=window, center=True).mean()
        rolling_std = data.rolling(window=window, center=True).std()

        z_scores = np.abs((data - rolling_mean) / (rolling_std + 1e-8))
        anomalies = (z_scores > self.config['zscore_threshold']).astype(int)

        return anomalies.fillna(0).values, z_scores.fillna(0).values

# class MatrixProfileAnomalyDetector:
#     """
#     Matrix Profile based anomaly detection using STUMPY
#     """

#     def __init__(self, config: dict):
#         self.config = config

#     def detect_anomalies(self, data: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
#         """Detect anomalies using Matrix Profile"""
#         try:
#             # Clean data
#             clean_data = data.fillna(method='ffill').fillna(method='bfill')

#             if len(clean_data) < self.config['window_size'] * 2:
#                 # Not enough data for matrix profile
#                 return np.zeros(len(data)), np.zeros(len(data))

#             # Compute matrix profile
#             matrix_profile = stumpy.stump(
#                 clean_data.values,
#                 m=self.config['window_size'],
#                 normalize=self.config['normalize'],
#                 p=self.config['p']
#             )

#             # Extract distances (first column)
#             distances = matrix_profile[:, 0]

#             # Pad distances to match original length
#             padded_distances = np.zeros(len(data))
#             padded_distances[:len(distances)] = distances

#             # Determine threshold (95th percentile)
#             threshold = np.percentile(distances, 95)
#             anomalies = (padded_distances > threshold).astype(int)

#             # Normalize scores
#             scores = padded_distances / (threshold + 1e-8)

#             return anomalies, scores

#         except Exception as e:
#             print(f"Matrix Profile detection failed: {str(e)}")
#             return np.zeros(len(data)), np.zeros(len(data))

# Import with fallback handling
# Modified Matrix Profile Detector with fallback
class MatrixProfileAnomalyDetector:
    """
    Matrix Profile anomaly detection with CPU-only fallback
    """

    def __init__(self, config: dict):
        self.config = config
        self.use_stumpy = STUMPY_AVAILABLE

    def detect_anomalies(self, data: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """Detect anomalies using Matrix Profile"""

        if self.use_stumpy:
            return self._stumpy_detection(data)
        else:
            return self._simple_detection(data)

    def _stumpy_detection(self, data: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """Original STUMPY-based detection"""
        try:
            # Clean data
            clean_data = data.fillna(method='ffill').fillna(method='bfill')

            if len(clean_data) < self.config['window_size'] * 2:
                return np.zeros(len(data)), np.zeros(len(data))

            # Compute matrix profile (CPU only)
            matrix_profile = stumpy.stump(
                clean_data.values,
                m=self.config['window_size']
            )

            # Extract distances (first column)
            distances = matrix_profile[:, 0]

            # Pad distances to match original length
            padded_distances = np.zeros(len(data))
            padded_distances[:len(distances)] = distances

            # Determine threshold (95th percentile)
            threshold = np.percentile(distances, 95)
            anomalies = (padded_distances > threshold).astype(int)

            # Normalize scores
            scores = padded_distances / (threshold + 1e-8)

            return anomalies, scores

        except Exception as e:
            print(f"STUMPY Matrix Profile detection failed: {str(e)}")
            print("Falling back to simple detection")
            return self._simple_detection(data)

    def _simple_detection(self, data: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """Simple sliding window distance-based detection"""
        try:
            # Clean data
            clean_data = data.fillna(method='ffill').fillna(method='bfill')

            if len(clean_data) < self.config['window_size'] * 2:
                return np.zeros(len(data)), np.zeros(len(data))

            window_size = min(self.config['window_size'], 20)  # Limit for performance
            distances = []

            # Simplified approach using rolling statistics
            rolling_mean = clean_data.rolling(window_size).mean()
            rolling_std = clean_data.rolling(window_size).std()

            # Distance from local pattern
            for i in range(window_size, len(clean_data)):
                current_val = clean_data.iloc[i]
                expected_val = rolling_mean.iloc[i]
                expected_std = rolling_std.iloc[i]

                # Standardized distance
                if expected_std > 0:
                    distance = abs(current_val - expected_val) / expected_std
                else:
                    distance = 0

                distances.append(distance)

            # Pad distances to match original length
            padded_distances = np.zeros(len(data))
            padded_distances[window_size:window_size + len(distances)] = distances

            # Determine threshold
            if distances:
                threshold = np.percentile(distances, 90)  # Slightly lower threshold
                anomalies = (padded_distances > threshold).astype(int)
                scores = padded_distances / (threshold + 1e-8)
            else:
                anomalies = np.zeros(len(data))
                scores = np.zeros(len(data))

            return anomalies, scores

        except Exception as e:
            print(f"Simple Matrix Profile detection failed: {str(e)}")
            return np.zeros(len(data)), np.zeros(len(data))

class ChangePointDetector:
    """
    Change point detection for structural breaks in time series
    """

    def __init__(self, config: dict):
        self.config = config

    def detect_changes(self, data: pd.Series) -> Tuple[List[int], np.ndarray]:
        """Detect change points in time series"""
        try:
            # Clean data
            clean_data = data.fillna(method='ffill').fillna(method='bfill').values

            if len(clean_data) < self.config['min_size'] * 3:
                return [], np.zeros(len(data))

            # Initialize change point detection algorithm
            algo = rpt.Pelt(
                model=self.config['model'],
                min_size=self.config['min_size'],
                jump=self.config['jump']
            )

            # Fit and predict
            algo.fit(clean_data)
            change_points = algo.predict(pen=self.config['pen'])

            # Remove last point (it's always the end of series)
            change_points = change_points[:-1] if change_points else []

            # Create anomaly array
            anomalies = np.zeros(len(data))
            scores = np.zeros(len(data))

            # Mark regions around change points as anomalous
            for cp in change_points:
                start = max(0, cp - self.config['min_size'])
                end = min(len(data), cp + self.config['min_size'])
                anomalies[start:end] = 1
                scores[start:end] = 1.0

            return change_points, scores

        except Exception as e:
            print(f"Change point detection failed: {str(e)}")
            return [], np.zeros(len(data))

class OilRigAnomalyDetector:
    """
    Comprehensive anomaly detection system for oil rig sensor data
    """

    def __init__(self, config: Optional[AnomalyDetectionConfig] = None):
        self.config = config or AnomalyDetectionConfig()

        # Initialize detectors
        self.isolation_forest = {}
        # self.lstm_autoencoder = {}
        self.statistical_detector = StatisticalAnomalyDetector(self.config.statistical_params)
        self.matrix_profile_detector = MatrixProfileAnomalyDetector(self.config.matrix_profile_params)
        self.changepoint_detector = ChangePointDetector(self.config.changepoint_params)

        # Results storage
        self.anomaly_results = {}
        self.ensemble_results = {}
        self.detection_log = []

        # Performance metrics
        self.performance_metrics = {}

    def log_step(self, step_name: str, details: str):
        """Log detection steps"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.detection_log.append({
            'timestamp': timestamp,
            'step': step_name,
            'details': details
        })
        print(f"[{timestamp}] {step_name}: {details}")

    def prepare_data_for_equipment(self, sensor_data: pd.DataFrame, equipment: str) -> pd.DataFrame:
        """Prepare data for a specific equipment"""
        equip_data = sensor_data[sensor_data['equipment'] == equipment].copy()
        equip_data = equip_data.sort_values('timestamp').reset_index(drop=True)

        # Get numeric sensor columns
        numeric_cols = [col for col in equip_data.columns
                       if col not in ['timestamp', 'equipment'] and
                       pd.api.types.is_numeric_dtype(equip_data[col])]

        return equip_data[['timestamp'] + numeric_cols]

    def fit_isolation_forest(self, X: pd.DataFrame, equipment: str):
        """Fit Isolation Forest for equipment"""
        self.log_step("ISOLATION_FOREST", f"Training Isolation Forest for {equipment}")

        # Get equipment-specific contamination rate
        contamination = self.config.equipment_thresholds.get(
            equipment, {}
        ).get('contamination', self.config.contamination)

        # Prepare parameters
        params = self.config.iforest_params.copy()
        params['contamination'] = contamination

        # Get numeric columns
        numeric_cols = [col for col in X.columns if col != 'timestamp']
        X_numeric = X[numeric_cols].fillna(X[numeric_cols].mean())

        # Fit model
        model = IsolationForest(**params)
        model.fit(X_numeric)

        self.isolation_forest[equipment] = {
            'model': model,
            'feature_cols': numeric_cols,
            'contamination': contamination
        }

        self.log_step("ISOLATION_FOREST", f"Isolation Forest fitted for {equipment} with contamination={contamination}")

    # def fit_lstm_autoencoder(self, X: pd.DataFrame, equipment: str):
    #     """Fit LSTM Autoencoder for equipment"""
    #     self.log_step("LSTM_AUTOENCODER", f"Training LSTM Autoencoder for {equipment}")

    #     # Get numeric columns
    #     numeric_cols = [col for col in X.columns if col != 'timestamp']
    #     X_numeric = X[numeric_cols].fillna(X[numeric_cols].mean()).values

    #     if len(X_numeric) < self.config.lstm_params['sequence_length'] * 2:
    #         self.log_step("LSTM_AUTOENCODER", f"Insufficient data for {equipment}, skipping LSTM")
    #         return

    #     # Initialize and train autoencoder
    #     autoencoder = LSTMAutoencoder(
    #         input_dim=len(numeric_cols),
    #         sequence_length=self.config.lstm_params['sequence_length'],
    #         config=self.config.lstm_params
    #     )

    #     autoencoder.fit(X_numeric)

    #     self.lstm_autoencoder[equipment] = {
    #         'model': autoencoder,
    #         'feature_cols': numeric_cols
    #     }

    #     self.log_step("LSTM_AUTOENCODER", f"LSTM Autoencoder trained for {equipment}")

    def detect_anomalies_for_equipment(self, X: pd.DataFrame, equipment: str) -> Dict:
        """Detect anomalies for a specific equipment using all methods"""
        results = {
            'equipment': equipment,
            'timestamps': X['timestamp'].values,
            'isolation_forest': None,
            # 'lstm_autoencoder': None,
            'statistical': {},
            'matrix_profile': {},
            'changepoint': {},
            'ensemble': None
        }

        # Get numeric columns
        numeric_cols = [col for col in X.columns if col != 'timestamp']

        # 1. Isolation Forest
        if equipment in self.isolation_forest:
            X_numeric = X[numeric_cols].fillna(X[numeric_cols].mean())
            model = self.isolation_forest[equipment]['model']

            predictions = model.predict(X_numeric)  # -1 for anomalies, 1 for normal
            scores = model.decision_function(X_numeric)  # Lower scores = more anomalous

            # Convert to binary (1 for anomaly, 0 for normal)
            anomalies = (predictions == -1).astype(int)
            # Convert scores (higher = more anomalous)
            anomaly_scores = -scores

            results['isolation_forest'] = {
                'anomalies': anomalies,
                'scores': anomaly_scores
            }

        # 2. LSTM Autoencoder
        # if equipment in self.lstm_autoencoder:
        #     X_numeric = X[numeric_cols].fillna(X[numeric_cols].mean()).values
        #     autoencoder = self.lstm_autoencoder[equipment]['model']

        #     anomalies, scores = autoencoder.predict(X_numeric)

        #     results['lstm_autoencoder'] = {
        #         'anomalies': anomalies,
        #         'scores': scores
        #     }

        # 3. Statistical Methods
        for col in numeric_cols[:5]:  # Limit to first 5 sensors to avoid overloading
            if col in X.columns:
                data_series = X[col]

                # Z-score
                z_anomalies, z_scores = self.statistical_detector.zscore_detection(data_series)
                results['statistical'][f'{col}_zscore'] = {
                    'anomalies': z_anomalies,
                    'scores': z_scores
                }

                # IQR
                iqr_anomalies, iqr_scores = self.statistical_detector.iqr_detection(data_series)
                results['statistical'][f'{col}_iqr'] = {
                    'anomalies': iqr_anomalies,
                    'scores': iqr_scores
                }

        # 4. Matrix Profile (on first sensor)
        if numeric_cols:
            first_sensor = numeric_cols[0]
            mp_anomalies, mp_scores = self.matrix_profile_detector.detect_anomalies(X[first_sensor])
            results['matrix_profile'] = {
                'anomalies': mp_anomalies,
                'scores': mp_scores,
                'sensor': first_sensor
            }

        # 5. Change Point Detection (on first sensor)
        if numeric_cols:
            first_sensor = numeric_cols[0]
            change_points, cp_scores = self.changepoint_detector.detect_changes(X[first_sensor])
            results['changepoint'] = {
                'change_points': change_points,
                'scores': cp_scores,
                'sensor': first_sensor
            }

        return results

    def ensemble_scoring(self, results: Dict) -> Tuple[np.ndarray, np.ndarray]:
        """Combine all anomaly detection results using weighted ensemble"""
        n_samples = len(results['timestamps'])
        ensemble_scores = np.zeros(n_samples)
        ensemble_anomalies = np.zeros(n_samples)

        weights = self.config.ensemble_params['weights']
        total_weight = 0

        # Isolation Forest
        if results['isolation_forest'] is not None:
            weight = weights['isolation_forest']
            scores = results['isolation_forest']['scores']
            normalized_scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)
            ensemble_scores += weight * normalized_scores
            total_weight += weight

        # LSTM Autoencoder
        # if results['lstm_autoencoder'] is not None:
        #     weight = weights['lstm_autoencoder']
        #     scores = results['lstm_autoencoder']['scores']
        #     normalized_scores = np.clip(scores, 0, 5) / 5.0  # Clip and normalize
        #     ensemble_scores += weight * normalized_scores
        #     total_weight += weight

        # Statistical Methods (average them)
        if results['statistical']:
            weight = weights['statistical']
            stat_scores = []
            for method_results in results['statistical'].values():
                scores = method_results['scores']
                normalized_scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)
                stat_scores.append(normalized_scores)

            if stat_scores:
                avg_stat_scores = np.mean(stat_scores, axis=0)
                ensemble_scores += weight * avg_stat_scores
                total_weight += weight

        # Matrix Profile
        if results['matrix_profile']:
            weight = weights['matrix_profile']
            scores = results['matrix_profile']['scores']
            normalized_scores = np.clip(scores, 0, 3) / 3.0  # Clip and normalize
            ensemble_scores += weight * normalized_scores
            total_weight += weight

        # Change Point
        if results['changepoint']:
            weight = weights['changepoint']
            scores = results['changepoint']['scores']
            ensemble_scores += weight * scores
            total_weight += weight

        # Normalize final ensemble scores
        if total_weight > 0:
            ensemble_scores /= total_weight

        # Determine ensemble anomalies (threshold at 75th percentile)
        threshold = np.percentile(ensemble_scores, 75)
        ensemble_anomalies = (ensemble_scores > threshold).astype(int)

        return ensemble_anomalies, ensemble_scores

    def fit_and_detect(self, sensor_data: pd.DataFrame) -> Dict:
        """Complete anomaly detection pipeline"""
        self.log_step("PIPELINE_START", "Starting anomaly detection pipeline")

        all_results = {}

        # Process each equipment separately
        for equipment in sensor_data['equipment'].unique():
            self.log_step("EQUIPMENT_PROCESSING", f"Processing {equipment}")

            # Prepare equipment data
            equip_data = self.prepare_data_for_equipment(sensor_data, equipment)

            if len(equip_data) < 50:  # Minimum data requirement
                self.log_step("EQUIPMENT_SKIP", f"Insufficient data for {equipment}")
                continue

            # Split data for training (first 80%) and detection (full dataset)
            split_idx = int(len(equip_data) * 0.8)
            train_data = equip_data[:split_idx]

            # Fit models on training data
            self.fit_isolation_forest(train_data, equipment)
            # self.fit_lstm_autoencoder(train_data, equipment)

            # Detect anomalies on full dataset
            equipment_results = self.detect_anomalies_for_equipment(equip_data, equipment)

            # Ensemble scoring
            ensemble_anomalies, ensemble_scores = self.ensemble_scoring(equipment_results)
            equipment_results['ensemble'] = {
                'anomalies': ensemble_anomalies,
                'scores': ensemble_scores
            }

            all_results[equipment] = equipment_results

            self.log_step("EQUIPMENT_COMPLETE", f"Completed anomaly detection for {equipment}")

        self.anomaly_results = all_results
        self.log_step("PIPELINE_COMPLETE", f"Anomaly detection complete for {len(all_results)} equipment")

        return all_results

    def get_anomaly_summary(self) -> Dict:
        """Get summary of detected anomalies"""
        if not self.anomaly_results:
            return {}

        summary = {}

        for equipment, results in self.anomaly_results.items():
            equip_summary = {
                'total_samples': len(results['timestamps']),
                'methods': {}
            }

            # Isolation Forest
            if results['isolation_forest']:
                anomaly_count = results['isolation_forest']['anomalies'].sum()
                equip_summary['methods']['isolation_forest'] = {
                    'anomaly_count': int(anomaly_count),
                    'anomaly_rate': float(anomaly_count / len(results['timestamps']))
                }

            # LSTM Autoencoder
            # if results['lstm_autoencoder']:
            #     anomaly_count = results['lstm_autoencoder']['anomalies'].sum()
            #     equip_summary['methods']['lstm_autoencoder'] = {
            #         'anomaly_count': int(anomaly_count),
            #         'anomaly_rate': float(anomaly_count / len(results['timestamps']))
            #     }

            # Ensemble
            if results['ensemble']:
                anomaly_count = results['ensemble']['anomalies'].sum()
                equip_summary['methods']['ensemble'] = {
                    'anomaly_count': int(anomaly_count),
                    'anomaly_rate': float(anomaly_count / len(results['timestamps']))
                }

            summary[equipment] = equip_summary

        return summary

    def visualize_anomalies(self, equipment: str, sensor_cols: Optional[List[str]] = None):
        """Visualize anomaly detection results for specific equipment"""
        if equipment not in self.anomaly_results:
            print(f"No results found for {equipment}")
            return

        results = self.anomaly_results[equipment]

        # Create subplots
        fig = make_subplots(
            rows=4, cols=1,
            subplot_titles=(
                f'{equipment} - Sensor Data with Anomalies',
                'Isolation Forest Results',
                'Ensemble Results'
            ),
            vertical_spacing=0.08
        )

        timestamps = pd.to_datetime(results['timestamps'])

        # Plot 1: Original sensor data with ensemble anomalies
        if results['ensemble']:
            anomaly_mask = results['ensemble']['anomalies'] == 1
            normal_timestamps = timestamps[~anomaly_mask]
            anomaly_timestamps = timestamps[anomaly_mask]

            # Plot first sensor as example
            if results['isolation_forest']:
                feature_cols = self.isolation_forest[equipment]['feature_cols']
                if feature_cols:
                    sensor_name = feature_cols[0]
                    # We need to get original sensor data - this is a placeholder
                    # In real implementation, you'd pass the original sensor values
                    fig.add_trace(
                        go.Scatter(
                            x=normal_timestamps,
                            y=np.random.normal(50, 10, len(normal_timestamps)),  # Placeholder
                            mode='lines',
                            name='Normal Data',
                            line=dict(color='blue', width=1)
                        ), row=1, col=1
                    )

                    fig.add_trace(
                        go.Scatter(
                            x=anomaly_timestamps,
                            y=np.random.normal(50, 10, len(anomaly_timestamps)),  # Placeholder
                            mode='markers',
                            name='Anomalies',
                            marker=dict(color='red', size=6)
                        ), row=1, col=1
                    )

        # Plot 2: Isolation Forest scores
        if results['isolation_forest']:
            scores = results['isolation_forest']['scores']
            fig.add_trace(
                go.Scatter(
                    x=timestamps,
                    y=scores,
                    mode='lines',
                    name='IF Scores',
                    line=dict(color='green')
                ), row=2, col=1
            )

            # Add threshold line
            anomalies = results['isolation_forest']['anomalies']
            if anomalies.sum() > 0:
                threshold = np.min(scores[anomalies == 1])
                fig.add_hline(y=threshold, line_dash="dash", line_color="red", row=2, col=1)

        # # Plot 3: LSTM Autoencoder scores
        # if results['lstm_autoencoder']:
        #     scores = results['lstm_autoencoder']['scores']
        #     fig.add_trace(
        #         go.Scatter(
        #             x=timestamps,
        #             y=scores,
        #             mode='lines',
        #             name='LSTM Scores',
        #             line=dict(color='purple')
        #         ), row=3, col=1
        #     )

        # Plot 4: Ensemble scores
        if results['ensemble']:
            scores = results['ensemble']['scores']
            fig.add_trace(
                go.Scatter(
                    x=timestamps,
                    y=scores,
                    mode='lines',
                    name='Ensemble Scores',
                    line=dict(color='orange')
                ), row=4, col=1
            )

        fig.update_layout(height=1000, title=f'Anomaly Detection Results - {equipment}')
        fig.show()

processed_data1 = pd.read_csv('processed_sensor_data.csv')
processed_data1['equipment'].unique()

processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])

print(f"Loaded {len(processed_data)} processed sensor readings")
print(f"Equipment: {processed_data['equipment'].unique()}")
print(f"Date range: {processed_data['timestamp'].min()} to {processed_data['timestamp'].max()}")

# Initialize configuration
config = AnomalyDetectionConfig()
# Reduce parameters for faster testing
# config.lstm_params['epochs'] = 5
# config.lstm_params['sequence_length'] = 4
# Initialize detector
detector = OilRigAnomalyDetector(config)

processed_data2 = processed_data[processed_data['equipment'].isin(['centrifugal_pump_01','separator_01', 'generator_01'])]
# Run anomaly detection
results = detector.fit_and_detect(processed_data2)

# Get summary
summary = detector.get_anomaly_summary()

# Display results
print("\nAnomaly Detection Results:")
print(f"Equipment processed: {len(results)}")

display(results)

for equipment, equip_summary in summary.items():
  print(f"\n{equipment}:")
  print(f"  Total samples: {equip_summary['total_samples']}")
  for method, metrics in equip_summary['methods'].items():
    print(f"  {method}: {metrics['anomaly_count']} anomalies ({metrics['anomaly_rate']:.2%})")

def convert_numpy_to_json_serializable(obj):
    """
    Recursively convert NumPy arrays and datetime objects to JSON-serializable formats
    """
    if isinstance(obj, dict):
        return {key: convert_numpy_to_json_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_to_json_serializable(item) for item in obj]
    elif isinstance(obj, np.ndarray):
        # Handle datetime64 arrays
        if obj.dtype.kind == 'M':  # datetime64 type
            return obj.astype(str).tolist()  # Convert to ISO string format
        else:
            return obj.tolist()  # Convert regular arrays to lists
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, (np.bool_, bool)):
        return bool(obj)
    else:
        return obj

# Convert your results to JSON-serializable format
results_json_serializable = convert_numpy_to_json_serializable(results)

# Convert to JSON string
json_string = json.dumps(results_json_serializable, indent=2)

# Save to file
with open('anomaly_detection_results.json', 'w') as f:
    json.dump(results_json_serializable, f, indent=2)

print("✅ Successfully converted to JSON!")
print(f"JSON file size: {len(json_string):,} characters")

# Preview the structure
print("\n📊 JSON Structure Preview:")
print("Number of equipment:", len(results_json_serializable))
for i, equipment_dict in enumerate(results_json_serializable):
    equipment_name = list(equipment_dict.keys())[0]
    equipment_data = equipment_dict[equipment_name]
    print(f"  {i+1}. {equipment_name}")
    print(f"     - Timestamps: {len(equipment_data['timestamps'])} entries")
    print(f"     - Methods: {list(equipment_data.keys())}")

    # Show sample timestamp format
    sample_timestamp = equipment_data['timestamps'][0]
    print(f"     - Sample timestamp: {sample_timestamp}")

# Alternative: More compact JSON (no indentation)
compact_json = json.dumps(results_json_serializable)
print(f"\n💾 Compact JSON size: {len(compact_json):,} characters")

# You can also create separate JSON files for each equipment
for equipment_dict in results_json_serializable:
    equipment_name = list(equipment_dict.keys())[0]
    equipment_data = equipment_dict[equipment_name]

    filename = f"{equipment_name}_anomaly_results.json"
    with open(filename, 'w') as f:
        json.dump({equipment_name: equipment_data}, f, indent=2)
    print(f"✅ Saved {filename}")

